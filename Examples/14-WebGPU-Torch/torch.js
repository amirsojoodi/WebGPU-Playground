!function webpackUniversalModuleDefinition(e,r){"object"==typeof exports&&"object"==typeof module?module.exports=r():"function"==typeof define&&define.amd?define([],r):"object"==typeof exports?exports.torch=r():e.torch=r()}(self,(()=>(()=>{"use strict";var __webpack_modules__={149:(e,r,n)=>{Object.defineProperty(r,"__esModule",{value:!0}),r.noGrad=r.enableGrad=r.shouldCreateGradient=r.setGradEnabled=r.isGradEnabled=r.AutoFunction=r.GradientContext=void 0;const s=n(647);function isTensor(e){return e instanceof s.TensorBase}class GradientContext{needsInputGradient;inputsWithGradient;savedTensors=[];constructor(e){this.needsInputGradient=e.map((e=>isTensor(e)&&e.requiresGrad)),this.inputsWithGradient=e.map((e=>isTensor(e)&&e.requiresGrad?e:null))}saveForBackward(...e){this.savedTensors=e}}r.GradientContext=GradientContext;r.AutoFunction=class AutoFunction{static forward(e){throw new Error("Do not call forward on AutoFunction directly.")}static setupContext(e,r,n){throw new Error("Do not call setupContext on AutoFunction directly.")}static backward(e,r){throw new Error("Do not call backward on AutoFunction directly.")}static apply(...e){const r=new GradientContext(e),n=e.map((e=>isTensor(e)?e.detach():e)),s=this.forward(n);return this.setupContext(r,n,s),s.setGradientFunction(r,this.backward),s}};let a=!0;function isGradEnabled(){return a}function setGradEnabled(e){a=e}r.isGradEnabled=isGradEnabled,r.setGradEnabled=setGradEnabled,r.shouldCreateGradient=function shouldCreateGradient(...e){if(!a)return!1;for(const r of e)if(void 0!==r&&r.requiresGrad)return!0;return!1},r.enableGrad=function enableGrad(e){const r=isGradEnabled();setGradEnabled(!0);try{return e()}finally{setGradEnabled(r)}},r.noGrad=function noGrad(e){const r=isGradEnabled();setGradEnabled(!1);try{return e()}finally{setGradEnabled(r)}}},44:(e,r,n)=>{Object.defineProperty(r,"__esModule",{value:!0}),r.Device=void 0;const s=n(679),a=n(950);r.Device=class Device{_id;_type;_kernels={};_heapEnabled=!1;_heaps=[];_heapFinalizers;get id(){return this._id}get type(){return this._type}constructor(e,r){this._id=e,this._type=r,this._heapFinalizers=new FinalizationRegistry((e=>{e.free()}))}heapAlloc(e){if(!1===this._heapEnabled)return this.alloc(e);let r=null;for(let n of this._heaps){const s=n.alloc(e);null!==s&&(r=s)}if(null===r){let n=0;for(let e of this._heaps)n+=e.size/1024/1024;if(n>8192)return this.alloc(e);const s=this.allocBufferHeap();if(this._heaps.push(s),r=s.alloc(e),null===r)throw new Error(`Out of memory when trying to allocate buffer of size ${e}. Heap size is ${s.size}.`)}const n=this.createHeapStorage(r);return this._heapFinalizers.register(n,r),n}getKernel(e,r){const n=a.registry[e];if(void 0===n)throw new Error(`Kernel "${e}" not found`);const i=(0,s.getKernelConfig)(n,r),o=(0,s.getKernelKey)(n,i);let u=this._kernels[o];return void 0===u&&(u=this.createKernel(n,i),this._kernels[o]=u),u}}},441:(e,r,n)=>{Object.defineProperty(r,"__esModule",{value:!0}),r.DeviceCPU=void 0;const s=n(44),a=n(421),i=n(566),o=n(320),u=n(81);class DeviceCPU extends s.Device{get workgroupMaxSize(){return[256,256,64]}get workgroupMaxCount(){return 65535}constructor(){super("cpu","cpu")}initStorage(e,r,n){const s=(0,o.dtypeByteSize)(r),i=(0,u.shapeSize)(e)*s,p=new a.ArrayBufferStorage(i);return n(p.getTypedArray(r)),p}alloc(e){return new a.ArrayBufferStorage(e)}allocBufferHeap(){const e=268435456,r=new ArrayBuffer(e);return new a.BufferHeap(r,e,8)}createHeapStorage(e){return new a.ArrayBufferStorage(e)}createKernel(e,r){return new i.KernelCPU(e,r,this)}}r.DeviceCPU=DeviceCPU},7:(e,r,n)=>{Object.defineProperty(r,"__esModule",{value:!0}),r.DeviceWebGPU=void 0;const s=n(44),a=n(320),i=n(421),o=n(610),u=n(81);class DeviceWebGPU extends s.Device{_device;_bufferPools={};_finalizationRegistry;get gpuDevice(){return this._device}get workgroupMaxSize(){return[this._device.limits.maxComputeWorkgroupSizeX,this._device.limits.maxComputeWorkgroupSizeY,this._device.limits.maxComputeWorkgroupSizeZ]}get workgroupMaxCount(){return this._device.limits.maxComputeWorkgroupsPerDimension}constructor(e,r,n){super(e,"webgpu"),this._device=n,this._finalizationRegistry=new FinalizationRegistry((e=>{const r=e.size;let n=this._bufferPools[r];void 0===n&&(n=[],this._bufferPools[r]=n),n.length<256&&n.push({usage:e.usage,buffer:e})}))}initStorage(e,r,n){const s=(0,a.dtypeByteSize)(r),o=(0,u.shapeSize)(e)*s,p=4*Math.floor((o+3)/4),d=this._device.createBuffer({mappedAtCreation:!0,size:p,usage:GPUBufferUsage.STORAGE|GPUBufferUsage.COPY_SRC}),h=d.getMappedRange();return n((0,a.dtypedBufferToTypedArray)(r,h)),d.unmap(),new i.GPUBufferStorage(d,this)}alloc(e){const r=4*Math.floor((e+3)/4),n=this._device.createBuffer({mappedAtCreation:!1,size:r,usage:GPUBufferUsage.STORAGE|GPUBufferUsage.COPY_SRC});return new i.GPUBufferStorage(n,this)}allocBufferHeap(){const e=this.gpuDevice.limits.maxBufferSize,r=this._device.createBuffer({mappedAtCreation:!1,size:e,usage:GPUBufferUsage.STORAGE|GPUBufferUsage.COPY_SRC}),n=Math.ceil(Math.log2(this.gpuDevice.limits.minStorageBufferOffsetAlignment));return new i.BufferHeap(r,e,n)}createHeapStorage(e){return new i.GPUBufferStorage(e,this)}createKernel(e,r){return new o.KernelWebGPU(e,r,this)}}r.DeviceWebGPU=DeviceWebGPU},459:(e,r,n)=>{Object.defineProperty(r,"__esModule",{value:!0}),r.getDevice=r.discoverWebGPUDevicesAsync=r.cpuDevice=void 0;const s=n(441),a=n(7);r.cpuDevice=new s.DeviceCPU;let i=null;const o={cpu:r.cpuDevice};r.discoverWebGPUDevicesAsync=async function discoverWebGPUDevicesAsync(){if(!navigator.gpu)return!1;const e="webgpu";if(e in o)return!0;const r=await navigator.gpu.requestAdapter(),n=await r.requestDevice(),s=new a.DeviceWebGPU(e,r,n);return o[e]=s,i=s,console.log("Found WebGPU device",n),!0},r.getDevice=function getDevice(e){if(null==e)return i||r.cpuDevice;if("string"==typeof e){if(e in o)return o[e];{const r=function findDeviceWithType(e){for(const r in o)if(o[r].type===e)return o[r];return null}(e);if(r)return r;throw new Error(`Device ${e} not found`)}}return e}},320:(e,r)=>{Object.defineProperty(r,"__esModule",{value:!0}),r.getDtype=r.dtypeByteSize=r.dtypedBufferToTypedArray=r.newTypedArrayForDtype=void 0;const n={int8:Int8Array,uint8:Uint8Array,int32:Int32Array,uint32:Uint32Array,float32:Float32Array};r.newTypedArrayForDtype=function newTypedArrayForDtype(e,r){switch(r){case"int8":return new Int8Array(e);case"uint8":return new Uint8Array(e);case"int32":return new Int32Array(e);case"uint32":return new Uint32Array(e);case"float32":return new Float32Array(e);default:throw new Error(`Unsupported dtype: ${r}`)}},r.dtypedBufferToTypedArray=function dtypedBufferToTypedArray(e,r,s,a){if("int64"===e)throw new Error("int64 not supported");const i=n[e];if(void 0===i)throw new Error(`Invalid dtype "${e}"`);return void 0===s&&(s=0),void 0===a&&(a=r.byteLength-s),new i(r,s,a/i.BYTES_PER_ELEMENT)},r.dtypeByteSize=function dtypeByteSize(e){switch(e){case"int8":case"uint8":return 1;case"int32":case"uint32":case"float32":return 4;case"int64":return 8;default:throw new Error(`Invalid dtype ${e}`)}},r.getDtype=function getDtype(e,r){if(null==e)return r||"float32";if("int8"===e)return"int8";if("uint8"===e)return"uint8";if("int32"===e)return"int32";if("uint32"===e)return"uint32";if("float32"===e)return"float32";if(e instanceof Int8Array)return"int8";if(e instanceof Uint8Array)return"uint8";if(e instanceof Int32Array)return"int32";if(e instanceof Uint32Array)return"uint32";if(e instanceof Float32Array)return"float32";throw new Error(`Invalid dtype ${JSON.stringify(e)}`)}},653:(e,r)=>{Object.defineProperty(r,"__esModule",{value:!0}),r.evalCode=r.compileCode=r.exprCodeToWebGLShader=r.exprNodeToWebGLShader=r.exprNodeToString=r.substituteIdentifiers=r.substitute=r.parseCode=r.ManifestNumber=void 0;class ManifestNumber{type;value;constructor(e,r){this.type=e,this.value=r}}function parseCode(e){if("number"==typeof e)return new ManifestNumber(e%1==0?"intAbstract":"floatAbstract",e);const r=function lexn(e){const r=[],n=e.length;let s=0;for(;s<n;){const a=e[s];if(" "!==a&&"\t"!==a&&"\n"!==a)if("+"!==a&&"-"!==a&&"*"!==a&&"/"!==a&&","!=a&&";"!=a&&":"!=a&&"?"!=a&&"^"!=a&&"%"!=a&&"~"!=a&&"("!=a&&")"!=a&&"["!=a&&"]"!=a&&"{"!=a&&"}"!=a&&"."!=a)if("="!=a&&"<"!=a&&">"!=a&&"!"!=a)if("&"!=a)if("|"!=a)if(a>="0"&&a<="9"){let a="",i=!1;for(;s<n&&(e[s]>="0"&&e[s]<="9"||"."===e[s]);){if("."===e[s]){if(i)throw new Error("Invalid number");i=!0}a+=e[s],s++}let o=i?"floatAbstract":"intAbstract";s<n&&"f"===e[s]&&(s++,o="floatAbstract"),r.push(new ManifestNumber(o,parseFloat(a)))}else{if(!(a>="a"&&a<="z"||a>="A"&&a<="Z"||"_"===a))throw new Error(`Unexpected character ${a}`);{let a="";for(;s<n&&(e[s]>="a"&&e[s]<="z"||e[s]>="A"&&e[s]<="Z"||e[s]>="0"&&e[s]<="9"||"_"===e[s]);)a+=e[s],s++;r.push(a)}}else{if(s+1<n&&"|"==e[s+1]){r.push("||"),s+=2;continue}r.push("|"),s++}else{if(s+1<n&&"&"==e[s+1]){r.push("&&"),s+=2;continue}r.push("&"),s++}else{if(s+1<n&&"="==e[s+1]){r.push(a+"="),s+=2;continue}r.push(a),s++}else r.push(a),s++;else s++}return r}(e),parsePrimary=e=>{if(e>=r.length)return null;const n=r[e];if(n instanceof ManifestNumber)return[n,e+1];if("("===n){const n=parseExpr(e+1);if(null===n)throw new Error("Missing expression");if((e=n[1])>=r.length)throw new Error("Unexpected end of expression");const s=r[e];if("string"!=typeof s||")"!==s)throw new Error("Expected )");return[n[0],e+1]}if(!function tokenIsIdent(e){if(e instanceof ManifestNumber)return!1;if(e.length<=0)return!1;const r=e[0];return(r>="a"&&r<="z"||r>="A"&&r<="Z"||"_"===r)&&"if"!==r&&"else"!==r&&"return"!=r}(n))return null;let s=[n,e+1];if(e+1<r.length&&"("===r[e+1]){const n=[];let a=e+2;for(;a<r.length;){const e=parseExpr(a);if(null===e)throw new Error("Missing argument");if(n.push(e[0]),a=e[1],a>=r.length)throw new Error("Unexpected end of expression");const s=r[a];if("string"!=typeof s)throw new Error("Expected , or )");if(")"===s)break;if(","!==s)throw new Error("Expected ,");a++}const i=s[0];n.splice(0,0,i),s=[["apply",n],a+1]}return s};function parseExpr(e){return e>=r.length?null:parseConditional(e)}function parseConditional(n){if(n>=r.length)return null;let s=h(n);if(null===s)return null;if((n=s[1])>=r.length)return s;const a=r[n];if("?"!==a)return s;const i=parseExpr(n+1);if(null===i)throw new Error("Missing expression after ?");if((n=i[1])>=r.length)throw new Error("Unexpected end of conditional expression");const o=r[n];if("string"!=typeof o||":"!==o)throw new Error(`Expected ':', got ${o}`);const u=parseConditional(n+1);if(null===u)throw new Error(`Missing expression after : \`${e}\``);return[[a,[s[0],i[0],u[0]]],u[1]]}function genericParseSeparatedList(e,n){const s={};for(const e of n)s[e]=!0;return n=>{let a=e(n);if(null===a)return null;for(n=a[1];n<r.length;){const i=r[n];if("string"!=typeof i)break;if(!(i in s))break;{const r=e(n+1);if(null===r)throw new Error("Missing expression after "+i);a=[[i,[a[0],r[0]]],r[1]],n=r[1]}}return a}}const n=genericParseSeparatedList((function parseUnary(e){if(e>=r.length)return null;const n=r[e];if("string"==typeof n){if("+"===n||"-"===n){const r=parseUnary(e+1);if(null===r)throw new Error("Missing expression");return"+"===n?r:[["negate",[r[0]]],r[1]]}if("!"===n){const r=parseUnary(e+1);if(null===r)throw new Error("Missing expression");return[[n,[r[0]]],r[1]]}}return parsePrimary(e)}),["*","/"]),s=genericParseSeparatedList(n,["+","-"]),a=genericParseSeparatedList(s,["<",">","<=",">="]),i=genericParseSeparatedList(a,["==","!="]),o=genericParseSeparatedList(i,["&"]),u=genericParseSeparatedList(o,["|"]),p=genericParseSeparatedList(u,["|"]),d=genericParseSeparatedList(p,["&&"]),h=genericParseSeparatedList(d,["||"]);function parseStatement(e){if(e>=r.length)return null;if(";"===r[e])return[["block",[]],e+1];if("{"===r[e]){let n=e+1;if(n>=r.length)throw new Error("Missing } at end of input");if("}"===r[n])return[["block",[]],n+1];const s=parseStatements(n);if(null===s)throw new Error("Missing statements after {");if(n=s[1],"}"!==r[n])throw new Error(`Missing } (${r})`);const a=s[0];return a instanceof Array&&"statements"===a[0]?[["block",a[1]],n+1]:[["block",[a]],n+1]}if("if"===r[e]){let n=e+1;if(n>=r.length)throw new Error("Missing ( after if");if("("!==r[n])throw new Error("Missing ( after if");const s=parseExpr(n+1);if(null===s)throw new Error("Missing condition after if");if(n=s[1],n>=r.length||")"!==r[n])throw new Error("Missing ) after if condition");const a=parseStatement(n+1);if(null===a)throw new Error("Missing then after if condition");if(n=a[1],n>=r.length||"else"!==r[n])return[["if",[s[0],a[0]]],n];const i=parseStatement(n+1);if(null===i)throw new Error("Missing else after if condition");return n=i[1],[["if",[s[0],a[0],i[0]]],n]}if("var"===r[e]){const n=r[e+1];if("string"!=typeof n)throw new Error("Missing variable name after var");if("="!==r[e+2])throw new Error("Missing = after var");const s=parseExpr(e+3);if(null===s)throw new Error("Missing initial value after var");return[["var",[n,s[0]]],s[1]]}if("return"===r[e]){const r=parseExpr(e+1);if(null===r)throw new Error("Missing expression after return");return[["return",[r[0]]],r[1]]}const n=parseExpr(e);if(null===n)return null;if((e=n[1])<r.length&&"="===r[e]){const r=parseExpr(e+1);if(null===r)throw new Error("Missing right hand side of assignment");return[["assign",[n[0],r[0]]],r[1]]}return n}function parseStatements(e){if(e>=r.length)return null;const n=parseStatement(e);if(null===n)return null;if((e=n[1])>=r.length)return n;let s=[n[0]];for(;e<r.length;){if(";"!==r[e])break;const n=parseStatement(e+=1);if(null===n)break;s.push(n[0]),e=n[1]}return 1===s.length?[s[0],e]:[["statements",s],e]}const c=parseStatements(0);if(null===c)throw new Error("Missing expression");if(c[1]<r.length)throw new Error(`Unexpected token '${r[c[1]]}' after parsing ${JSON.stringify(c[0])} from tokens [${r.slice(0,c[1]+1)}]`);return c[0]}function substitute(e,r,n){if(e instanceof ManifestNumber)return e;if("string"==typeof e)return e;const s=[];for(const a of e[1])s.push(substitute(a,r,n));return r(e=[e[0],s])&&(e=n(e)),e}function substituteIdentifiers(e,r){if(e instanceof ManifestNumber)return e;if("string"==typeof e)return e in r?r[e]:e;const n=[];for(const s of e[1])n.push(substituteIdentifiers(s,r));return[e[0],n]}function exprNodeToString(e){if(e instanceof ManifestNumber){let r="";return"floatAbstract"===e.type?(r=e.value.toString(),-1===r.indexOf(".")&&(r+="f")):r=e.value.toString(),r}if("string"==typeof e)return e;switch(e[0]){case"+":return`(${exprNodeToString(e[1][0])} + ${exprNodeToString(e[1][1])})`;case"-":return`(${exprNodeToString(e[1][0])} - ${exprNodeToString(e[1][1])})`;case"*":return`(${exprNodeToString(e[1][0])} * ${exprNodeToString(e[1][1])})`;case"/":return`(${exprNodeToString(e[1][0])} / ${exprNodeToString(e[1][1])})`;case"%":return`(${exprNodeToString(e[1][0])} % ${exprNodeToString(e[1][1])})`;case"==":return`(${exprNodeToString(e[1][0])} == ${exprNodeToString(e[1][1])})`;case"!=":return`(${exprNodeToString(e[1][0])} != ${exprNodeToString(e[1][1])})`;case"<":return`(${exprNodeToString(e[1][0])} < ${exprNodeToString(e[1][1])})`;case">":return`(${exprNodeToString(e[1][0])} > ${exprNodeToString(e[1][1])})`;case"<=":return`(${exprNodeToString(e[1][0])} <= ${exprNodeToString(e[1][1])})`;case">=":return`(${exprNodeToString(e[1][0])} >= ${exprNodeToString(e[1][1])})`;case"&&":case"&&":return`(${exprNodeToString(e[1][0])} && ${exprNodeToString(e[1][1])})`;case"||":case"||":return`(${exprNodeToString(e[1][0])} || ${exprNodeToString(e[1][1])})`;case"&":return`(${exprNodeToString(e[1][0])} & ${exprNodeToString(e[1][1])})`;case"|":return`(${exprNodeToString(e[1][0])} | ${exprNodeToString(e[1][1])})`;case"^":return`(${exprNodeToString(e[1][0])} ^ ${exprNodeToString(e[1][1])})`;case"?":return`(${exprNodeToString(e[1][0])} ? ${exprNodeToString(e[1][1])} : ${exprNodeToString(e[1][2])})`;case"apply":return`${exprNodeToString(e[1][0])}(${e[1].slice(1).map(exprNodeToString).join(", ")})`;case"block":return`{ ${e[1].map(exprNodeToString).join("; ")} }`;case"assign":const r=e[1][0],n=e[1][1];return`${exprNodeToString(r)} = ${exprNodeToString(n)}`;case"if":const s=e[1][0],a=e[1][1],i=e[1][2];return`if (${exprNodeToString(s)}) { ${exprNodeToString(a)} } else { ${exprNodeToString(i)} }`;case"negate":return`(-${exprNodeToString(e[1][0])})`;case"statements":return e[1].map(exprNodeToString).join("; ");case"return":return`return ${exprNodeToString(e[1][0])}`;case"var":return`var ${e[1][0]} = ${exprNodeToString(e[1][1])}`;default:throw new Error(`Unknown AST node type when printing: ${e[0]}`)}}function exprNodeToWebGLShader(e){return exprNodeToString(e=substitute(e,(e=>e instanceof Array&&"?"===e[0]),(e=>{const r=e[1][0],n=e[1][1];return["apply",["select",e[1][2],n,r]]})))}r.ManifestNumber=ManifestNumber,r.parseCode=parseCode,r.substitute=substitute,r.substituteIdentifiers=substituteIdentifiers,r.exprNodeToString=exprNodeToString,r.exprNodeToWebGLShader=exprNodeToWebGLShader,r.exprCodeToWebGLShader=function exprCodeToWebGLShader(e,r){let n=parseCode(e);return r&&(n=substituteIdentifiers(n,r)),exprNodeToWebGLShader(n)};const n=0,s=1,a=2,i=3,o=4,u=5,p=6,d=7,h=8;function compileCode(e){if("number"==typeof e)return r=>e;const r=parseCode(e),c=[];return function emit(e){if(e instanceof ManifestNumber)c.push([n,e.value]);else if("string"==typeof e)c.push([a,e]);else if("+"===e[0])emit(e[1][0]),emit(e[1][1]),c.push([o,null]);else if("-"===e[0])emit(e[1][0]),emit(e[1][1]),c.push([p,null]);else if("*"===e[0])emit(e[1][0]),emit(e[1][1]),c.push([d,null]);else if("/"===e[0])emit(e[1][0]),emit(e[1][1]),c.push([h,null]);else if("apply"===e[0]){const r=e[1][0],n=e[1].slice(1);for(const e of n)emit(e);c.push([u,exprNodeToString(r)])}else if("statements"===e[0]){const r=e[1];let n=0;for(const e of r)emit(e),n<r.length-1&&c.push([s,null]),n++}else{if("var"!==e[0])throw new Error(`Unknown compile node type ${e}`);{const r=e[1][0];emit(e[1][1]),c.push([i,exprNodeToString(r)])}}}(r),e=>{const r=[];for(const l of c){const c=l[0];if(c===n)r.push(l[1]);else if(c===s)r.pop();else if(c==a)r.push(e[l[1]]);else if(c==i){const n=r.pop();e[l[1]]=n,r.push(n)}else if(c==u){const n=e[l[1]];if("function"!=typeof n)throw new Error(`Expected function for "${l[1]}", got ${n}`);const s=[];for(let e=0;e<n.length;e++)s.push(r.pop());s.reverse(),r.push(n(...s))}else{const e=r.pop(),n=r.pop();switch(c){case o:r.push(n+e);break;case p:r.push(n-e);break;case d:r.push(n*e);break;case h:r.push(n/e);break;default:throw new Error(`Unknown op ${c}`)}}}return r[0]}}r.compileCode=compileCode,r.evalCode=function evalCode(e,r){return"number"==typeof e?e:compileCode(e)(r)}},882:(e,r,n)=>{Object.defineProperty(r,"__esModule",{value:!0}),r.zeros=r.ones=r.empty=void 0;const s=n(81),a=n(459),i=n(320),o=n(967);function initTensor(e,r,n,u){const p=(0,a.getDevice)(n),d=(0,s.getShape)(e),h=(0,i.getDtype)(r),c=p.initStorage(d,h,u);return new o.Tensor({data:c,dtype:h,shape:d,strides:(0,s.defaultStrides)(d),device:p})}r.empty=function empty(e,r,n){return initTensor(e,r,n,(e=>{}))},r.ones=function ones(e,r,n){return initTensor(e,r,n,(e=>{e.fill(1)}))},r.zeros=function zeros(e,r,n){return initTensor(e,r,n,(e=>{e.fill(0)}))}},25:(e,r,n)=>{Object.defineProperty(r,"__esModule",{value:!0}),r.LinearFunction=r.GatherFunction=void 0;const s=n(149),a=n(500);class GatherFunction extends s.AutoFunction{static forward(e){let[r,n,s]=e;n<0&&(n+=r.shape.length);const a=r.shape.length;if(s.shape.length!==a)throw new Error(`Index shape ${s.shape} does not match input shape ${r.shape}`);const i=s.shape.slice();return r.runKernel("gather",{dtype:r.dtype},{dim:n},[i],s)[0]}static setupContext(e,r,n){const[s,a,i]=r;e.saveForBackward(s,i),e.dim=a}static backward(e,r){const[n,s]=e.savedTensors;e.dim;throw new Error("Gather backward not implemented")}}r.GatherFunction=GatherFunction;class LinearFunction extends s.AutoFunction{static forward(e){const[r,n,s]=e,i=(0,a.matmul)(r,n.t());return s&&i.add_(s),i}static setupContext(e,r,n){const[s,a,i]=r;e.saveForBackward(s,a,i)}static backward(e,r){const[n,s,i]=e.savedTensors;let o=null,u=null,p=null;return e.needsInputGradient[0]&&(o=(0,a.matmul)(r,s)),e.needsInputGradient[1]&&(u=(0,a.matmul)(r.t(),n)),e.needsInputGradient[2]&&(p=function sumTo(e,r){const n=e.shape,s=[],a=n.length-r.length;for(let e=0;e<a;e++)s.push(e);for(let e=a;e<n.length;e++)1===r[e-a]&&1!==n[e]&&s.push(e);return s.length>0&&(e=e.sum(s,!0)),a>0?e.view(r):e}(r,i.shape)),[o,u,p]}}r.LinearFunction=LinearFunction},869:(e,r,n)=>{Object.defineProperty(r,"__esModule",{value:!0}),r.AllFunction=r.XlogyFunction=r.TruncFunction=r.TanhFunction=r.TanFunction=r.SubFunction=r.SquareFunction=r.SqrtFunction=r.SinhFunction=r.SincFunction=r.SinFunction=r.SiluFunction=r.SignFunction=r.SigmoidFunction=r.RsqrtFunction=r.RoundFunction=r.ReluFunction=r.ReciprocalFunction=r.Rad2degFunction=r.PowFunction=r.PositiveFunction=r.NegFunction=r.MulFunction=r.Logaddexp2Function=r.LogaddexpFunction=r.Log2Function=r.Log1pFunction=r.Log10Function=r.LogFunction=r.LdexpFunction=r.HypotFunction=r.FracFunction=r.FloorFunction=r.Expm1Function=r.Exp2Function=r.ExpFunction=r.DivFunction=r.Deg2radFunction=r.CoshFunction=r.CosFunction=r.CopysignFunction=r.CeilFunction=r.Atan2Function=r.AtanFunction=r.AsinhFunction=r.AsinFunction=r.AddFunction=r.AcoshFunction=r.AcosFunction=r.AbsFunction=void 0,r.CountNonzeroFunction=r.SumFunction=r.ProdFunction=r.NormFunction=r.MeanFunction=r.AnyFunction=void 0;const s=n(149),a=n(81);class AbsFunction extends s.AutoFunction{static forward(e){const[r]=e,n={size:(0,a.shapeSize)(r.shape)};return r.runKernel("abs",{dtype:"float32"},n,[r.shape])[0]}static setupContext(e,r,n){const[s]=r;e.saveForBackward(s)}static backward(e,r){const[n]=e.savedTensors,s={size:(0,a.shapeSize)(n.shape)};return n.runKernel("abs_grad",{dtype:"float32"},s,[n.shape],r)}}r.AbsFunction=AbsFunction;class AcosFunction extends s.AutoFunction{static forward(e){const[r]=e,n={size:(0,a.shapeSize)(r.shape)};return r.runKernel("acos",{dtype:"float32"},n,[r.shape])[0]}static setupContext(e,r,n){const[s]=r;e.saveForBackward(s)}static backward(e,r){const[n]=e.savedTensors,s={size:(0,a.shapeSize)(n.shape)};return n.runKernel("acos_grad",{dtype:"float32"},s,[n.shape],r)}}r.AcosFunction=AcosFunction;class AcoshFunction extends s.AutoFunction{static forward(e){const[r]=e,n={size:(0,a.shapeSize)(r.shape)};return r.runKernel("acosh",{dtype:"float32"},n,[r.shape])[0]}static setupContext(e,r,n){const[s]=r;e.saveForBackward(s)}static backward(e,r){const[n]=e.savedTensors,s={size:(0,a.shapeSize)(n.shape)};return n.runKernel("acosh_grad",{dtype:"float32"},s,[n.shape],r)}}r.AcoshFunction=AcoshFunction;class AddFunction extends s.AutoFunction{static forward(e){const[r,n,s]=e;if("number"==typeof n){const e={size:(0,a.shapeSize)(r.shape),other:n,alpha:s||1};return r.runKernel("add_scalar",{dtype:"float32"},e,[r.shape])[0]}{const e=(0,a.broadcastShapes)(r,n);if((0,a.stridedShapeIsContiguous)(e.a)&&(0,a.stridedShapeIsContiguous)(e.b)){if((0,a.shapeSize)(r.shape)!==(0,a.shapeSize)(n.shape))throw new Error(`Shape sizes must match. Got ${r.shape} and ${n.shape}`);const e={size:(0,a.shapeSize)(r.shape),alpha:s||1};return r.runKernel("add",{dtype:"float32"},e,[r.shape],n)[0]}{const i=e.a.shape.length,o=e.b.shape.length;if(i>4||o>4)throw new Error("Broadcasting not supported for tensors with more than 4 dimensions");const u={inputStrides0:i>0?e.a.strides[0]:0,otherStrides0:o>0?e.b.strides[0]:0,outputStrides0:e.output.shape.length>0?e.output.strides[0]:1,inputStrides1:i>1?e.a.strides[1]:0,otherStrides1:o>1?e.b.strides[1]:0,outputStrides1:e.output.shape.length>1?e.output.strides[1]:1,inputStrides2:i>2?e.a.strides[2]:0,otherStrides2:o>2?e.b.strides[2]:0,outputStrides2:e.output.shape.length>2?e.output.strides[2]:1,inputStrides3:i>3?e.a.strides[3]:0,otherStrides3:o>3?e.b.strides[3]:0,outputStrides3:e.output.shape.length>3?e.output.strides[3]:1,size:(0,a.shapeSize)(e.output.shape),alpha:s||1};return r.runKernel("add_strided",{dtype:"float32"},u,[e.output.shape],n)[0]}}}static setupContext(e,r,n){const[s,a,i]=r;e.alpha=i,e.saveForBackward(s,a)}static backward(e,r){const[n,s]=e.savedTensors;if("number"==typeof s){const i={size:(0,a.shapeSize)(n.shape),other:s,alpha:e.alpha||1};return n.runKernel("add_scalar_grad",{dtype:"float32"},i,[n.shape],r)}{const i={size:(0,a.shapeSize)(n.shape),alpha:e.alpha||1};return n.runKernel("add_grad",{dtype:"float32"},i,[n.shape,s.shape],s,r)}}}r.AddFunction=AddFunction;class AsinFunction extends s.AutoFunction{static forward(e){const[r]=e,n={size:(0,a.shapeSize)(r.shape)};return r.runKernel("asin",{dtype:"float32"},n,[r.shape])[0]}static setupContext(e,r,n){const[s]=r;e.saveForBackward(s)}static backward(e,r){const[n]=e.savedTensors,s={size:(0,a.shapeSize)(n.shape)};return n.runKernel("asin_grad",{dtype:"float32"},s,[n.shape],r)}}r.AsinFunction=AsinFunction;class AsinhFunction extends s.AutoFunction{static forward(e){const[r]=e,n={size:(0,a.shapeSize)(r.shape)};return r.runKernel("asinh",{dtype:"float32"},n,[r.shape])[0]}static setupContext(e,r,n){const[s]=r;e.saveForBackward(s)}static backward(e,r){const[n]=e.savedTensors,s={size:(0,a.shapeSize)(n.shape)};return n.runKernel("asinh_grad",{dtype:"float32"},s,[n.shape],r)}}r.AsinhFunction=AsinhFunction;class AtanFunction extends s.AutoFunction{static forward(e){const[r]=e,n={size:(0,a.shapeSize)(r.shape)};return r.runKernel("atan",{dtype:"float32"},n,[r.shape])[0]}static setupContext(e,r,n){const[s]=r;e.saveForBackward(s)}static backward(e,r){const[n]=e.savedTensors,s={size:(0,a.shapeSize)(n.shape)};return n.runKernel("atan_grad",{dtype:"float32"},s,[n.shape],r)}}r.AtanFunction=AtanFunction;class Atan2Function extends s.AutoFunction{static forward(e){const[r,n]=e;if("number"==typeof n){const e={size:(0,a.shapeSize)(r.shape),other:n};return r.runKernel("atan2_scalar",{dtype:"float32"},e,[r.shape])[0]}{const e=(0,a.broadcastShapes)(r,n);if((0,a.stridedShapeIsContiguous)(e.a)&&(0,a.stridedShapeIsContiguous)(e.b)){if((0,a.shapeSize)(r.shape)!==(0,a.shapeSize)(n.shape))throw new Error(`Shape sizes must match. Got ${r.shape} and ${n.shape}`);const e={size:(0,a.shapeSize)(r.shape)};return r.runKernel("atan2",{dtype:"float32"},e,[r.shape],n)[0]}{const s=e.a.shape.length,i=e.b.shape.length;if(s>4||i>4)throw new Error("Broadcasting not supported for tensors with more than 4 dimensions");const o={inputStrides0:s>0?e.a.strides[0]:0,otherStrides0:i>0?e.b.strides[0]:0,outputStrides0:e.output.shape.length>0?e.output.strides[0]:1,inputStrides1:s>1?e.a.strides[1]:0,otherStrides1:i>1?e.b.strides[1]:0,outputStrides1:e.output.shape.length>1?e.output.strides[1]:1,inputStrides2:s>2?e.a.strides[2]:0,otherStrides2:i>2?e.b.strides[2]:0,outputStrides2:e.output.shape.length>2?e.output.strides[2]:1,inputStrides3:s>3?e.a.strides[3]:0,otherStrides3:i>3?e.b.strides[3]:0,outputStrides3:e.output.shape.length>3?e.output.strides[3]:1,size:(0,a.shapeSize)(e.output.shape)};return r.runKernel("atan2_strided",{dtype:"float32"},o,[e.output.shape],n)[0]}}}static setupContext(e,r,n){const[s,a]=r;e.saveForBackward(s,a)}static backward(e,r){const[n,s]=e.savedTensors;if("number"==typeof s){const e={size:(0,a.shapeSize)(n.shape),other:s};return n.runKernel("atan2_scalar_grad",{dtype:"float32"},e,[n.shape],r)}{const e={size:(0,a.shapeSize)(n.shape)};return n.runKernel("atan2_grad",{dtype:"float32"},e,[n.shape,s.shape],s,r)}}}r.Atan2Function=Atan2Function;class CeilFunction extends s.AutoFunction{static forward(e){const[r]=e,n={size:(0,a.shapeSize)(r.shape)};return r.runKernel("ceil",{dtype:"float32"},n,[r.shape])[0]}static setupContext(e,r,n){const[s]=r;e.saveForBackward(s)}static backward(e,r){const[n]=e.savedTensors,s={size:(0,a.shapeSize)(n.shape)};return n.runKernel("ceil_grad",{dtype:"float32"},s,[n.shape],r)}}r.CeilFunction=CeilFunction;class CopysignFunction extends s.AutoFunction{static forward(e){const[r,n]=e;if("number"==typeof n){const e={size:(0,a.shapeSize)(r.shape),other:n};return r.runKernel("copysign_scalar",{dtype:"float32"},e,[r.shape])[0]}{const e=(0,a.broadcastShapes)(r,n);if((0,a.stridedShapeIsContiguous)(e.a)&&(0,a.stridedShapeIsContiguous)(e.b)){if((0,a.shapeSize)(r.shape)!==(0,a.shapeSize)(n.shape))throw new Error(`Shape sizes must match. Got ${r.shape} and ${n.shape}`);const e={size:(0,a.shapeSize)(r.shape)};return r.runKernel("copysign",{dtype:"float32"},e,[r.shape],n)[0]}{const s=e.a.shape.length,i=e.b.shape.length;if(s>4||i>4)throw new Error("Broadcasting not supported for tensors with more than 4 dimensions");const o={inputStrides0:s>0?e.a.strides[0]:0,otherStrides0:i>0?e.b.strides[0]:0,outputStrides0:e.output.shape.length>0?e.output.strides[0]:1,inputStrides1:s>1?e.a.strides[1]:0,otherStrides1:i>1?e.b.strides[1]:0,outputStrides1:e.output.shape.length>1?e.output.strides[1]:1,inputStrides2:s>2?e.a.strides[2]:0,otherStrides2:i>2?e.b.strides[2]:0,outputStrides2:e.output.shape.length>2?e.output.strides[2]:1,inputStrides3:s>3?e.a.strides[3]:0,otherStrides3:i>3?e.b.strides[3]:0,outputStrides3:e.output.shape.length>3?e.output.strides[3]:1,size:(0,a.shapeSize)(e.output.shape)};return r.runKernel("copysign_strided",{dtype:"float32"},o,[e.output.shape],n)[0]}}}static setupContext(e,r,n){const[s,a]=r;e.saveForBackward(s,a)}static backward(e,r){const[n,s]=e.savedTensors;if("number"==typeof s){const e={size:(0,a.shapeSize)(n.shape),other:s};return n.runKernel("copysign_scalar_grad",{dtype:"float32"},e,[n.shape],r)}{const e={size:(0,a.shapeSize)(n.shape)};return n.runKernel("copysign_grad",{dtype:"float32"},e,[n.shape,s.shape],s,r)}}}r.CopysignFunction=CopysignFunction;class CosFunction extends s.AutoFunction{static forward(e){const[r]=e,n={size:(0,a.shapeSize)(r.shape)};return r.runKernel("cos",{dtype:"float32"},n,[r.shape])[0]}static setupContext(e,r,n){const[s]=r;e.saveForBackward(s)}static backward(e,r){const[n]=e.savedTensors,s={size:(0,a.shapeSize)(n.shape)};return n.runKernel("cos_grad",{dtype:"float32"},s,[n.shape],r)}}r.CosFunction=CosFunction;class CoshFunction extends s.AutoFunction{static forward(e){const[r]=e,n={size:(0,a.shapeSize)(r.shape)};return r.runKernel("cosh",{dtype:"float32"},n,[r.shape])[0]}static setupContext(e,r,n){const[s]=r;e.saveForBackward(s)}static backward(e,r){const[n]=e.savedTensors,s={size:(0,a.shapeSize)(n.shape)};return n.runKernel("cosh_grad",{dtype:"float32"},s,[n.shape],r)}}r.CoshFunction=CoshFunction;class Deg2radFunction extends s.AutoFunction{static forward(e){const[r]=e,n={size:(0,a.shapeSize)(r.shape)};return r.runKernel("deg2rad",{dtype:"float32"},n,[r.shape])[0]}static setupContext(e,r,n){const[s]=r;e.saveForBackward(s)}static backward(e,r){const[n]=e.savedTensors,s={size:(0,a.shapeSize)(n.shape)};return n.runKernel("deg2rad_grad",{dtype:"float32"},s,[n.shape],r)}}r.Deg2radFunction=Deg2radFunction;class DivFunction extends s.AutoFunction{static forward(e){const[r,n]=e;if("number"==typeof n){const e={size:(0,a.shapeSize)(r.shape),other:n};return r.runKernel("div_scalar",{dtype:"float32"},e,[r.shape])[0]}{const e=(0,a.broadcastShapes)(r,n);if((0,a.stridedShapeIsContiguous)(e.a)&&(0,a.stridedShapeIsContiguous)(e.b)){if((0,a.shapeSize)(r.shape)!==(0,a.shapeSize)(n.shape))throw new Error(`Shape sizes must match. Got ${r.shape} and ${n.shape}`);const e={size:(0,a.shapeSize)(r.shape)};return r.runKernel("div",{dtype:"float32"},e,[r.shape],n)[0]}{const s=e.a.shape.length,i=e.b.shape.length;if(s>4||i>4)throw new Error("Broadcasting not supported for tensors with more than 4 dimensions");const o={inputStrides0:s>0?e.a.strides[0]:0,otherStrides0:i>0?e.b.strides[0]:0,outputStrides0:e.output.shape.length>0?e.output.strides[0]:1,inputStrides1:s>1?e.a.strides[1]:0,otherStrides1:i>1?e.b.strides[1]:0,outputStrides1:e.output.shape.length>1?e.output.strides[1]:1,inputStrides2:s>2?e.a.strides[2]:0,otherStrides2:i>2?e.b.strides[2]:0,outputStrides2:e.output.shape.length>2?e.output.strides[2]:1,inputStrides3:s>3?e.a.strides[3]:0,otherStrides3:i>3?e.b.strides[3]:0,outputStrides3:e.output.shape.length>3?e.output.strides[3]:1,size:(0,a.shapeSize)(e.output.shape)};return r.runKernel("div_strided",{dtype:"float32"},o,[e.output.shape],n)[0]}}}static setupContext(e,r,n){const[s,a]=r;e.saveForBackward(s,a)}static backward(e,r){const[n,s]=e.savedTensors;if("number"==typeof s){const e={size:(0,a.shapeSize)(n.shape),other:s};return n.runKernel("div_scalar_grad",{dtype:"float32"},e,[n.shape],r)}{const e={size:(0,a.shapeSize)(n.shape)};return n.runKernel("div_grad",{dtype:"float32"},e,[n.shape,s.shape],s,r)}}}r.DivFunction=DivFunction;class ExpFunction extends s.AutoFunction{static forward(e){const[r]=e,n={size:(0,a.shapeSize)(r.shape)};return r.runKernel("exp",{dtype:"float32"},n,[r.shape])[0]}static setupContext(e,r,n){const[s]=r;e.saveForBackward(s)}static backward(e,r){const[n]=e.savedTensors,s={size:(0,a.shapeSize)(n.shape)};return n.runKernel("exp_grad",{dtype:"float32"},s,[n.shape],r)}}r.ExpFunction=ExpFunction;class Exp2Function extends s.AutoFunction{static forward(e){const[r]=e,n={size:(0,a.shapeSize)(r.shape)};return r.runKernel("exp2",{dtype:"float32"},n,[r.shape])[0]}static setupContext(e,r,n){const[s]=r;e.saveForBackward(s)}static backward(e,r){const[n]=e.savedTensors,s={size:(0,a.shapeSize)(n.shape)};return n.runKernel("exp2_grad",{dtype:"float32"},s,[n.shape],r)}}r.Exp2Function=Exp2Function;class Expm1Function extends s.AutoFunction{static forward(e){const[r]=e,n={size:(0,a.shapeSize)(r.shape)};return r.runKernel("expm1",{dtype:"float32"},n,[r.shape])[0]}static setupContext(e,r,n){const[s]=r;e.saveForBackward(s)}static backward(e,r){const[n]=e.savedTensors,s={size:(0,a.shapeSize)(n.shape)};return n.runKernel("expm1_grad",{dtype:"float32"},s,[n.shape],r)}}r.Expm1Function=Expm1Function;class FloorFunction extends s.AutoFunction{static forward(e){const[r]=e,n={size:(0,a.shapeSize)(r.shape)};return r.runKernel("floor",{dtype:"float32"},n,[r.shape])[0]}static setupContext(e,r,n){const[s]=r;e.saveForBackward(s)}static backward(e,r){const[n]=e.savedTensors,s={size:(0,a.shapeSize)(n.shape)};return n.runKernel("floor_grad",{dtype:"float32"},s,[n.shape],r)}}r.FloorFunction=FloorFunction;class FracFunction extends s.AutoFunction{static forward(e){const[r]=e,n={size:(0,a.shapeSize)(r.shape)};return r.runKernel("frac",{dtype:"float32"},n,[r.shape])[0]}static setupContext(e,r,n){const[s]=r;e.saveForBackward(s)}static backward(e,r){const[n]=e.savedTensors,s={size:(0,a.shapeSize)(n.shape)};return n.runKernel("frac_grad",{dtype:"float32"},s,[n.shape],r)}}r.FracFunction=FracFunction;class HypotFunction extends s.AutoFunction{static forward(e){const[r,n]=e;if("number"==typeof n){const e={size:(0,a.shapeSize)(r.shape),other:n};return r.runKernel("hypot_scalar",{dtype:"float32"},e,[r.shape])[0]}{const e=(0,a.broadcastShapes)(r,n);if((0,a.stridedShapeIsContiguous)(e.a)&&(0,a.stridedShapeIsContiguous)(e.b)){if((0,a.shapeSize)(r.shape)!==(0,a.shapeSize)(n.shape))throw new Error(`Shape sizes must match. Got ${r.shape} and ${n.shape}`);const e={size:(0,a.shapeSize)(r.shape)};return r.runKernel("hypot",{dtype:"float32"},e,[r.shape],n)[0]}{const s=e.a.shape.length,i=e.b.shape.length;if(s>4||i>4)throw new Error("Broadcasting not supported for tensors with more than 4 dimensions");const o={inputStrides0:s>0?e.a.strides[0]:0,otherStrides0:i>0?e.b.strides[0]:0,outputStrides0:e.output.shape.length>0?e.output.strides[0]:1,inputStrides1:s>1?e.a.strides[1]:0,otherStrides1:i>1?e.b.strides[1]:0,outputStrides1:e.output.shape.length>1?e.output.strides[1]:1,inputStrides2:s>2?e.a.strides[2]:0,otherStrides2:i>2?e.b.strides[2]:0,outputStrides2:e.output.shape.length>2?e.output.strides[2]:1,inputStrides3:s>3?e.a.strides[3]:0,otherStrides3:i>3?e.b.strides[3]:0,outputStrides3:e.output.shape.length>3?e.output.strides[3]:1,size:(0,a.shapeSize)(e.output.shape)};return r.runKernel("hypot_strided",{dtype:"float32"},o,[e.output.shape],n)[0]}}}static setupContext(e,r,n){const[s,a]=r;e.saveForBackward(s,a)}static backward(e,r){const[n,s]=e.savedTensors;if("number"==typeof s){const e={size:(0,a.shapeSize)(n.shape),other:s};return n.runKernel("hypot_scalar_grad",{dtype:"float32"},e,[n.shape],r)}{const e={size:(0,a.shapeSize)(n.shape)};return n.runKernel("hypot_grad",{dtype:"float32"},e,[n.shape,s.shape],s,r)}}}r.HypotFunction=HypotFunction;class LdexpFunction extends s.AutoFunction{static forward(e){const[r,n]=e;if("number"==typeof n){const e={size:(0,a.shapeSize)(r.shape),other:n};return r.runKernel("ldexp_scalar",{dtype:"float32"},e,[r.shape])[0]}{const e=(0,a.broadcastShapes)(r,n);if((0,a.stridedShapeIsContiguous)(e.a)&&(0,a.stridedShapeIsContiguous)(e.b)){if((0,a.shapeSize)(r.shape)!==(0,a.shapeSize)(n.shape))throw new Error(`Shape sizes must match. Got ${r.shape} and ${n.shape}`);const e={size:(0,a.shapeSize)(r.shape)};return r.runKernel("ldexp",{dtype:"float32"},e,[r.shape],n)[0]}{const s=e.a.shape.length,i=e.b.shape.length;if(s>4||i>4)throw new Error("Broadcasting not supported for tensors with more than 4 dimensions");const o={inputStrides0:s>0?e.a.strides[0]:0,otherStrides0:i>0?e.b.strides[0]:0,outputStrides0:e.output.shape.length>0?e.output.strides[0]:1,inputStrides1:s>1?e.a.strides[1]:0,otherStrides1:i>1?e.b.strides[1]:0,outputStrides1:e.output.shape.length>1?e.output.strides[1]:1,inputStrides2:s>2?e.a.strides[2]:0,otherStrides2:i>2?e.b.strides[2]:0,outputStrides2:e.output.shape.length>2?e.output.strides[2]:1,inputStrides3:s>3?e.a.strides[3]:0,otherStrides3:i>3?e.b.strides[3]:0,outputStrides3:e.output.shape.length>3?e.output.strides[3]:1,size:(0,a.shapeSize)(e.output.shape)};return r.runKernel("ldexp_strided",{dtype:"float32"},o,[e.output.shape],n)[0]}}}static setupContext(e,r,n){const[s,a]=r;e.saveForBackward(s,a)}static backward(e,r){const[n,s]=e.savedTensors;if("number"==typeof s){const e={size:(0,a.shapeSize)(n.shape),other:s};return n.runKernel("ldexp_scalar_grad",{dtype:"float32"},e,[n.shape],r)}{const e={size:(0,a.shapeSize)(n.shape)};return n.runKernel("ldexp_grad",{dtype:"float32"},e,[n.shape,s.shape],s,r)}}}r.LdexpFunction=LdexpFunction;class LogFunction extends s.AutoFunction{static forward(e){const[r]=e,n={size:(0,a.shapeSize)(r.shape)};return r.runKernel("log",{dtype:"float32"},n,[r.shape])[0]}static setupContext(e,r,n){const[s]=r;e.saveForBackward(s)}static backward(e,r){const[n]=e.savedTensors,s={size:(0,a.shapeSize)(n.shape)};return n.runKernel("log_grad",{dtype:"float32"},s,[n.shape],r)}}r.LogFunction=LogFunction;class Log10Function extends s.AutoFunction{static forward(e){const[r]=e,n={size:(0,a.shapeSize)(r.shape)};return r.runKernel("log10",{dtype:"float32"},n,[r.shape])[0]}static setupContext(e,r,n){const[s]=r;e.saveForBackward(s)}static backward(e,r){const[n]=e.savedTensors,s={size:(0,a.shapeSize)(n.shape)};return n.runKernel("log10_grad",{dtype:"float32"},s,[n.shape],r)}}r.Log10Function=Log10Function;class Log1pFunction extends s.AutoFunction{static forward(e){const[r]=e,n={size:(0,a.shapeSize)(r.shape)};return r.runKernel("log1p",{dtype:"float32"},n,[r.shape])[0]}static setupContext(e,r,n){const[s]=r;e.saveForBackward(s)}static backward(e,r){const[n]=e.savedTensors,s={size:(0,a.shapeSize)(n.shape)};return n.runKernel("log1p_grad",{dtype:"float32"},s,[n.shape],r)}}r.Log1pFunction=Log1pFunction;class Log2Function extends s.AutoFunction{static forward(e){const[r]=e,n={size:(0,a.shapeSize)(r.shape)};return r.runKernel("log2",{dtype:"float32"},n,[r.shape])[0]}static setupContext(e,r,n){const[s]=r;e.saveForBackward(s)}static backward(e,r){const[n]=e.savedTensors,s={size:(0,a.shapeSize)(n.shape)};return n.runKernel("log2_grad",{dtype:"float32"},s,[n.shape],r)}}r.Log2Function=Log2Function;class LogaddexpFunction extends s.AutoFunction{static forward(e){const[r,n]=e;if("number"==typeof n){const e={size:(0,a.shapeSize)(r.shape),other:n};return r.runKernel("logaddexp_scalar",{dtype:"float32"},e,[r.shape])[0]}{const e=(0,a.broadcastShapes)(r,n);if((0,a.stridedShapeIsContiguous)(e.a)&&(0,a.stridedShapeIsContiguous)(e.b)){if((0,a.shapeSize)(r.shape)!==(0,a.shapeSize)(n.shape))throw new Error(`Shape sizes must match. Got ${r.shape} and ${n.shape}`);const e={size:(0,a.shapeSize)(r.shape)};return r.runKernel("logaddexp",{dtype:"float32"},e,[r.shape],n)[0]}{const s=e.a.shape.length,i=e.b.shape.length;if(s>4||i>4)throw new Error("Broadcasting not supported for tensors with more than 4 dimensions");const o={inputStrides0:s>0?e.a.strides[0]:0,otherStrides0:i>0?e.b.strides[0]:0,outputStrides0:e.output.shape.length>0?e.output.strides[0]:1,inputStrides1:s>1?e.a.strides[1]:0,otherStrides1:i>1?e.b.strides[1]:0,outputStrides1:e.output.shape.length>1?e.output.strides[1]:1,inputStrides2:s>2?e.a.strides[2]:0,otherStrides2:i>2?e.b.strides[2]:0,outputStrides2:e.output.shape.length>2?e.output.strides[2]:1,inputStrides3:s>3?e.a.strides[3]:0,otherStrides3:i>3?e.b.strides[3]:0,outputStrides3:e.output.shape.length>3?e.output.strides[3]:1,size:(0,a.shapeSize)(e.output.shape)};return r.runKernel("logaddexp_strided",{dtype:"float32"},o,[e.output.shape],n)[0]}}}static setupContext(e,r,n){const[s,a]=r;e.saveForBackward(s,a)}static backward(e,r){const[n,s]=e.savedTensors;if("number"==typeof s){const e={size:(0,a.shapeSize)(n.shape),other:s};return n.runKernel("logaddexp_scalar_grad",{dtype:"float32"},e,[n.shape],r)}{const e={size:(0,a.shapeSize)(n.shape)};return n.runKernel("logaddexp_grad",{dtype:"float32"},e,[n.shape,s.shape],s,r)}}}r.LogaddexpFunction=LogaddexpFunction;class Logaddexp2Function extends s.AutoFunction{static forward(e){const[r,n]=e;if("number"==typeof n){const e={size:(0,a.shapeSize)(r.shape),other:n};return r.runKernel("logaddexp2_scalar",{dtype:"float32"},e,[r.shape])[0]}{const e=(0,a.broadcastShapes)(r,n);if((0,a.stridedShapeIsContiguous)(e.a)&&(0,a.stridedShapeIsContiguous)(e.b)){if((0,a.shapeSize)(r.shape)!==(0,a.shapeSize)(n.shape))throw new Error(`Shape sizes must match. Got ${r.shape} and ${n.shape}`);const e={size:(0,a.shapeSize)(r.shape)};return r.runKernel("logaddexp2",{dtype:"float32"},e,[r.shape],n)[0]}{const s=e.a.shape.length,i=e.b.shape.length;if(s>4||i>4)throw new Error("Broadcasting not supported for tensors with more than 4 dimensions");const o={inputStrides0:s>0?e.a.strides[0]:0,otherStrides0:i>0?e.b.strides[0]:0,outputStrides0:e.output.shape.length>0?e.output.strides[0]:1,inputStrides1:s>1?e.a.strides[1]:0,otherStrides1:i>1?e.b.strides[1]:0,outputStrides1:e.output.shape.length>1?e.output.strides[1]:1,inputStrides2:s>2?e.a.strides[2]:0,otherStrides2:i>2?e.b.strides[2]:0,outputStrides2:e.output.shape.length>2?e.output.strides[2]:1,inputStrides3:s>3?e.a.strides[3]:0,otherStrides3:i>3?e.b.strides[3]:0,outputStrides3:e.output.shape.length>3?e.output.strides[3]:1,size:(0,a.shapeSize)(e.output.shape)};return r.runKernel("logaddexp2_strided",{dtype:"float32"},o,[e.output.shape],n)[0]}}}static setupContext(e,r,n){const[s,a]=r;e.saveForBackward(s,a)}static backward(e,r){const[n,s]=e.savedTensors;if("number"==typeof s){const e={size:(0,a.shapeSize)(n.shape),other:s};return n.runKernel("logaddexp2_scalar_grad",{dtype:"float32"},e,[n.shape],r)}{const e={size:(0,a.shapeSize)(n.shape)};return n.runKernel("logaddexp2_grad",{dtype:"float32"},e,[n.shape,s.shape],s,r)}}}r.Logaddexp2Function=Logaddexp2Function;class MulFunction extends s.AutoFunction{static forward(e){const[r,n]=e;if("number"==typeof n){const e={size:(0,a.shapeSize)(r.shape),other:n};return r.runKernel("mul_scalar",{dtype:"float32"},e,[r.shape])[0]}{const e=(0,a.broadcastShapes)(r,n);if((0,a.stridedShapeIsContiguous)(e.a)&&(0,a.stridedShapeIsContiguous)(e.b)){if((0,a.shapeSize)(r.shape)!==(0,a.shapeSize)(n.shape))throw new Error(`Shape sizes must match. Got ${r.shape} and ${n.shape}`);const e={size:(0,a.shapeSize)(r.shape)};return r.runKernel("mul",{dtype:"float32"},e,[r.shape],n)[0]}{const s=e.a.shape.length,i=e.b.shape.length;if(s>4||i>4)throw new Error("Broadcasting not supported for tensors with more than 4 dimensions");const o={inputStrides0:s>0?e.a.strides[0]:0,otherStrides0:i>0?e.b.strides[0]:0,outputStrides0:e.output.shape.length>0?e.output.strides[0]:1,inputStrides1:s>1?e.a.strides[1]:0,otherStrides1:i>1?e.b.strides[1]:0,outputStrides1:e.output.shape.length>1?e.output.strides[1]:1,inputStrides2:s>2?e.a.strides[2]:0,otherStrides2:i>2?e.b.strides[2]:0,outputStrides2:e.output.shape.length>2?e.output.strides[2]:1,inputStrides3:s>3?e.a.strides[3]:0,otherStrides3:i>3?e.b.strides[3]:0,outputStrides3:e.output.shape.length>3?e.output.strides[3]:1,size:(0,a.shapeSize)(e.output.shape)};return r.runKernel("mul_strided",{dtype:"float32"},o,[e.output.shape],n)[0]}}}static setupContext(e,r,n){const[s,a]=r;e.saveForBackward(s,a)}static backward(e,r){const[n,s]=e.savedTensors;if("number"==typeof s){const e={size:(0,a.shapeSize)(n.shape),other:s};return n.runKernel("mul_scalar_grad",{dtype:"float32"},e,[n.shape],r)}{const e={size:(0,a.shapeSize)(n.shape)};return n.runKernel("mul_grad",{dtype:"float32"},e,[n.shape,s.shape],s,r)}}}r.MulFunction=MulFunction;class NegFunction extends s.AutoFunction{static forward(e){const[r]=e,n={size:(0,a.shapeSize)(r.shape)};return r.runKernel("neg",{dtype:"float32"},n,[r.shape])[0]}static setupContext(e,r,n){const[s]=r;e.saveForBackward(s)}static backward(e,r){const[n]=e.savedTensors,s={size:(0,a.shapeSize)(n.shape)};return n.runKernel("neg_grad",{dtype:"float32"},s,[n.shape],r)}}r.NegFunction=NegFunction;class PositiveFunction extends s.AutoFunction{static forward(e){const[r]=e,n={size:(0,a.shapeSize)(r.shape)};return r.runKernel("positive",{dtype:"float32"},n,[r.shape])[0]}static setupContext(e,r,n){const[s]=r;e.saveForBackward(s)}static backward(e,r){const[n]=e.savedTensors,s={size:(0,a.shapeSize)(n.shape)};return n.runKernel("positive_grad",{dtype:"float32"},s,[n.shape],r)}}r.PositiveFunction=PositiveFunction;class PowFunction extends s.AutoFunction{static forward(e){const[r,n]=e;if("number"==typeof n){const e={size:(0,a.shapeSize)(r.shape),other:n};return r.runKernel("pow_scalar",{dtype:"float32"},e,[r.shape])[0]}{const e=(0,a.broadcastShapes)(r,n);if((0,a.stridedShapeIsContiguous)(e.a)&&(0,a.stridedShapeIsContiguous)(e.b)){if((0,a.shapeSize)(r.shape)!==(0,a.shapeSize)(n.shape))throw new Error(`Shape sizes must match. Got ${r.shape} and ${n.shape}`);const e={size:(0,a.shapeSize)(r.shape)};return r.runKernel("pow",{dtype:"float32"},e,[r.shape],n)[0]}{const s=e.a.shape.length,i=e.b.shape.length;if(s>4||i>4)throw new Error("Broadcasting not supported for tensors with more than 4 dimensions");const o={inputStrides0:s>0?e.a.strides[0]:0,otherStrides0:i>0?e.b.strides[0]:0,outputStrides0:e.output.shape.length>0?e.output.strides[0]:1,inputStrides1:s>1?e.a.strides[1]:0,otherStrides1:i>1?e.b.strides[1]:0,outputStrides1:e.output.shape.length>1?e.output.strides[1]:1,inputStrides2:s>2?e.a.strides[2]:0,otherStrides2:i>2?e.b.strides[2]:0,outputStrides2:e.output.shape.length>2?e.output.strides[2]:1,inputStrides3:s>3?e.a.strides[3]:0,otherStrides3:i>3?e.b.strides[3]:0,outputStrides3:e.output.shape.length>3?e.output.strides[3]:1,size:(0,a.shapeSize)(e.output.shape)};return r.runKernel("pow_strided",{dtype:"float32"},o,[e.output.shape],n)[0]}}}static setupContext(e,r,n){const[s,a]=r;e.saveForBackward(s,a)}static backward(e,r){const[n,s]=e.savedTensors;if("number"==typeof s){const e={size:(0,a.shapeSize)(n.shape),other:s};return n.runKernel("pow_scalar_grad",{dtype:"float32"},e,[n.shape],r)}{const e={size:(0,a.shapeSize)(n.shape)};return n.runKernel("pow_grad",{dtype:"float32"},e,[n.shape,s.shape],s,r)}}}r.PowFunction=PowFunction;class Rad2degFunction extends s.AutoFunction{static forward(e){const[r]=e,n={size:(0,a.shapeSize)(r.shape)};return r.runKernel("rad2deg",{dtype:"float32"},n,[r.shape])[0]}static setupContext(e,r,n){const[s]=r;e.saveForBackward(s)}static backward(e,r){const[n]=e.savedTensors,s={size:(0,a.shapeSize)(n.shape)};return n.runKernel("rad2deg_grad",{dtype:"float32"},s,[n.shape],r)}}r.Rad2degFunction=Rad2degFunction;class ReciprocalFunction extends s.AutoFunction{static forward(e){const[r]=e,n={size:(0,a.shapeSize)(r.shape)};return r.runKernel("reciprocal",{dtype:"float32"},n,[r.shape])[0]}static setupContext(e,r,n){const[s]=r;e.saveForBackward(s)}static backward(e,r){const[n]=e.savedTensors,s={size:(0,a.shapeSize)(n.shape)};return n.runKernel("reciprocal_grad",{dtype:"float32"},s,[n.shape],r)}}r.ReciprocalFunction=ReciprocalFunction;class ReluFunction extends s.AutoFunction{static forward(e){const[r]=e,n={size:(0,a.shapeSize)(r.shape)};return r.runKernel("relu",{dtype:"float32"},n,[r.shape])[0]}static setupContext(e,r,n){const[s]=r;e.saveForBackward(s)}static backward(e,r){const[n]=e.savedTensors,s={size:(0,a.shapeSize)(n.shape)};return n.runKernel("relu_grad",{dtype:"float32"},s,[n.shape],r)}}r.ReluFunction=ReluFunction;class RoundFunction extends s.AutoFunction{static forward(e){const[r]=e,n={size:(0,a.shapeSize)(r.shape)};return r.runKernel("round",{dtype:"float32"},n,[r.shape])[0]}static setupContext(e,r,n){const[s]=r;e.saveForBackward(s)}static backward(e,r){const[n]=e.savedTensors,s={size:(0,a.shapeSize)(n.shape)};return n.runKernel("round_grad",{dtype:"float32"},s,[n.shape],r)}}r.RoundFunction=RoundFunction;class RsqrtFunction extends s.AutoFunction{static forward(e){const[r]=e,n={size:(0,a.shapeSize)(r.shape)};return r.runKernel("rsqrt",{dtype:"float32"},n,[r.shape])[0]}static setupContext(e,r,n){const[s]=r;e.saveForBackward(s)}static backward(e,r){const[n]=e.savedTensors,s={size:(0,a.shapeSize)(n.shape)};return n.runKernel("rsqrt_grad",{dtype:"float32"},s,[n.shape],r)}}r.RsqrtFunction=RsqrtFunction;class SigmoidFunction extends s.AutoFunction{static forward(e){const[r]=e,n={size:(0,a.shapeSize)(r.shape)};return r.runKernel("sigmoid",{dtype:"float32"},n,[r.shape])[0]}static setupContext(e,r,n){const[s]=r;e.saveForBackward(s)}static backward(e,r){const[n]=e.savedTensors,s={size:(0,a.shapeSize)(n.shape)};return n.runKernel("sigmoid_grad",{dtype:"float32"},s,[n.shape],r)}}r.SigmoidFunction=SigmoidFunction;class SignFunction extends s.AutoFunction{static forward(e){const[r]=e,n={size:(0,a.shapeSize)(r.shape)};return r.runKernel("sign",{dtype:"float32"},n,[r.shape])[0]}static setupContext(e,r,n){const[s]=r;e.saveForBackward(s)}static backward(e,r){const[n]=e.savedTensors,s={size:(0,a.shapeSize)(n.shape)};return n.runKernel("sign_grad",{dtype:"float32"},s,[n.shape],r)}}r.SignFunction=SignFunction;class SiluFunction extends s.AutoFunction{static forward(e){const[r]=e,n={size:(0,a.shapeSize)(r.shape)};return r.runKernel("silu",{dtype:"float32"},n,[r.shape])[0]}static setupContext(e,r,n){const[s]=r;e.saveForBackward(s)}static backward(e,r){const[n]=e.savedTensors,s={size:(0,a.shapeSize)(n.shape)};return n.runKernel("silu_grad",{dtype:"float32"},s,[n.shape],r)}}r.SiluFunction=SiluFunction;class SinFunction extends s.AutoFunction{static forward(e){const[r]=e,n={size:(0,a.shapeSize)(r.shape)};return r.runKernel("sin",{dtype:"float32"},n,[r.shape])[0]}static setupContext(e,r,n){const[s]=r;e.saveForBackward(s)}static backward(e,r){const[n]=e.savedTensors,s={size:(0,a.shapeSize)(n.shape)};return n.runKernel("sin_grad",{dtype:"float32"},s,[n.shape],r)}}r.SinFunction=SinFunction;class SincFunction extends s.AutoFunction{static forward(e){const[r]=e,n={size:(0,a.shapeSize)(r.shape)};return r.runKernel("sinc",{dtype:"float32"},n,[r.shape])[0]}static setupContext(e,r,n){const[s]=r;e.saveForBackward(s)}static backward(e,r){const[n]=e.savedTensors,s={size:(0,a.shapeSize)(n.shape)};return n.runKernel("sinc_grad",{dtype:"float32"},s,[n.shape],r)}}r.SincFunction=SincFunction;class SinhFunction extends s.AutoFunction{static forward(e){const[r]=e,n={size:(0,a.shapeSize)(r.shape)};return r.runKernel("sinh",{dtype:"float32"},n,[r.shape])[0]}static setupContext(e,r,n){const[s]=r;e.saveForBackward(s)}static backward(e,r){const[n]=e.savedTensors,s={size:(0,a.shapeSize)(n.shape)};return n.runKernel("sinh_grad",{dtype:"float32"},s,[n.shape],r)}}r.SinhFunction=SinhFunction;class SqrtFunction extends s.AutoFunction{static forward(e){const[r]=e,n={size:(0,a.shapeSize)(r.shape)};return r.runKernel("sqrt",{dtype:"float32"},n,[r.shape])[0]}static setupContext(e,r,n){const[s]=r;e.saveForBackward(s)}static backward(e,r){const[n]=e.savedTensors,s={size:(0,a.shapeSize)(n.shape)};return n.runKernel("sqrt_grad",{dtype:"float32"},s,[n.shape],r)}}r.SqrtFunction=SqrtFunction;class SquareFunction extends s.AutoFunction{static forward(e){const[r]=e,n={size:(0,a.shapeSize)(r.shape)};return r.runKernel("square",{dtype:"float32"},n,[r.shape])[0]}static setupContext(e,r,n){const[s]=r;e.saveForBackward(s)}static backward(e,r){const[n]=e.savedTensors,s={size:(0,a.shapeSize)(n.shape)};return n.runKernel("square_grad",{dtype:"float32"},s,[n.shape],r)}}r.SquareFunction=SquareFunction;class SubFunction extends s.AutoFunction{static forward(e){const[r,n,s]=e;if("number"==typeof n){const e={size:(0,a.shapeSize)(r.shape),other:n,alpha:s||1};return r.runKernel("sub_scalar",{dtype:"float32"},e,[r.shape])[0]}{const e=(0,a.broadcastShapes)(r,n);if((0,a.stridedShapeIsContiguous)(e.a)&&(0,a.stridedShapeIsContiguous)(e.b)){if((0,a.shapeSize)(r.shape)!==(0,a.shapeSize)(n.shape))throw new Error(`Shape sizes must match. Got ${r.shape} and ${n.shape}`);const e={size:(0,a.shapeSize)(r.shape),alpha:s||1};return r.runKernel("sub",{dtype:"float32"},e,[r.shape],n)[0]}{const i=e.a.shape.length,o=e.b.shape.length;if(i>4||o>4)throw new Error("Broadcasting not supported for tensors with more than 4 dimensions");const u={inputStrides0:i>0?e.a.strides[0]:0,otherStrides0:o>0?e.b.strides[0]:0,outputStrides0:e.output.shape.length>0?e.output.strides[0]:1,inputStrides1:i>1?e.a.strides[1]:0,otherStrides1:o>1?e.b.strides[1]:0,outputStrides1:e.output.shape.length>1?e.output.strides[1]:1,inputStrides2:i>2?e.a.strides[2]:0,otherStrides2:o>2?e.b.strides[2]:0,outputStrides2:e.output.shape.length>2?e.output.strides[2]:1,inputStrides3:i>3?e.a.strides[3]:0,otherStrides3:o>3?e.b.strides[3]:0,outputStrides3:e.output.shape.length>3?e.output.strides[3]:1,size:(0,a.shapeSize)(e.output.shape),alpha:s||1};return r.runKernel("sub_strided",{dtype:"float32"},u,[e.output.shape],n)[0]}}}static setupContext(e,r,n){const[s,a,i]=r;e.alpha=i,e.saveForBackward(s,a)}static backward(e,r){const[n,s]=e.savedTensors;if("number"==typeof s){const i={size:(0,a.shapeSize)(n.shape),other:s,alpha:e.alpha||1};return n.runKernel("sub_scalar_grad",{dtype:"float32"},i,[n.shape],r)}{const i={size:(0,a.shapeSize)(n.shape),alpha:e.alpha||1};return n.runKernel("sub_grad",{dtype:"float32"},i,[n.shape,s.shape],s,r)}}}r.SubFunction=SubFunction;class TanFunction extends s.AutoFunction{static forward(e){const[r]=e,n={size:(0,a.shapeSize)(r.shape)};return r.runKernel("tan",{dtype:"float32"},n,[r.shape])[0]}static setupContext(e,r,n){const[s]=r;e.saveForBackward(s)}static backward(e,r){const[n]=e.savedTensors,s={size:(0,a.shapeSize)(n.shape)};return n.runKernel("tan_grad",{dtype:"float32"},s,[n.shape],r)}}r.TanFunction=TanFunction;class TanhFunction extends s.AutoFunction{static forward(e){const[r]=e,n={size:(0,a.shapeSize)(r.shape)};return r.runKernel("tanh",{dtype:"float32"},n,[r.shape])[0]}static setupContext(e,r,n){const[s]=r;e.saveForBackward(s)}static backward(e,r){const[n]=e.savedTensors,s={size:(0,a.shapeSize)(n.shape)};return n.runKernel("tanh_grad",{dtype:"float32"},s,[n.shape],r)}}r.TanhFunction=TanhFunction;class TruncFunction extends s.AutoFunction{static forward(e){const[r]=e,n={size:(0,a.shapeSize)(r.shape)};return r.runKernel("trunc",{dtype:"float32"},n,[r.shape])[0]}static setupContext(e,r,n){const[s]=r;e.saveForBackward(s)}static backward(e,r){const[n]=e.savedTensors,s={size:(0,a.shapeSize)(n.shape)};return n.runKernel("trunc_grad",{dtype:"float32"},s,[n.shape],r)}}r.TruncFunction=TruncFunction;class XlogyFunction extends s.AutoFunction{static forward(e){const[r,n]=e;if("number"==typeof n){const e={size:(0,a.shapeSize)(r.shape),other:n};return r.runKernel("xlogy_scalar",{dtype:"float32"},e,[r.shape])[0]}{const e=(0,a.broadcastShapes)(r,n);if((0,a.stridedShapeIsContiguous)(e.a)&&(0,a.stridedShapeIsContiguous)(e.b)){if((0,a.shapeSize)(r.shape)!==(0,a.shapeSize)(n.shape))throw new Error(`Shape sizes must match. Got ${r.shape} and ${n.shape}`);const e={size:(0,a.shapeSize)(r.shape)};return r.runKernel("xlogy",{dtype:"float32"},e,[r.shape],n)[0]}{const s=e.a.shape.length,i=e.b.shape.length;if(s>4||i>4)throw new Error("Broadcasting not supported for tensors with more than 4 dimensions");const o={inputStrides0:s>0?e.a.strides[0]:0,otherStrides0:i>0?e.b.strides[0]:0,outputStrides0:e.output.shape.length>0?e.output.strides[0]:1,inputStrides1:s>1?e.a.strides[1]:0,otherStrides1:i>1?e.b.strides[1]:0,outputStrides1:e.output.shape.length>1?e.output.strides[1]:1,inputStrides2:s>2?e.a.strides[2]:0,otherStrides2:i>2?e.b.strides[2]:0,outputStrides2:e.output.shape.length>2?e.output.strides[2]:1,inputStrides3:s>3?e.a.strides[3]:0,otherStrides3:i>3?e.b.strides[3]:0,outputStrides3:e.output.shape.length>3?e.output.strides[3]:1,size:(0,a.shapeSize)(e.output.shape)};return r.runKernel("xlogy_strided",{dtype:"float32"},o,[e.output.shape],n)[0]}}}static setupContext(e,r,n){const[s,a]=r;e.saveForBackward(s,a)}static backward(e,r){const[n,s]=e.savedTensors;if("number"==typeof s){const e={size:(0,a.shapeSize)(n.shape),other:s};return n.runKernel("xlogy_scalar_grad",{dtype:"float32"},e,[n.shape],r)}{const e={size:(0,a.shapeSize)(n.shape)};return n.runKernel("xlogy_grad",{dtype:"float32"},e,[n.shape,s.shape],s,r)}}}r.XlogyFunction=XlogyFunction;class AllFunction extends s.AutoFunction{static forward(e){let[r,n,s]=e;if(void 0!==n){if(n=Array.isArray(n)&&1===n.length?n[0]:n,"number"==typeof n){const e=r.shape;let i=r.shape.slice();i[n]=1;let o=(0,a.defaultStrides)(i);const u={size:(0,a.shapeSize)(i),inputShape0:r.shape.length>0?r.shape[0]:1,inputStride0:r.shape.length>0?r.strides[0]:1,outputStride0:i.length>0?o[0]:1,inputShape1:r.shape.length>1?r.shape[1]:1,inputStride1:r.shape.length>1?r.strides[1]:1,outputStride1:i.length>1?o[1]:1,inputShape2:r.shape.length>2?r.shape[2]:1,inputStride2:r.shape.length>2?r.strides[2]:1,outputStride2:i.length>2?o[2]:1,inputShape3:r.shape.length>3?r.shape[3]:1,inputStride3:r.shape.length>3?r.strides[3]:1,outputStride3:i.length>3?o[3]:1};return s||i.splice(n,1),r.runKernel("all_dim",{dim:n,maxdim:e.length,dtype:"float32"},u,[i])[0]}throw new Error("Multi-dimension reduction not supported")}{const e={size:(0,a.shapeSize)(r.shape)};return r.runKernel("all",{dtype:"float32",workgroupSize:256},e,[[]])[0]}}static setupContext(e,r,n){let[s,a,i]=r;e.dim=a,e.keepdim=i,e.saveForBackward(s,n)}static backward(e,r){const[n,s]=e.savedTensors,i=e.dim;e.keepdim;if(void 0!==i){if("number"==typeof i){const e={size:(0,a.shapeSize)(n.shape)};return n.runKernel("all_dim_grad",{dtype:"float32",workgroupSize:256},e,[n.shape],s,r)}throw new Error("Multi-dimension backward reduction not supported")}{const e={size:(0,a.shapeSize)(n.shape)};return n.runKernel("all_grad",{dtype:"float32",workgroupSize:256},e,[n.shape],s,r)}}}r.AllFunction=AllFunction;class AnyFunction extends s.AutoFunction{static forward(e){let[r,n,s]=e;if(void 0!==n){if(n=Array.isArray(n)&&1===n.length?n[0]:n,"number"==typeof n){const e=r.shape;let i=r.shape.slice();i[n]=1;let o=(0,a.defaultStrides)(i);const u={size:(0,a.shapeSize)(i),inputShape0:r.shape.length>0?r.shape[0]:1,inputStride0:r.shape.length>0?r.strides[0]:1,outputStride0:i.length>0?o[0]:1,inputShape1:r.shape.length>1?r.shape[1]:1,inputStride1:r.shape.length>1?r.strides[1]:1,outputStride1:i.length>1?o[1]:1,inputShape2:r.shape.length>2?r.shape[2]:1,inputStride2:r.shape.length>2?r.strides[2]:1,outputStride2:i.length>2?o[2]:1,inputShape3:r.shape.length>3?r.shape[3]:1,inputStride3:r.shape.length>3?r.strides[3]:1,outputStride3:i.length>3?o[3]:1};return s||i.splice(n,1),r.runKernel("any_dim",{dim:n,maxdim:e.length,dtype:"float32"},u,[i])[0]}throw new Error("Multi-dimension reduction not supported")}{const e={size:(0,a.shapeSize)(r.shape)};return r.runKernel("any",{dtype:"float32",workgroupSize:256},e,[[]])[0]}}static setupContext(e,r,n){let[s,a,i]=r;e.dim=a,e.keepdim=i,e.saveForBackward(s,n)}static backward(e,r){const[n,s]=e.savedTensors,i=e.dim;e.keepdim;if(void 0!==i){if("number"==typeof i){const e={size:(0,a.shapeSize)(n.shape)};return n.runKernel("any_dim_grad",{dtype:"float32",workgroupSize:256},e,[n.shape],s,r)}throw new Error("Multi-dimension backward reduction not supported")}{const e={size:(0,a.shapeSize)(n.shape)};return n.runKernel("any_grad",{dtype:"float32",workgroupSize:256},e,[n.shape],s,r)}}}r.AnyFunction=AnyFunction;class MeanFunction extends s.AutoFunction{static forward(e){let[r,n,s]=e;if(void 0!==n){if(n=Array.isArray(n)&&1===n.length?n[0]:n,"number"==typeof n){const e=r.shape;let i=r.shape.slice();i[n]=1;let o=(0,a.defaultStrides)(i);const u={size:(0,a.shapeSize)(i),inputShape0:r.shape.length>0?r.shape[0]:1,inputStride0:r.shape.length>0?r.strides[0]:1,outputStride0:i.length>0?o[0]:1,inputShape1:r.shape.length>1?r.shape[1]:1,inputStride1:r.shape.length>1?r.strides[1]:1,outputStride1:i.length>1?o[1]:1,inputShape2:r.shape.length>2?r.shape[2]:1,inputStride2:r.shape.length>2?r.strides[2]:1,outputStride2:i.length>2?o[2]:1,inputShape3:r.shape.length>3?r.shape[3]:1,inputStride3:r.shape.length>3?r.strides[3]:1,outputStride3:i.length>3?o[3]:1};return s||i.splice(n,1),r.runKernel("mean_dim",{dim:n,maxdim:e.length,dtype:"float32"},u,[i])[0]}throw new Error("Multi-dimension reduction not supported")}{const e={size:(0,a.shapeSize)(r.shape)};return r.runKernel("mean",{dtype:"float32",workgroupSize:256},e,[[]])[0]}}static setupContext(e,r,n){let[s,a,i]=r;e.dim=a,e.keepdim=i,e.saveForBackward(s,n)}static backward(e,r){const[n,s]=e.savedTensors,i=e.dim;e.keepdim;if(void 0!==i){if("number"==typeof i){const e={size:(0,a.shapeSize)(n.shape)};return n.runKernel("mean_dim_grad",{dtype:"float32",workgroupSize:256},e,[n.shape],s,r)}throw new Error("Multi-dimension backward reduction not supported")}{const e={size:(0,a.shapeSize)(n.shape)};return n.runKernel("mean_grad",{dtype:"float32",workgroupSize:256},e,[n.shape],s,r)}}}r.MeanFunction=MeanFunction;class NormFunction extends s.AutoFunction{static forward(e){let[r,n,s]=e;if(void 0!==n){if(n=Array.isArray(n)&&1===n.length?n[0]:n,"number"==typeof n){const e=r.shape;let i=r.shape.slice();i[n]=1;let o=(0,a.defaultStrides)(i);const u={size:(0,a.shapeSize)(i),inputShape0:r.shape.length>0?r.shape[0]:1,inputStride0:r.shape.length>0?r.strides[0]:1,outputStride0:i.length>0?o[0]:1,inputShape1:r.shape.length>1?r.shape[1]:1,inputStride1:r.shape.length>1?r.strides[1]:1,outputStride1:i.length>1?o[1]:1,inputShape2:r.shape.length>2?r.shape[2]:1,inputStride2:r.shape.length>2?r.strides[2]:1,outputStride2:i.length>2?o[2]:1,inputShape3:r.shape.length>3?r.shape[3]:1,inputStride3:r.shape.length>3?r.strides[3]:1,outputStride3:i.length>3?o[3]:1};return s||i.splice(n,1),r.runKernel("norm_dim",{dim:n,maxdim:e.length,dtype:"float32"},u,[i])[0]}throw new Error("Multi-dimension reduction not supported")}{const e={size:(0,a.shapeSize)(r.shape)};return r.runKernel("norm",{dtype:"float32",workgroupSize:256},e,[[]])[0]}}static setupContext(e,r,n){let[s,a,i]=r;e.dim=a,e.keepdim=i,e.saveForBackward(s,n)}static backward(e,r){const[n,s]=e.savedTensors,i=e.dim;e.keepdim;if(void 0!==i){if("number"==typeof i){const e={size:(0,a.shapeSize)(n.shape)};return n.runKernel("norm_dim_grad",{dtype:"float32",workgroupSize:256},e,[n.shape],s,r)}throw new Error("Multi-dimension backward reduction not supported")}{const e={size:(0,a.shapeSize)(n.shape)};return n.runKernel("norm_grad",{dtype:"float32",workgroupSize:256},e,[n.shape],s,r)}}}r.NormFunction=NormFunction;class ProdFunction extends s.AutoFunction{static forward(e){let[r,n,s]=e;if(void 0!==n){if(n=Array.isArray(n)&&1===n.length?n[0]:n,"number"==typeof n){const e=r.shape;let i=r.shape.slice();i[n]=1;let o=(0,a.defaultStrides)(i);const u={size:(0,a.shapeSize)(i),inputShape0:r.shape.length>0?r.shape[0]:1,inputStride0:r.shape.length>0?r.strides[0]:1,outputStride0:i.length>0?o[0]:1,inputShape1:r.shape.length>1?r.shape[1]:1,inputStride1:r.shape.length>1?r.strides[1]:1,outputStride1:i.length>1?o[1]:1,inputShape2:r.shape.length>2?r.shape[2]:1,inputStride2:r.shape.length>2?r.strides[2]:1,outputStride2:i.length>2?o[2]:1,inputShape3:r.shape.length>3?r.shape[3]:1,inputStride3:r.shape.length>3?r.strides[3]:1,outputStride3:i.length>3?o[3]:1};return s||i.splice(n,1),r.runKernel("prod_dim",{dim:n,maxdim:e.length,dtype:"float32"},u,[i])[0]}throw new Error("Multi-dimension reduction not supported")}{const e={size:(0,a.shapeSize)(r.shape)};return r.runKernel("prod",{dtype:"float32",workgroupSize:256},e,[[]])[0]}}static setupContext(e,r,n){let[s,a,i]=r;e.dim=a,e.keepdim=i,e.saveForBackward(s,n)}static backward(e,r){const[n,s]=e.savedTensors,i=e.dim;e.keepdim;if(void 0!==i){if("number"==typeof i){const e={size:(0,a.shapeSize)(n.shape)};return n.runKernel("prod_dim_grad",{dtype:"float32",workgroupSize:256},e,[n.shape],s,r)}throw new Error("Multi-dimension backward reduction not supported")}{const e={size:(0,a.shapeSize)(n.shape)};return n.runKernel("prod_grad",{dtype:"float32",workgroupSize:256},e,[n.shape],s,r)}}}r.ProdFunction=ProdFunction;class SumFunction extends s.AutoFunction{static forward(e){let[r,n,s]=e;if(void 0!==n){if(n=Array.isArray(n)&&1===n.length?n[0]:n,"number"==typeof n){const e=r.shape;let i=r.shape.slice();i[n]=1;let o=(0,a.defaultStrides)(i);const u={size:(0,a.shapeSize)(i),inputShape0:r.shape.length>0?r.shape[0]:1,inputStride0:r.shape.length>0?r.strides[0]:1,outputStride0:i.length>0?o[0]:1,inputShape1:r.shape.length>1?r.shape[1]:1,inputStride1:r.shape.length>1?r.strides[1]:1,outputStride1:i.length>1?o[1]:1,inputShape2:r.shape.length>2?r.shape[2]:1,inputStride2:r.shape.length>2?r.strides[2]:1,outputStride2:i.length>2?o[2]:1,inputShape3:r.shape.length>3?r.shape[3]:1,inputStride3:r.shape.length>3?r.strides[3]:1,outputStride3:i.length>3?o[3]:1};return s||i.splice(n,1),r.runKernel("sum_dim",{dim:n,maxdim:e.length,dtype:"float32"},u,[i])[0]}throw new Error("Multi-dimension reduction not supported")}{const e={size:(0,a.shapeSize)(r.shape)};return r.runKernel("sum",{dtype:"float32",workgroupSize:256},e,[[]])[0]}}static setupContext(e,r,n){let[s,a,i]=r;e.dim=a,e.keepdim=i,e.saveForBackward(s,n)}static backward(e,r){const[n,s]=e.savedTensors,i=e.dim;e.keepdim;if(void 0!==i){if("number"==typeof i){const e={size:(0,a.shapeSize)(n.shape)};return n.runKernel("sum_dim_grad",{dtype:"float32",workgroupSize:256},e,[n.shape],s,r)}throw new Error("Multi-dimension backward reduction not supported")}{const e={size:(0,a.shapeSize)(n.shape)};return n.runKernel("sum_grad",{dtype:"float32",workgroupSize:256},e,[n.shape],s,r)}}}r.SumFunction=SumFunction;class CountNonzeroFunction extends s.AutoFunction{static forward(e){let[r,n,s]=e;if(void 0!==n){if(n=Array.isArray(n)&&1===n.length?n[0]:n,"number"==typeof n){const e=r.shape;let i=r.shape.slice();i[n]=1;let o=(0,a.defaultStrides)(i);const u={size:(0,a.shapeSize)(i),inputShape0:r.shape.length>0?r.shape[0]:1,inputStride0:r.shape.length>0?r.strides[0]:1,outputStride0:i.length>0?o[0]:1,inputShape1:r.shape.length>1?r.shape[1]:1,inputStride1:r.shape.length>1?r.strides[1]:1,outputStride1:i.length>1?o[1]:1,inputShape2:r.shape.length>2?r.shape[2]:1,inputStride2:r.shape.length>2?r.strides[2]:1,outputStride2:i.length>2?o[2]:1,inputShape3:r.shape.length>3?r.shape[3]:1,inputStride3:r.shape.length>3?r.strides[3]:1,outputStride3:i.length>3?o[3]:1};return s||i.splice(n,1),r.runKernel("countNonzero_dim",{dim:n,maxdim:e.length,dtype:"float32"},u,[i])[0]}throw new Error("Multi-dimension reduction not supported")}{const e={size:(0,a.shapeSize)(r.shape)};return r.runKernel("countNonzero",{dtype:"float32",workgroupSize:256},e,[[]])[0]}}static setupContext(e,r,n){let[s,a,i]=r;e.dim=a,e.keepdim=i,e.saveForBackward(s,n)}static backward(e,r){const[n,s]=e.savedTensors,i=e.dim;e.keepdim;if(void 0!==i){if("number"==typeof i){const e={size:(0,a.shapeSize)(n.shape)};return n.runKernel("countNonzero_dim_grad",{dtype:"float32",workgroupSize:256},e,[n.shape],s,r)}throw new Error("Multi-dimension backward reduction not supported")}{const e={size:(0,a.shapeSize)(n.shape)};return n.runKernel("countNonzero_grad",{dtype:"float32",workgroupSize:256},e,[n.shape],s,r)}}}r.CountNonzeroFunction=CountNonzeroFunction},700:(e,r,n)=>{Object.defineProperty(r,"__esModule",{value:!0}),r.KernelNode=r.ViewNode=r.ComputedNode=r.SourceNode=r.GraphNode=r.GraphNodeOutputRef=void 0;const s=n(320);class GraphNodeOutputRef{node;outputIndex;get dtype(){return this.node.outputs[this.outputIndex].dtype}constructor(e,r){this.node=e,this.outputIndex=r}addRef(){return this.node.addRef(),this}}r.GraphNodeOutputRef=GraphNodeOutputRef;class GraphNode{static nextId=0;id;_outputRefs=[];_refCount=0;get refCount(){return this._refCount}get isComputed(){return!this.isSource}constructor(){this.id=GraphNode.nextId++}getOutputRef(e){return void 0===this._outputRefs[e]&&(this._outputRefs[e]=new GraphNodeOutputRef(this,e)),this._outputRefs[e]}addRef(){this._refCount++}}r.GraphNode=GraphNode;r.SourceNode=class SourceNode extends GraphNode{_outputs;_storages;get isSource(){return!0}get device(){return this._storages[0].device}get inputs(){return[]}get outputs(){return this._outputs}get storageAvailable(){return!0}get storages(){return this._storages}constructor(e,r,n,s){super(),this._outputs=[{dtype:r,shape:n,strides:s}],this._storages=[e]}eager(){}};class ComputedNode extends GraphNode{_storages=null;inputs;outputs;get isSource(){return!1}get storageAvailable(){return null!==this._storages}get storages(){return null===this._storages&&run([this]),this._storages}constructor(e,r){super(),this.inputs=e,this.outputs=r}eager(){null===this._storages&&run([this])}setStorages(e){this._storages=e}}r.ComputedNode=ComputedNode;r.ViewNode=class ViewNode extends ComputedNode{get device(){return this.inputs[0].node.device}get storageAvailable(){return this.inputs[0].node.storageAvailable}get storages(){return[this.inputs[0].node.storages[this.inputs[0].outputIndex]]}constructor(e,r,n){super([e],[{shape:r,strides:n,dtype:e.dtype}])}runNode(e,r){return e}};function run(e){const r=e[0].device,[n,s,a,i]=function createExecutionPlan(e){const r=[],n=new Set,s=e.slice(),a={};function topoSort(e){if(!n.has(e.id)){if(n.add(e.id),e.storageAvailable)a[e.id]=e;else{for(let r of e.inputs)topoSort(r.node);e.refCount>1&&s.push(e)}r.push(e)}}for(let r of e)topoSort(r);const i=function getLiveness(e,r){const n=e.length,s=[],a=[];for(let e=0;e<n;e++)s.push(new Set),a.push(new Set);let i=!0;for(;i;){i=!1;for(let o=n-1;o>=0;o--){const u=e[o],p=new Set;if(o+1<n)for(let e of s[o+1])p.add(e);else for(let e of r)p.add(e.id);i=i||!setsAreEqual(p,a[o]);const d=new Set(p);d.delete(u.id);for(let e of u.inputs)d.add(e.node.id);i=i||!setsAreEqual(d,s[o]),s[o]=d,a[o]=p}}return{ins:s,outs:a}}(r,s);return[r,a,i,s]}(e),o={},u=new Map,addRef=e=>{const r=u.get(e);void 0===r?u.set(e,1):u.set(e,r+1)},alloc=e=>{e in o||(o[e]=[]);let n=o[e].pop();return void 0===n&&(n=r.alloc(e)),n},remRef=e=>{const r=u.get(e);if(void 0===r)throw new Error("Storage is not ref counted");if(1===r){u.delete(e);const r=e.byteSize;r in o?o[r].push(e):o[r]=[e]}else u.set(e,r-1)},p={},d=n.length;for(let e=0;e<d;e++){const r=n[e],i=r.id;if(i in s){p[i]=s[i].storages;continue}if(!(r instanceof ComputedNode))throw new Error(`Node ${i} is not a ComputedNode, but it is not in nodesWithStorage`);const o=r.inputs.map(((e,n)=>{const s=p[e.node.id][e.outputIndex];if(void 0===s)throw new Error(`Input #${n} of node ${r.id} not computed yet`);return s})),u=r.runNode(o,alloc);u.forEach(addRef),p[i]=u;for(let r of a.ins[e]){if(a.outs[e].has(r))continue;const n=p[r];if(delete p[r],!(r in s))for(let e of n)remRef(e)}}for(let e of i)if(e instanceof ComputedNode){const r=e.id,n=p[r];if(void 0===n)throw new Error(`Node ${r} not computed`);e.setStorages(n)}}function setsAreEqual(e,r){if(e.size!==r.size)return!1;for(let n of e)if(!r.has(n))return!1;return!0}r.KernelNode=class KernelNode extends ComputedNode{kernel;params;get device(){return this.kernel.device}constructor(e,r,n,s){if(super(r,s),r.length!==e.spec.inputs.length)throw new Error(`Kernel "${e.spec.name}" expects ${e.spec.inputs.length} inputs, but ${r.length} were provided`);this.kernel=e,this.params=n}runNode(e,r){const n=this.kernel,[a,i]=n.getRunEnv(this.params),o=this.outputs.map(((e,i)=>{const o=n.spec.outputs[i].size(a)*(0,s.dtypeByteSize)(e.dtype);return r(o)}));return n.run(e,this.params,o),o}}},156:function(e,r,n){var s=this&&this.__createBinding||(Object.create?function(e,r,n,s){void 0===s&&(s=n);var a=Object.getOwnPropertyDescriptor(r,n);a&&!("get"in a?!r.__esModule:a.writable||a.configurable)||(a={enumerable:!0,get:function(){return r[n]}}),Object.defineProperty(e,s,a)}:function(e,r,n,s){void 0===s&&(s=n),e[s]=r[n]}),a=this&&this.__setModuleDefault||(Object.create?function(e,r){Object.defineProperty(e,"default",{enumerable:!0,value:r})}:function(e,r){e.default=r}),i=this&&this.__exportStar||function(e,r){for(var n in e)"default"===n||Object.prototype.hasOwnProperty.call(r,n)||s(r,e,n)},o=this&&this.__importStar||function(e){if(e&&e.__esModule)return e;var r={};if(null!=e)for(var n in e)"default"!==n&&Object.prototype.hasOwnProperty.call(e,n)&&s(r,e,n);return a(r,e),r};Object.defineProperty(r,"__esModule",{value:!0}),r.initWebGPUAsync=r.hasWebGPU=r.UntypedStorage=r.nn=r.Kernel=r.init=r.Device=r.GradientContext=void 0;var u=n(149);Object.defineProperty(r,"GradientContext",{enumerable:!0,get:function(){return u.GradientContext}});var p=n(44);Object.defineProperty(r,"Device",{enumerable:!0,get:function(){return p.Device}}),i(n(882),r),r.init=o(n(336));var d=n(679);Object.defineProperty(r,"Kernel",{enumerable:!0,get:function(){return d.Kernel}}),i(n(980),r),i(n(500),r),r.nn=o(n(934)),i(n(504),r),i(n(81),r);var h=n(421);Object.defineProperty(r,"UntypedStorage",{enumerable:!0,get:function(){return h.UntypedStorage}}),i(n(967),r);var c=n(414);Object.defineProperty(r,"hasWebGPU",{enumerable:!0,get:function(){return c.hasWebGPU}}),Object.defineProperty(r,"initWebGPUAsync",{enumerable:!0,get:function(){return c.initWebGPUAsync}})},336:(e,r,n)=>{Object.defineProperty(r,"__esModule",{value:!0}),r.uniform_=r.kaimingUniform_=r.calculateFanInAndFanOut=void 0;const s=n(149);function calculateFanInAndFanOut(e){const r=e.ndim;if(r<2)throw new Error("Fan in and fan out can not be computed for tensor with fewer than 2 dimensions");const n=e.shape[1],s=e.shape[0],a=r>2?e.shape.slice(2).reduce(((e,r)=>e*r),1):1;return[n*a,s*a]}r.calculateFanInAndFanOut=calculateFanInAndFanOut;const a={linear:null,conv1d:null,conv2d:null,conv3d:null,conv_transpose1d:null,conv_transpose2d:null,conv_transpose3d:null};r.kaimingUniform_=function kaimingUniform_(e,r=0,n="fanIn",i="leakyRelu"){if(e.shape.includes(0))return e;const[o,u]=calculateFanInAndFanOut(e),p="fanIn"===n?o:u,d=function calculateGain(e,r){if(e in a||"sigmoid"===e)return 1;if("tanh"===e)return 5/3;if("relu"===e)return Math.sqrt(2);if("leakyRelu"===e){const e=r||.01;return Math.sqrt(2/(1+Math.pow(e,2)))}if("selu"===e)return 3/4;throw new Error(`Unsupported nonlinearity ${e}`)}(i,r)/Math.sqrt(p),h=Math.sqrt(3)*d;return(0,s.noGrad)((()=>{e.uniform_(-h,h)})),e},r.uniform_=function uniform_(e,r,n){return(0,s.noGrad)((()=>{e.uniform_(r,n)})),e}},679:(e,r,n)=>{Object.defineProperty(r,"__esModule",{value:!0}),r.getKernelJavaScriptCode=r.getKernelShaderCode=r.shaderTypeToCode=r.shaderTypeToDtype=r.getShaderTypeElementByteSize=r.getKernelKey=r.getKernelConfig=r.Kernel=void 0;const s=n(653),a=n(795);function getKernelKey(e,r){let n=[e.name];for(let s=0;s<e.config.length;s++){let a=e.config[s],i=r[s];n.push(`${a.name}=${i}`)}return n.join(",")}function configShader(e,r){const n=[],s={};for(let a=0;a<e.config.length;a++){let i=e.config[a],o=r[a];n.push([i.name,o.toString()]),s[i.name]=o}let a=e.shader.trim();for(let[e,r]of n)a=a.replace(new RegExp(`\\$\\$${e}\\$\\$`,"g"),r);return[a,s]}function shaderTypeToCode(e){if("string"==typeof e)return e;if(e instanceof Array)return e[0].replace(">",", "+e[1]+">");throw new Error(`Unknown shader type ${e}`)}r.Kernel=class Kernel{_key;_spec;_config;_device;_configEnv;get key(){return this._key}get spec(){return this._spec}get config(){return this._config}get device(){return this._device}constructor(e,r,n,a){this._key=getKernelKey(e,r),this._device=n,this._spec=function compileKernelSpec(e){return{name:e.name,parameters:e.parameters,config:e.config,shader:e.shader,workgroupCount:e.workgroupCount.map((e=>(0,s.compileCode)(e))),workgroupSize:e.workgroupSize.map((e=>(0,s.compileCode)(e))),inputs:e.inputs,outputs:e.outputs.map((e=>({name:e.name,shaderType:e.shaderType,size:(0,s.compileCode)(e.size)})))}}(e),this._config=r,this._configEnv=Object.assign({},a);for(let e=0;e<this._spec.config.length;e++){const r=this._spec.config[e],n=this._config[e];this._configEnv[r.name]=n}}getRunEnv(e){const r=Object.assign({},this._configEnv),n=[];for(let s=0;s<this.spec.parameters.length;s++){const a=this.spec.parameters[s],i=e[a.name];if(void 0===i)throw new Error(`Missing parameter "${a.name}" for kernel "${this.spec.name}"`);n.push(i),r[a.name]=i}return[r,n]}getWorkgroupCounts(e){const r=Math.ceil(this._spec.workgroupCount[0](e)),n=Math.ceil(this._spec.workgroupCount[1](e)),s=Math.ceil(this._spec.workgroupCount[2](e));if(r>this.device.workgroupMaxCount)throw new Error(`Workgroup count X (${r}) exceeds the maximum allowed value (${this.device.workgroupMaxCount})`);if(n>this.device.workgroupMaxCount)throw new Error(`Workgroup count Y (${n}) exceeds the maximum allowed value (${this.device.workgroupMaxCount})`);if(s>this.device.workgroupMaxCount)throw new Error(`Workgroup count Z (${s}) exceeds the maximum allowed value (${this.device.workgroupMaxCount})`);return[r,n,s]}},r.getKernelConfig=function getKernelConfig(e,r){let n=[];for(let s=0;s<e.config.length;s++){let a=e.config[s],i=r[a.name];if(void 0===i)throw new Error(`Missing config value "${a.name}" for kernel "${e.name}"`);n.push(i)}return n},r.getKernelKey=getKernelKey,r.getShaderTypeElementByteSize=function getShaderTypeElementByteSize(e){switch(e){case"f32":case"i32":case"u32":case"array<f32>":case"array<i32>":case"array<u32>":return 4;case"u8":case"array<u8>":return 1;default:throw new Error(`Unknown shader type ${e}`)}},r.shaderTypeToDtype=function shaderTypeToDtype(e){switch(e){case"f32":case"array<f32>":return"float32";case"i32":case"array<i32>":return"int32";case"u32":case"array<u32>":return"uint32";case"u8":case"array<u8>":return"uint8";default:throw new Error(`Unknown shader type ${e}`)}},r.shaderTypeToCode=shaderTypeToCode,r.getKernelShaderCode=function getKernelShaderCode(e,r,n){const[a,i]=configShader(e,r);let o=["// "+e.name+" kernel"];o.push(`struct ${e.name}Parameters {`);for(let r=0;r<e.parameters.length;r++){let n=e.parameters[r];o.push(`    ${n.name}: ${n.shaderType},`)}o.push("}");let u=0;for(let r=0;r<e.inputs.length;r++,u++){let n=e.inputs[r];o.push(`@group(0) @binding(${u}) var<storage, read> ${n.name}: ${n.shaderType};`)}for(let r=0;r<e.outputs.length;r++,u++){let n=e.outputs[r];o.push(`@group(0) @binding(${u}) var<storage, read_write> ${n.name}: ${n.shaderType};`)}if(o.push(`@group(0) @binding(${u}) var<storage, read> parameters: ${e.name}Parameters;`),void 0!==e.workgroupVariables)for(let r of e.workgroupVariables)o.push(`var<workgroup> ${r.name}: ${shaderTypeToCode(r.shaderType)};`);const[p,d,h]=n.workgroupMaxSize,c=Math.min(p,Math.ceil((0,s.evalCode)(e.workgroupSize[0],i))),l=Math.min(d,Math.ceil((0,s.evalCode)(e.workgroupSize[1],i))),f=Math.min(h,Math.ceil((0,s.evalCode)(e.workgroupSize[2],i)));o.push(`@compute @workgroup_size(${c}, ${l}, ${f})`),o.push("fn main(");let g="";return a.includes("global_id")&&(o.push(`    ${g}@builtin(global_invocation_id) global_id: vec3u`),g=", "),a.includes("local_id")&&(o.push(`    ${g}@builtin(local_invocation_id) local_id: vec3u`),g=", "),o.push("    ) {"),o.push("    "+a),o.push("}"),o.join("\n")};const i=[["(\\d+)u","$1"],["(\\d+)f","$1"],["(\\d+\\.\\d+)f","$1"],[">>",">>>"],["global_id\\.x","global_id_x"],["global_id\\.y","global_id_y"],["global_id\\.z","global_id_z"],["local_id\\.x","local_id_x"],["local_id\\.y","local_id_y"],["local_id\\.z","local_id_z"],["workgroupBarrier\\s*\\(\\s*\\)\\s*;",'yield "workgroupBarrier";'],["storageBarrier\\s*\\(\\s*\\)\\s*;",'yield "storageBarrier";']].map((([e,r])=>[new RegExp(e,"g"),r]));for(const e of Object.getOwnPropertyNames(Math))"function"==typeof Math[e]&&i.push([new RegExp(`\\b${e}\\(`,"g"),`Math.${e}(`]);const o={exp2:"function exp2(x) { return Math.pow(2, x); }",fract:"function fract(x) { return x - Math.floor(x); }",select:"function select(falseValue, trueValue, condition) { return condition ? trueValue : falseValue; }",f32:"function f32(x) { return x; }",i32:"function i32(x) { return Math.floor(x) | 0; }",u32:"function u32(x) { return Math.floor(x) >>> 0; }"};r.getKernelJavaScriptCode=function getKernelJavaScriptCode(e,r){const[n,u]=configShader(e,r),p=n.includes("global_id"),d=n.includes("local_id"),h=n.includes("workgroupBarrier")||n.includes("storageBarrier");let c=n;for(const[e,r]of i)c=c.replace(e,r);const l=function getIdentifiers(e){const r=/[a-zA-Z_][a-zA-Z0-9_]*/g,n=new Set;let s;for(;null!==(s=r.exec(e));)n.add(s[0]);return Array.from(n)}(c),f=new Set;for(const e of l)e in o&&f.add(e);const g=Array.from(f),y=new a.CodeWriter,S=[];let m=0;for(let r=0;r<e.inputs.length;r++,m++){let n=e.inputs[r];S.push(n.name)}for(let r=0;r<e.outputs.length;r++,m++){let n=e.outputs[r];S.push(n.name)}S.push("parameters"),S.push("workgroupCountX"),S.push("workgroupCountY"),S.push("workgroupCountZ");const w=Math.ceil((0,s.evalCode)(e.workgroupSize[0],u)),b=Math.ceil((0,s.evalCode)(e.workgroupSize[1],u)),_=Math.ceil((0,s.evalCode)(e.workgroupSize[2],u));y.writeLine(`((${S.join(", ")}) => {`),y.indent();for(const e of g)y.writeLine(o[e]);const z=[];p&&(z.push("global_id_x"),z.push("global_id_y"),z.push("global_id_z")),d&&(z.push("local_id_x"),z.push("local_id_y"),z.push("local_id_z"));const x=z.join(", ");y.writeLine(`function${h?"*":""} ${e.name}Kernel(${x}) {`),y.indent(),y.writeLine(c),y.dedent(),y.writeLine("}");const v=w*b*_;return h&&y.writeLine(`var barriers = new Array(${v});`),y.writeLine("for (let wgZ = 0; wgZ < workgroupCountZ; wgZ++) {"),y.indent(),y.writeLine("for (let wgY = 0; wgY < workgroupCountY; wgY++) {"),y.indent(),y.writeLine("for (let group_id_x = 0; group_id_x < workgroupCountX; group_id_x++) {"),y.indent(),y.writeLine(`const globalStartX = group_id_x * ${w};`),y.writeLine(`const globalEndX = globalStartX + ${w};`),y.writeLine(`const globalStartY = wgY * ${b};`),y.writeLine(`const globalEndY = globalStartY + ${b};`),y.writeLine(`const globalStartZ = wgZ * ${_};`),y.writeLine(`const globalEndZ = globalStartZ + ${_};`),h&&y.writeLine("let allDone = true;"),y.writeLine("for (let global_id_z = globalStartZ; global_id_z < globalEndZ; global_id_z++) {"),y.indent(),y.writeLine("for (let global_id_y = globalStartY; global_id_y < globalEndY; global_id_y++) {"),y.indent(),y.writeLine("for (let global_id_x = globalStartX; global_id_x < globalEndX; global_id_x++) {"),y.indent(),d&&(y.writeLine("const local_id_x = global_id_x - globalStartX;"),y.writeLine("const local_id_y = global_id_y - globalStartY;"),y.writeLine("const local_id_z = global_id_z - globalStartZ;")),h?(y.writeLine(`const local_index = local_id_x + local_id_y * ${w} + local_id_z * ${w*b};`),y.writeLine(`const barrier = ${e.name}Kernel(${x});`),y.writeLine("const firstBarrierValue = barrier.next();"),y.writeLine("allDone = allDone && firstBarrierValue.done;"),y.writeLine("barriers[local_index] = barrier;")):y.writeLine(`${e.name}Kernel(${x});`),y.dedent(),y.writeLine("}"),y.dedent(),y.writeLine("}"),y.dedent(),y.writeLine("}"),h&&(y.writeLine("while (!allDone) {"),y.indent(),y.writeLine("allDone = true;"),y.writeLine(`for (let local_index = 0; local_index < ${v}; local_index++) {`),y.indent(),y.writeLine("const barrier = barriers[local_index];"),y.writeLine("const barrierValue = barrier.next();"),y.writeLine("allDone = allDone && barrierValue.done;"),y.dedent(),y.writeLine("}"),y.dedent(),y.writeLine("}")),y.dedent(),y.writeLine("}"),y.dedent(),y.writeLine("}"),y.dedent(),y.writeLine("}"),y.dedent(),y.writeLine("})"),y.toString()}},566:(__unused_webpack_module,exports,__webpack_require__)=>{Object.defineProperty(exports,"__esModule",{value:!0}),exports.KernelCPU=void 0;const kernel_1=__webpack_require__(679),storage_1=__webpack_require__(421);class KernelCPU extends kernel_1.Kernel{_javaScriptCode;_javaScriptFunction;constructor(spec,config,device){super(spec,config,device,jsMathEnv),this._javaScriptCode=(0,kernel_1.getKernelJavaScriptCode)(spec,config),this._javaScriptFunction=eval(this._javaScriptCode)}run(e,r,n){if(e.length!==this.spec.inputs.length)throw new Error(`Expected ${this.spec.inputs.length} inputs for kernel "${this.spec.name}", got ${e.length}`);const[s,a]=this.getRunEnv(r),i=[],o=[];this.spec.inputs.forEach(((r,n)=>{const a=this.getStorageInputBuffer(r,e[n],n,s).getTypedArray((0,kernel_1.shaderTypeToDtype)(r.shaderType));i.push(a)})),this.spec.outputs.forEach(((e,r)=>{const a=this.getStorageOutputBuffer(e,n?n[r]:null,r,s),u=a.getTypedArray((0,kernel_1.shaderTypeToDtype)(e.shaderType));i.push(u),o.push(a)})),i.push(r);const[u,p,d]=this.getWorkgroupCounts(s);return i.push(u),i.push(p),i.push(d),this._javaScriptFunction.apply(null,i),o}getStorageInputBuffer(e,r,n,s){if(r instanceof storage_1.ArrayBufferStorage)return r;throw new Error(`Input buffer #${n} (out of ${this.spec.inputs.length}) named "${e.name}" in kernel "${this.key}" is not an ArrayBufferStorage`)}getStorageOutputBuffer(e,r,n,s){if(null!==r){if(r instanceof storage_1.ArrayBufferStorage)return r;throw new Error(`Output buffer #${n} (out of ${this.spec.outputs.length}) named "${e.name}" in kernel "${this.key}" is not an ArrayBufferStorage. It's a ${r.constructor.name}`)}{const r=(0,kernel_1.getShaderTypeElementByteSize)(e.shaderType)*Math.ceil(this._spec.outputs[n].size(s));return this.device.heapAlloc(r)}}}exports.KernelCPU=KernelCPU;const jsMathEnv={};for(const e of Object.getOwnPropertyNames(Math)){const r=Math[e];"function"==typeof r&&(jsMathEnv[e]=r)}},610:(e,r,n)=>{Object.defineProperty(r,"__esModule",{value:!0}),r.KernelWebGPU=void 0;const s=n(679);class KernelWebGPU extends s.Kernel{_gpuDevice;_bindGroupLayout;_computePipeline;_shaderCode;_runId=0;constructor(e,r,n){super(e,r,n,{});const a=n.gpuDevice;if(!a)throw new Error("Cannot create a GPU kernel without a GPU device");this._gpuDevice=a;let i=[],o=0;for(let r=0;r<e.inputs.length;r++,o++)i.push({binding:o,visibility:GPUShaderStage.COMPUTE,buffer:{type:"read-only-storage"}});for(let r=0;r<e.outputs.length;r++,o++)i.push({binding:o,visibility:GPUShaderStage.COMPUTE,buffer:{type:"storage"}});i.push({binding:o,visibility:GPUShaderStage.COMPUTE,buffer:{type:"read-only-storage"}}),this._bindGroupLayout=a.createBindGroupLayout({entries:i}),this._shaderCode=(0,s.getKernelShaderCode)(e,r,n);const u=a.createShaderModule({code:this._shaderCode});this._computePipeline=a.createComputePipeline({layout:a.createPipelineLayout({bindGroupLayouts:[this._bindGroupLayout]}),compute:{module:u,entryPoint:"main"}})}run(e,r,n){this._runId++;const[s,a]=this.getRunEnv(r),i=this.spec.inputs.map(((r,n)=>this.getStorageInputBuffer(r,e[n]?e[n]:null,n,s))),o=this.spec.outputs.map(((e,r)=>this.getStorageOutputBuffer(e,n?n[r]:null,r,s))),u=this.createBindGroup(i,this.getParamsBuffer(a),o),[p,d,h]=this.getWorkgroupCounts(s),c=this._gpuDevice.createCommandEncoder(),l=c.beginComputePass();l.setPipeline(this._computePipeline),l.setBindGroup(0,u),l.dispatchWorkgroups(p,d,h),l.end();const f=c.finish();return this._gpuDevice.queue.submit([f]),o}cachedParamBuffers={};getParamsBuffer(e){const r=e.join(",");if(this.cachedParamBuffers[r])return this.cachedParamBuffers[r];let n=0;for(let e=0;e<this.spec.parameters.length;e++){const r=this.spec.parameters[e];n+=(0,s.getShaderTypeElementByteSize)(r.shaderType)}const a=this._gpuDevice.createBuffer({mappedAtCreation:!0,size:n,usage:GPUBufferUsage.STORAGE}),i=a.getMappedRange();for(let r of["u32","f32"]){let n=new("u32"===r?Uint32Array:Float32Array)(i);for(let s=0;s<this.spec.parameters.length;s++){this.spec.parameters[s].shaderType===r&&(n[s]=+e[s])}}return a.unmap(),this.cachedParamBuffers[r]=a,a}getStorageInputBuffer(e,r,n,s){if(null===r)throw new Error(`Missing input buffer #${n} (out of ${this.spec.inputs.length}) named "${e.name}" in kernel "${this.key}"`);const a=r.gpuBuffer;if(!(a.usage&GPUBufferUsage.STORAGE))throw new Error("Provided input buffer is not a storage buffer");return"mapped"===a.mapState&&a.unmap(),r}getStorageOutputBuffer(e,r,n,a){if(null!==r){const e=r.gpuBuffer;if(!(e.usage&GPUBufferUsage.STORAGE))throw new Error("Provided output buffer is not a storage buffer");return"mapped"===e.mapState&&e.unmap(),r}{const r=(0,s.getShaderTypeElementByteSize)(e.shaderType)*Math.ceil(this._spec.outputs[n].size(a));return this.device.alloc(r)}}createBindGroup(e,r,n){const s=[];let a=0;for(let r=0;r<e.length;r++,a++){const n=e[r];s.push({binding:a,resource:{buffer:n.gpuBuffer,offset:n.byteOffset,size:n.byteSize}})}for(let e=0;e<this.spec.outputs.length;e++,a++){const r=n[e],i={binding:a,resource:{buffer:r.gpuBuffer,offset:r.byteOffset,size:r.byteSize}};s.push(i)}s.push({binding:a,resource:{buffer:r}});return this._gpuDevice.createBindGroup({layout:this._bindGroupLayout,entries:s})}}r.KernelWebGPU=KernelWebGPU},950:(e,r,n)=>{Object.defineProperty(r,"__esModule",{value:!0}),r.registry=void 0,r.registry={};const s=n(690);for(const e in s.kernels)r.registry[e]=s.kernels[e];const a=n(922);for(const e in a.kernels)r.registry[e]=a.kernels[e]},922:(e,r)=>{Object.defineProperty(r,"__esModule",{value:!0}),r.kernels=void 0,r.kernels={conv2d:{name:"conv2d",config:[{name:"dtype"}],parameters:[{name:"batchSize",shaderType:"u32"},{name:"inputChannels",shaderType:"u32"},{name:"outputChannels",shaderType:"u32"},{name:"inputHeight",shaderType:"u32"},{name:"inputWidth",shaderType:"u32"},{name:"kernelHeight",shaderType:"u32"},{name:"kernelWidth",shaderType:"u32"},{name:"outputHeight",shaderType:"u32"},{name:"outputWidth",shaderType:"u32"}],inputs:[{name:"input",shaderType:"array<f32>"},{name:"weight",shaderType:"array<f32>"}],outputs:[{name:"output",shaderType:"array<f32>",size:"batchSize * outputChannels * outputHeight * outputWidth"}],workgroupSize:[4,4,1],workgroupCount:["outputWidth/4","outputHeight/4",1],shader:"\n    if (global_id.x >= parameters.outputWidth || global_id.y >= parameters.outputHeight) {\n        return;\n    }\n    // input shape = [B, C, H, W]\n    for (var batch = 0u; batch < parameters.batchSize; batch++) {\n        for (var outputChannel = 0u; outputChannel < parameters.outputChannels; outputChannel++) {\n            var result = 0.0;\n            // Do the convolution\n            for (var inputChannel = 0u; inputChannel < parameters.inputChannels; inputChannel++) {\n                for (var kernelY = 0u; kernelY < parameters.kernelHeight; kernelY++) {\n                    for (var kernelX = 0u; kernelX < parameters.kernelWidth; kernelX++) {\n                        var inputY = global_id.y + kernelY;\n                        var inputX = global_id.x + kernelX;\n                        var inputIndex =\n                            batch * parameters.inputChannels * parameters.inputHeight * parameters.inputWidth +\n                            inputChannel * parameters.inputHeight * parameters.inputWidth +\n                            inputY * parameters.inputWidth +\n                            inputX;\n                        var kernelIndex =\n                            outputChannel * parameters.inputChannels * parameters.kernelHeight * parameters.kernelWidth +\n                            inputChannel * parameters.kernelHeight * parameters.kernelWidth +\n                            kernelY * parameters.kernelWidth +\n                            kernelX;\n                        result = result + input[inputIndex] * weight[kernelIndex];\n                    }\n                }\n            }\n            // Output\n            let outputIndex = \n                batch * parameters.outputChannels * parameters.outputHeight * parameters.outputWidth +\n                outputChannel * parameters.outputHeight * parameters.outputWidth +\n                global_id.y * parameters.outputWidth +\n                global_id.x;\n            output[outputIndex] = result;\n        }\n    }\n"},dot:{name:"dot",config:[{name:"resultDtype"}],parameters:[{name:"aRows",shaderType:"u32"},{name:"aRowStride",shaderType:"u32"},{name:"bRowStride",shaderType:"u32"}],inputs:[{name:"a",shaderType:"array<f32>"},{name:"b",shaderType:"array<f32>"}],outputs:[{name:"output",shaderType:"array<f32>",size:"1"}],workgroupSize:[1,1,1],workgroupCount:[1,1,1],shader:"\n    var result = 0.0;\n    var aIndex = 0u;\n    var bIndex = 0u;\n    for (var aRow = 0u; aRow < parameters.aRows; aRow = aRow + 1u) {\n        result = result + a[aIndex] * b[bIndex];\n        aIndex = aIndex + parameters.aRowStride;\n        bIndex = bIndex + parameters.bRowStride;\n    }\n    output[0] = result;\n"},mv:{name:"mv",config:[{name:"resultDtype"}],parameters:[{name:"aRows",shaderType:"u32"},{name:"aCols",shaderType:"u32"},{name:"aRowStride",shaderType:"u32"},{name:"aColStride",shaderType:"u32"},{name:"bRowStride",shaderType:"u32"}],inputs:[{name:"a",shaderType:"array<f32>"},{name:"b",shaderType:"array<f32>"}],outputs:[{name:"output",shaderType:"array<f32>",size:"aRows"}],workgroupSize:[256,1,1],workgroupCount:["aRows/256",1,1],shader:"\n    let outputRow = global_id.x;\n    if (outputRow >= parameters.aRows) {\n        return;\n    }\n    var result = 0.0;\n    var aIndex = outputRow * parameters.aRowStride;\n    var bIndex = 0u;\n    for (var aCol = 0u; aCol < parameters.aCols; aCol = aCol + 1u) {\n        result = result + a[aIndex] * b[bIndex];\n        aIndex = aIndex + parameters.aColStride;\n        bIndex = bIndex + parameters.bRowStride;\n    }\n    output[outputRow] = result;\n"},mm:{name:"mm",config:[{name:"resultDtype"}],parameters:[{name:"aRows",shaderType:"u32"},{name:"aCols",shaderType:"u32"},{name:"bCols",shaderType:"u32"},{name:"aRowStride",shaderType:"u32"},{name:"aColStride",shaderType:"u32"},{name:"bRowStride",shaderType:"u32"},{name:"bColStride",shaderType:"u32"},{name:"alpha",shaderType:"f32"}],inputs:[{name:"a",shaderType:"array<f32>"},{name:"b",shaderType:"array<f32>"}],outputs:[{name:"output",shaderType:"array<f32>",size:"aRows * bCols"}],workgroupSize:[16,16,1],workgroupCount:["aRows/16","bCols/16",1],shader:"\n    let outputRow = global_id.x;\n    let outputCol = global_id.y;\n    if (outputRow >= parameters.aRows || outputCol >= parameters.bCols) {\n        return;\n    }\n    var result = 0.0;\n    var aIndex = outputRow * parameters.aRowStride;\n    var bIndex = outputCol * parameters.bColStride;\n    for (var aCol = 0u; aCol < parameters.aCols; aCol = aCol + 1u) {\n        result = result + a[aIndex] * b[bIndex];\n        aIndex = aIndex + parameters.aColStride;\n        bIndex = bIndex + parameters.bRowStride;\n    }\n    let outputIndex = outputCol + outputRow * parameters.bCols;\n    output[outputIndex] = result;\n"},bmm:{name:"bmm",config:[{name:"resultDtype"}],parameters:[{name:"batchSize",shaderType:"u32"},{name:"aRows",shaderType:"u32"},{name:"aCols",shaderType:"u32"},{name:"bCols",shaderType:"u32"},{name:"aBatchStride",shaderType:"u32"},{name:"aRowStride",shaderType:"u32"},{name:"aColStride",shaderType:"u32"},{name:"bBatchStride",shaderType:"u32"},{name:"bRowStride",shaderType:"u32"},{name:"bColStride",shaderType:"u32"},{name:"alpha",shaderType:"f32"}],inputs:[{name:"a",shaderType:"array<f32>"},{name:"b",shaderType:"array<f32>"}],outputs:[{name:"output",shaderType:"array<f32>",size:"batchSize * aRows * bCols"}],workgroupSize:[8,8,4],workgroupCount:["aRows/8","bCols/8","batchSize/4"],shader:"\n    let outputRow = global_id.x;\n    let outputCol = global_id.y;\n    let outputBatch = global_id.z;\n    if (outputRow >= parameters.aRows || outputCol >= parameters.bCols || outputBatch >= parameters.batchSize) {\n        return;\n    }\n    var result = 0.0;\n    var aIndex = outputBatch * parameters.aBatchStride + outputRow * parameters.aRowStride;\n    var bIndex = outputBatch * parameters.bBatchStride + outputCol * parameters.bColStride;\n    for (var aCol = 0u; aCol < parameters.aCols; aCol = aCol + 1u) {\n        result = result + a[aIndex] * b[bIndex];\n        aIndex = aIndex + parameters.aColStride;\n        bIndex = bIndex + parameters.bRowStride;\n    }\n    let outputRowStride = parameters.bCols;\n    let outputBatchStride = parameters.aRows * outputRowStride;\n    let outputIndex = outputBatch * outputBatchStride + outputRow * outputRowStride + outputCol;\n    output[outputIndex] = result;\n"},sumDim:{name:"sumDim",config:[{name:"dtype"},{name:"workgroupSize"}],parameters:[{name:"size",shaderType:"u32"}],inputs:[{name:"dimToSum",shaderType:"u32"},{name:"inputShape",shaderType:"vec3<u32>"},{name:"inputStrides",shaderType:"vec3<u32>"}],outputs:[{name:"output",shaderType:"array<f32>",size:"size"}],workgroupSize:["workgroupSize","workgroupSize","workgroupSize"],workgroupCount:[1,1,1],shader:"\n        // Global index flattening for the reformatted 3D tensor\n        var flatGlobalId: u32 = global_id.x * parameters.inputStrides.x + global_id.y * parameters.inputStrides.y + global_id.z * parameters.inputStrides.z;\n    \n        // Initialize sum\n        var sum: f32 = 0.0;\n    \n        let numReductions: u32 = parameters.inputShape.y;\n    \n        // Sum reduction\n        for (var i: u32 = 0; i < numReductions; i = i + 1) {\n            // Compute the input index by adding the reduction offset to the current flat global index\n            var dataIndex: u32 = flatGlobalId + i * parameters.inputStrides.y;\n    \n            if (dataIndex < input.length()) {\n                // Accumulate the input value into sum\n                sum = sum + input[dataIndex];\n            }\n        }\n    \n        // Write the reduced sum value to output tensor\n        if (flatGlobalId < output.length()) {\n            output[flatGlobalId] = sum;\n        }\n    "},uniform_:{name:"uniform_",config:[{name:"dtype"}],parameters:[{name:"size",shaderType:"u32"},{name:"seed",shaderType:"u32"},{name:"lowerBound",shaderType:"f32"},{name:"upperBound",shaderType:"f32"}],inputs:[],outputs:[{name:"output",shaderType:"array<f32>",size:"size"}],workgroupSize:[256,1,1],workgroupCount:["size/256",1,1],shader:"\n        let outputIndex = global_id.x;\n        if (outputIndex >= parameters.size) {\n            return;\n        }\n\n        var b = 0u;\n        let seed = u32(u32(parameters.seed + outputIndex) * 1099087573u);\n        b = ((seed << 13) ^ seed) >> 19;\n        let z1 = ((seed & 429496729u) << 12) ^ b;\n        b = ((seed << 2) ^ seed) >> 25;\n        let z2 = ((seed & 4294967288u) << 4) ^ b;\n        b = ((seed << 3) ^ seed) >> 11;\n        let z3 = ((seed & 429496280u) << 17) ^ b;\n        let z4 = u32(u32(1664525u * seed) + 1013904223u);\n        let r = z1 ^ z2 ^ z3 ^ z4;\n        let u = f32(u32(r)) * 2.3283064365387e-10;\n\n        output[outputIndex] = parameters.lowerBound + u * (parameters.upperBound - parameters.lowerBound);\n    "}}},690:(e,r,n)=>{Object.defineProperty(r,"__esModule",{value:!0}),r.opKernelSpecs=r.kernels=void 0;const s=n(795),a=n(302);r.kernels={},r.opKernelSpecs=[];for(const[e,n]of Object.entries(a.registry)){const e=(0,s.opSpecToKernelSpecs)(n);for(const s of e)r.kernels[s.name]=s,r.opKernelSpecs.push([n,s])}},934:function(e,r,n){var s=this&&this.__createBinding||(Object.create?function(e,r,n,s){void 0===s&&(s=n);var a=Object.getOwnPropertyDescriptor(r,n);a&&!("get"in a?!r.__esModule:a.writable||a.configurable)||(a={enumerable:!0,get:function(){return r[n]}}),Object.defineProperty(e,s,a)}:function(e,r,n,s){void 0===s&&(s=n),e[s]=r[n]}),a=this&&this.__setModuleDefault||(Object.create?function(e,r){Object.defineProperty(e,"default",{enumerable:!0,value:r})}:function(e,r){e.default=r}),i=this&&this.__exportStar||function(e,r){for(var n in e)"default"===n||Object.prototype.hasOwnProperty.call(r,n)||s(r,e,n)},o=this&&this.__importStar||function(e){if(e&&e.__esModule)return e;var r={};if(null!=e)for(var n in e)"default"!==n&&Object.prototype.hasOwnProperty.call(e,n)&&s(r,e,n);return a(r,e),r};Object.defineProperty(r,"__esModule",{value:!0}),r.applications=r.diffusers=void 0,i(n(601),r),i(n(519),r),i(n(743),r),i(n(255),r),r.diffusers=o(n(324)),r.applications=o(n(102)),i(n(90),r)},743:(e,r,n)=>{Object.defineProperty(r,"__esModule",{value:!0}),r.ConvTranspose2d=r.Conv2d=r.AvgPooling2d=void 0;const s=n(601);class AvgPooling2d extends s.Module{}r.AvgPooling2d=AvgPooling2d;class Conv2d extends s.Module{inChannels;outChannels;constructor(e,r,n,s,a,i){super(),this.inChannels=e,this.outChannels=r}}r.Conv2d=Conv2d;class ConvTranspose2d extends s.Module{}r.ConvTranspose2d=ConvTranspose2d},102:(e,r,n)=>{Object.defineProperty(r,"__esModule",{value:!0}),r.DeepSDF=void 0;const s=n(934);class DeepSDF extends s.Module{pointEncoder;block1;distanceHead;constructor(e){super();const r=e?.depth??8,n=e?.width??128;this.pointEncoder=new s.Linear(3,n),this.block1=new s.Sequential;for(let e=0;e<r;e++)this.block1.push(new s.Sequential(new s.Linear(n,n),new s.ReLU));this.distanceHead=new s.Linear(n,1)}forward(e){let r=this.pointEncoder.forward(e);r=this.block1.forward(r);return this.distanceHead.forward(r)}}r.DeepSDF=DeepSDF},519:(e,r,n)=>{Object.defineProperty(r,"__esModule",{value:!0}),r.Linear=void 0;const s=n(882),a=n(601),i=n(500),o=n(336);class Linear extends a.Module{inFeatures;outFeatures;weight;bias;constructor(e,r,n=!0,i,o){super(),this.inFeatures=e,this.outFeatures=r,this.weight=new a.Parameter((0,s.empty)([r,e],o,i)),n?this.bias=new a.Parameter((0,s.empty)([r],o,i)):this.registerBuffer("bias",null),this.resetParameters()}resetParameters(){if((0,o.kaimingUniform_)(this.weight,Math.sqrt(5)),this.bias){const[e,r]=(0,o.calculateFanInAndFanOut)(this.weight),n=e>0?1/Math.sqrt(e):0;(0,o.uniform_)(this.bias,-n,n)}}forward(e){return(0,i.linear)(e,this.weight,this.bias)}}r.Linear=Linear},324:(e,r,n)=>{Object.defineProperty(r,"__esModule",{value:!0}),r.TimestepEmbedSequential=r.UNetModel=void 0;const s=n(519),a=n(255),i=n(743),o=n(601),u=n(90),p=n(500);class UNetModel extends o.Module{inChannels;modelChannels;outChannels;numResBlocks;attentionResolutions;dropout;channelMult;convResample;numClasses;useCheckpoint;dtype;numHeadChannels;_featureSize;timeEmbed;inputBlocks;middleBlock;outputBlocks;out;constructor(e){if(super(),-1===e.numHeads&&-1===e.numHeadChannels)throw new Error("Must specify either numHeads or numHeadChannels");if(-1===e.numHeadChannels&&-1===e.numHeads)throw new Error("Must specify either numHeads or numHeadChannels");this.inChannels=e.inChannels,this.modelChannels=e.modelChannels,this.outChannels=e.outChannels,this.numResBlocks=e.numResBlocks||2,this.attentionResolutions=e.attentionResolutions||[],this.dropout=e.dropout||0,this.channelMult=e.channelMult||[1,2,4,8],this.convResample=void 0===e.convResample||e.convResample,this.numClasses=e.numClasses||null,this.useCheckpoint=e.useCheckpoint||!1,this.dtype=e.dtype||"float32",this.numHeadChannels=e.numHeadChannels||-1;const r=e.contextDim||null,n=e.dims||2,s=e.resblockUpdown||!1,a=e.transformerDepth||1,i=e.useCheckpoint||!1,p=e.useNewAttentionOrder||!1,d=4*e.modelChannels;this.timeEmbed=new o.Sequential(linear(this.modelChannels,d),new u.SiLU,linear(d,d)),this.inputBlocks=new o.ModuleList([new TimestepEmbedSequential(conv_nd(n,this.inChannels,this.modelChannels,3,1,1,this.dtype))]),this._featureSize=this.modelChannels;let h=e.numHeads||-1;const c=e.numHeadsUpSample||-1;let l=0;const f=[this.modelChannels];let g=this.modelChannels,y=1;for(const[o,u]of this.channelMult.entries()){for(let s=0;s<this.numResBlocks;s++){const s=[new ResBlock(g,d,u*this.modelChannels,n,!1,!1,this.dropout,this.useCheckpoint,void 0,e.useScaleShiftNorm)];g=u*this.modelChannels,this.attentionResolutions.includes(y)&&(-1===this.numHeadChannels?l=g/h:(h=g/this.numHeadChannels,l=this.numHeadChannels),e.useSpatialTransformer?s.push(new SpatialTransformer(g,h,l,a,0,r)):s.push(new AttentionBlock(g,h,l,i,p)))}if(o!=this.channelMult.length-1){const r=g;this.inputBlocks.push(new TimestepEmbedSequential(s?new ResBlock(g,d,r,n,!1,!0,this.dropout,i,void 0,e.useScaleShiftNorm):new Downsample(g,this.convResample,n,r))),g=r,f.push(g),y*=2,this._featureSize+=g}}-1===this.numHeadChannels?l=g/h:(h=g/this.numHeadChannels,l=this.numHeadChannels),this.middleBlock=new TimestepEmbedSequential(new ResBlock(g,d,g,n,!1,!1,this.dropout,this.useCheckpoint,void 0,e.useScaleShiftNorm),e.useSpatialTransformer?new SpatialTransformer(g,h,l,a,0,r):new AttentionBlock(g,h,l,i,p),new ResBlock(g,d,g,n,!1,!1,this.dropout,this.useCheckpoint,void 0,e.useScaleShiftNorm)),this._featureSize+=g,this.outputBlocks=new o.ModuleList([]);for(const[o,u]of this.channelMult.entries())for(let S=0;S<this.numResBlocks+1;S++){const m=f.pop()??0,w=[new ResBlock(g+m,d,this.modelChannels*u,n,!1,!1,this.dropout,this.useCheckpoint,void 0,e.useScaleShiftNorm)];if(g=this.modelChannels*u,this.attentionResolutions.includes(y)&&(-1===this.numHeadChannels?l=g/h:(h=g/this.numHeadChannels,l=this.numHeadChannels),w.push(e.useSpatialTransformer?new SpatialTransformer(g,h,l,a,0,r):new AttentionBlock(g,c,l,i,p))),o>0&&S==this.numResBlocks){const r=g;w.push(s?new ResBlock(g,d,r,n,!0,!1,this.dropout,this.useCheckpoint,void 0,e.useScaleShiftNorm):new Upsample(g,this.convResample,n,r)),y/=2}this.outputBlocks.push(new TimestepEmbedSequential(...w)),this._featureSize+=g}this.out=new o.Sequential(function normalization(e){return new GroupNorm32(32,e)}(g),new u.SiLU,function zeroModule(e){for(const r of e.parameters())r.detach().zero_();return e}(conv_nd(n,this.modelChannels,this.outChannels,3,1,1,this.dtype)))}forward(e,r,n,s){if(void 0!==s!=(null!==this.numClasses))throw new Error("Must specify y if and only if numClasses is set (the model is cross-conditional)");const a=[],i=function timestepEmbedding(e,r,n,s=!1,a=1e4){throw s?new Error("Not implemented due to missing repeat"):new Error("Not implemented due to missing arange, zerosLike, slicing")}(0,this.modelChannels,this.dtype,!1),o=this.timeEmbed.forward(i);if(null!==this.numClasses&&(1!=s?.shape.length||s.shape[0]!=e.shape[0]))throw new Error("y must be a 1-D batch of labels whose batch size matches x");let u=e;for(const e of this.inputBlocks)u=e.forward(u,o,n),a.push(u);u=this.middleBlock.forward(u,o,n);for(const e of this.outputBlocks)u=(0,p.cat)([u,a.pop()],1),u=e.forward(u,o,n);return this.out.forward(u)}}function conv_nd(e,r,n,s,a,o,u){if(2===e)return new i.Conv2d(r,n,s,a,o,u);throw new Error(`conv_nd: dims ${e} not implemented`)}function linear(e,r){return new s.Linear(e,r)}r.UNetModel=UNetModel;class AttentionBlock extends o.Module{constructor(e,r,n,s,a){super()}}class Downsample extends o.Module{constructor(e,r,n=2,s,a=1){super()}}class Upsample extends o.Module{constructor(e,r,n=2,s,a=1){super()}}class GroupNorm32 extends a.GroupNorm{constructor(e,r){super(e,r)}}class SpatialTransformer extends o.Module{constructor(e,r,n,s=1,a=0,i=null){super()}}class TimestepBlock extends o.Module{}class TimestepEmbedSequential extends TimestepBlock{constructor(...e){super();for(const[r,n]of e.entries())this.addModule(r,n)}forward(e,r,n){for(const n of this.children)e=n instanceof TimestepBlock?n.forward(e,r):n(e);return e}}r.TimestepEmbedSequential=TimestepEmbedSequential;class ResBlock extends TimestepBlock{constructor(e,r,n,s=2,a=!1,i=!1,o=0,u=!1,p=!1,d=!1){super()}}},601:(e,r,n)=>{Object.defineProperty(r,"__esModule",{value:!0}),r.Sequential=r.ModuleList=r.Container=r.Parameter=r.Module=void 0;const s=n(967);class Module{_children=null;get namedChildren(){if(null===this._children){this._children=[];for(const e in this){const r=this[e];r instanceof Module&&this._children.push([e,r])}}return this._children}get children(){return this.namedChildren.map((([e,r])=>r))}_parameters=null;get immediateParameters(){if(null===this._parameters){this._parameters=[];for(const e in this){const r=this[e];r instanceof Parameter&&this._parameters.push([e,r])}}return this._parameters}_buffers=[];_nonPersistentBuffersSet=new Set;_training=!0;get training(){return this._training}get[Symbol.toStringTag](){return"Module"}toString(){const e=[];for(const[r,n]of this.namedChildren){const s=n.toString().split("\n"),a=1===s.length?s[0]:s[0]+"\n"+s.splice(1).map((e=>`  ${e}`)).join("\n");e.push(`(${r}): ${a}`)}const r=e;let n=this.constructor.name+"(";return r.length>0&&(n+=`\n  ${r.join("\n  ")}\n`),n+=")",n}addModule(e,r){if(void 0!==this[e])throw new Error(`Module already has a child named ${e}`);if("string"==typeof e&&-1!==e.indexOf("."))throw new Error('Module name cannot contain "."');this[e]=r,this._children=null}*namedModules(e,r="",n=!0){if(!(e=e||new Set).has(this)){n&&e.add(this),yield[r,this];for(const[s,a]of this.namedChildren){if(!a)continue;const i=r?`${r}.${s}`:`${s}`;yield*a.namedModules(e,i,n)}}}*modules(){for(const[e,r]of this.namedModules())yield r}*_named_members(e,r="",n=!0,s=!0){const a=new Set,i=n?this.namedModules(void 0,r,s):[[r,this]];for(var[o,u]of i){const r=e(u);for(var[p,d]of r){if(null==d||a.has(d))continue;s&&a.add(d);const e=o?`${o}.${p}`:p;yield[e,d]}}}namedParameters(e="",r=!0,n=!0){return this._named_members((e=>e.immediateParameters),e,r,n)}*parameters(){for(const[e,r]of this.namedParameters())yield r}registerBuffer(e,r,n=!0){if(!e)throw new Error("Buffer name must not be empty.");if(-1!==e.indexOf("."))throw new Error("Buffer name must not contain a period.");if(this._buffers.some((([r,n])=>r===e)))throw new Error(`Buffer ${e} already registered.`);this._buffers.push([e,r]),n?this._nonPersistentBuffersSet.delete(e):this._nonPersistentBuffersSet.add(e)}namedBuffers(e="",r=!0,n=!0){return this._named_members((e=>e._buffers),e,r,n)}*buffers(){for(const[e,r]of this.namedBuffers())yield r}getBuffer(e){const r=this._buffers.find((([r,n])=>r===e));return r?r[1]:null}train(e=!0){this._training=e;for(const r of this.children)r.train(e);return this}eval(){return this.train(!1)}requiresGrad(e=!0){for(const r of this.parameters())r.requiresGrad=e;return this}zeroGrad(e=!0){for(const r of this.parameters())if(r.grad){if(!e)throw r.grad.gradFunc?r.grad.detach():r.grad.requiresGrad=!1,r.grad.zero_(),new Error("Not implemented: Cannot set gradients to 0. Use setToNull=true instead.");r.grad=null}return this}stateDict(e,r="",n=!0){e=e||{},this._saveToStateDict(e,r,n);for(const[s,a]of this.namedChildren)a&&a.stateDict(e,r+s+".",n);return e}_saveToStateDict(e,r,n){for(const[s,a]of this.namedParameters())a&&(e[r+s]=n?a:a.detach());for(const[s,a]of this.namedBuffers())a&&!this._nonPersistentBuffersSet.has(s)&&(e[r+s]=n?a:a.detach())}loadStateDict(e){!function load(e,r,n){e._loadFromStateDict(r,n);for(const[s,a]of e.namedChildren)if(a){const e=n+s+".",i={};for(const[n,s]of Object.entries(r))n.startsWith(e)&&(i[n]=s);load(a,i,e)}}(this,e,"")}_loadFromStateDict(e,r){throw new Error("State dict loading not implemented")}}r.Module=Module;class Parameter extends s.Tensor{constructor(e,r=!0){super({data:e.storage,dtype:e.dtype,shape:e.shape,strides:e.strides,requiresGrad:r,device:e.device})}}r.Parameter=Parameter;class Container extends Module{}r.Container=Container;class ModuleList extends Module{get length(){return this.children.length}[Symbol.iterator](){return this.children[Symbol.iterator]()}constructor(e=[]){super();for(const[r,n]of e.entries())this.addModule(r,n)}push(e){this.addModule(this.children.length,e)}}r.ModuleList=ModuleList;class Sequential extends Container{get length(){return this.children.length}[Symbol.iterator](){return this.children[Symbol.iterator]()}constructor(...e){super();for(const r of e)this.push(r)}push(e){this.addModule(this.children.length,e)}forward(e){for(const r of this.children)e=r.forward(e);return e}}r.Sequential=Sequential},255:(e,r,n)=>{Object.defineProperty(r,"__esModule",{value:!0}),r.GroupNorm=void 0;const s=n(601);class GroupNorm extends s.Module{numGroups;numChannels;constructor(e,r){super(),this.numGroups=e,this.numChannels=r}}r.GroupNorm=GroupNorm},90:(e,r,n)=>{Object.defineProperty(r,"__esModule",{value:!0}),r.Tanh=r.SiLU=r.Sigmoid=r.ReLU=void 0;const s=n(601);class ReLU extends s.Module{forward(e){return e.relu()}}r.ReLU=ReLU;class Sigmoid extends s.Module{forward(e){return e.sigmoid()}}r.Sigmoid=Sigmoid;class SiLU extends s.Module{forward(e){return e.silu()}}r.SiLU=SiLU;class Tanh extends s.Module{forward(e){return e.tanh()}}r.Tanh=Tanh},302:(e,r)=>{Object.defineProperty(r,"__esModule",{value:!0}),r.registry=void 0,r.registry=[{name:"abs",aliases:["absolute"],type:"unary",forward:"output = abs(input)",backward:"inputGrad = input == 0 ? 0 : (input > 0 ? outputGrad : -outputGrad)"},{name:"acos",aliases:["arccos"],type:"unary",forward:"output = acos(input)",backward:"inputGrad = -outputGrad / sqrt(1 - input * input)"},{name:"acosh",aliases:["arccosh"],type:"unary",forward:"output = acosh(input)",backward:"inputGrad = outputGrad / sqrt(input * input - 1)"},{name:"add",type:"binary",alpha:!0,forward:"output = input + other * alpha",backward:"inputGrad = outputGrad; otherGrad = outputGrad"},{name:"asin",aliases:["arcsin"],type:"unary",forward:"output = asin(input)",backward:"inputGrad = outputGrad / sqrt(1 - input * input)"},{name:"asinh",aliases:["arcsinh"],type:"unary",forward:"output = asinh(input)",backward:"inputGrad = outputGrad / sqrt(input * input + 1)"},{name:"atan",aliases:["arctan"],type:"unary",forward:"output = atan(input)",backward:"inputGrad = outputGrad / (input * input + 1)"},{name:"atan2",aliases:["arctan2"],type:"binary",forward:"output = atan2(input, other)",backward:"inputGrad = outputGrad * other / (input * input + other * other); otherGrad = -outputGrad * input / (input * input + other * other)"},{name:"ceil",type:"unary",forward:"output = ceil(input)",backward:"inputGrad = 0"},{name:"copysign",type:"binary",forward:"output = other >= 0 ? abs(input) : -abs(input)",backward:"var dir = other >= 0 ? (input >= 0 ? 1.0 : -1.0) : (input >= 0 ? -1.0 : 1.0); inputGrad = input == 0.0 ? 0.0 : outputGrad * dir; otherGrad = 0"},{name:"cos",type:"unary",forward:"output = cos(input)",backward:"inputGrad = -outputGrad * sin(input)"},{name:"cosh",type:"unary",forward:"output = cosh(input)",backward:"inputGrad = outputGrad * sinh(input)"},{name:"deg2rad",type:"unary",forward:"output = input * 0.017453292519943295",backward:"inputGrad = outputGrad * 0.017453292519943295"},{name:"div",aliases:["divide"],type:"binary",forward:"output = input / other",backward:"inputGrad = outputGrad / other; otherGrad = -outputGrad * input / (other * other)"},{name:"exp",type:"unary",forward:"output = exp(input)",backward:"inputGrad = outputGrad * exp(input)"},{name:"exp2",type:"unary",forward:"output = exp2(input)",backward:"inputGrad = outputGrad * exp2(input) * 0.6931471805599453"},{name:"expm1",type:"unary",forward:"output = exp(input) - 1.0",backward:"inputGrad = outputGrad * exp(input)"},{name:"floor",type:"unary",forward:"output = floor(input)",backward:"inputGrad = 0"},{name:"frac",type:"unary",forward:"output = input >= 0.0 ? fract(input) : -fract(-input)",backward:"inputGrad = outputGrad"},{name:"hypot",type:"binary",forward:"output = sqrt(input * input + other * other)",backward:"inputGrad = outputGrad * input / sqrt(input * input + other * other); otherGrad = outputGrad * other / sqrt(input * input + other * other)"},{name:"ldexp",type:"binary",forward:"output = input * pow(2.0, other)",backward:"var out = pow(2.0, other); inputGrad = outputGrad * out; otherGrad = outputGrad * input * out * 0.6931471805599453"},{name:"log",type:"unary",forward:"output = log(input)",backward:"inputGrad = outputGrad / input"},{name:"log10",type:"unary",forward:"output = log(input) * 0.4342944819032518",backward:"inputGrad = outputGrad / (input * 2.302585092994046)"},{name:"log1p",type:"unary",forward:"output = log(input + 1.0)",backward:"inputGrad = outputGrad / (input + 1.0)"},{name:"log2",type:"unary",forward:"output = log2(input)",backward:"inputGrad = outputGrad / (input * 0.6931471805599453)"},{name:"logaddexp",type:"binary",forward:"output = log(exp(input) + exp(other))",backward:"var ein = exp(input); var eoth = exp(other); var addeinv = outputGrad/(ein + eoth); inputGrad = addeinv * ein; otherGrad = addeinv * eoth"},{name:"logaddexp2",type:"binary",forward:"output = log2(exp2(input) + exp2(other))",backward:"var ein = exp2(input); var eoth = exp2(other); var sum_ein_eoth = ein + eoth; inputGrad = outputGrad * (ein / sum_ein_eoth); otherGrad = outputGrad * (eoth / sum_ein_eoth );"},{name:"mul",aliases:["multiply"],type:"binary",forward:"output = input * other",backward:"inputGrad = outputGrad * other; otherGrad = outputGrad * input"},{name:"neg",aliases:["negative"],type:"unary",forward:"output = -input",backward:"inputGrad = -outputGrad"},{name:"positive",type:"unary",forward:"output = input",backward:"inputGrad = outputGrad"},{name:"pow",type:"binary",forward:"output = input >= 0 || fract(other) != 0 ? pow(input, other) :\n            pow(-input, other) * ((i32(other) & 1) != 0 ? -1f : 1f)",backward:"inputGrad = input >= 0 || fract(other) != 0 ? outputGrad * other * pow(input, other - 1.0) :\n            outputGrad * other * pow(-input, other - 1) * ((i32(other - 1) & 1) != 0 ? -1f : 1f);\n        otherGrad = outputGrad * pow(input, other) * log(input)"},{name:"rad2deg",type:"unary",forward:"output = input * 57.29577951308232",backward:"inputGrad = outputGrad * 57.29577951308232"},{name:"reciprocal",type:"unary",forward:"output = 1.0 / input",backward:"inputGrad = -outputGrad / (input * input)"},{name:"relu",nnName:"ReLU",nnOp:!0,type:"unary",forward:"output = max(input, 0.0)",backward:"inputGrad = input > 0.0 ? outputGrad : 0.0"},{name:"round",type:"unary",forward:"output = round(input)",backward:"inputGrad = 0"},{name:"rsqrt",type:"unary",forward:"output = 1.0 / sqrt(input)",backward:"inputGrad = -outputGrad / (2.0 * sqrt(input) * input)"},{name:"sigmoid",nnOp:!0,type:"unary",forward:"output = 1.0 / (1.0 + exp(-input))",backward:"var out = 1.0 / (1.0 + exp(-input)); inputGrad = outputGrad * out * (1.0 - out)"},{name:"sign",type:"unary",forward:"output = sign(input)",backward:"inputGrad = 0"},{name:"silu",nnName:"SiLU",torchName:"torch.nn.functional.silu",nnOp:!0,type:"unary",forward:"output = input / (1.0 + exp(-input))",backward:"var out = 1.0 / (1.0 + exp(-input)); inputGrad = outputGrad * (out + input * out * (1.0 - out))"},{name:"sin",type:"unary",forward:"output = sin(input)",backward:"inputGrad = outputGrad * cos(input)"},{name:"sinc",type:"unary",forward:"var inpi = input * 3.141592653589793; output = input == 0.0 ? 1.0 : sin(inpi) / inpi",backward:"var inpi = input * 3.141592653589793; inputGrad = input == 0.0 ? 0.0 : (outputGrad * 3.141592653589793 * (inpi*cos(inpi) - sin(inpi)) / (inpi*inpi))"},{name:"sinh",type:"unary",forward:"output = sinh(input)",backward:"inputGrad = outputGrad * cosh(input)"},{name:"sqrt",type:"unary",forward:"output = sqrt(input)",backward:"inputGrad = outputGrad / (2.0 * sqrt(input))"},{name:"square",type:"unary",forward:"output = input * input",backward:"inputGrad = outputGrad * 2.0 * input"},{name:"sub",aliases:["subtract"],type:"binary",alpha:!0,forward:"output = input - other * alpha",backward:"inputGrad = outputGrad; otherGrad = -outputGrad"},{name:"tan",type:"unary",forward:"output = tan(input)",backward:"inputGrad = outputGrad / (cos(input) * cos(input))",precision:5},{name:"tanh",nnOp:!0,type:"unary",forward:"output = tanh(input)",backward:"inputGrad = outputGrad * (1.0 - tanh(input) * tanh(input))"},{name:"trunc",aliases:["fix"],type:"unary",forward:"output = trunc(input)",backward:"inputGrad = 0"},{name:"xlogy",type:"binary",forward:"output = input == 0.0 ? 0.0 : input * log(other)",backward:"inputGrad = input == 0.0 ? 0.0 : outputGrad * log(other); otherGrad = input == 0.0 ? 0.0 : outputGrad * (input / other);"},{name:"all",type:"reduction",combineOp:"&&",init:"output = 1",forward:"output = output && input",backward:"inputGrad = output ? outputGrad : 0.0"},{name:"any",type:"reduction",combineOp:"||",init:"output = 0",forward:"output = output || input",backward:"inputGrad = output ? outputGrad : 0.0"},{name:"mean",type:"reduction",combineOp:"+",init:"output = 0.0",forward:"output = output + input",reduce:"output = output / f32(inputSize)",backward:"inputGrad = outputGrad / f32(inputSize)"},{name:"norm",type:"reduction",combineOp:"+",init:"output = 0.0",forward:"output = output + input * input",reduce:"output = sqrt(output)",backward:"inputGrad = outputGrad * input / output"},{name:"prod",type:"reduction",combineOp:"*",init:"output = 1.0",forward:"output = output * input",backward:"inputGrad = outputGrad * output / input"},{name:"sum",type:"reduction",combineOp:"+",init:"output = 0.0",forward:"output = output + input",backward:"inputGrad = outputGrad"},{name:"countNonzero",type:"reduction",combineOp:"+",init:"output = 0.0",forward:"output = output + (input != 0)"}]},795:(e,r,n)=>{Object.defineProperty(r,"__esModule",{value:!0}),r.opSpecToKernelSpecs=r.CodeWriter=void 0;const s=n(653);function getReductionKernelSpec(e){const r=(0,s.exprCodeToWebGLShader)(e.init,{input:"input[local_id.x]",output:"accumulator"}),n=(0,s.exprCodeToWebGLShader)(e.forward,{input:"input[i]",output:"accumulator"}),a=void 0===e.reduce?"":(0,s.exprCodeToWebGLShader)(e.reduce,{input:"input[i]",output:"accumulator",inputSize:"parameters.size"}),i=`\n    var ${r};\n    // Load inputData into local memory\n    for (var i = local_id.x; i < parameters.size; i += $$workgroupSize$$) {\n        ${n};\n    }\n    // Write partial group sum to outputData\n    output[local_id.x] = accumulator;\n\n    workgroupBarrier(); // Make sure all threads have completed reduction\n\n    // First thread sums up results from all other threads\n    if (local_id.x == 0u) {\n        var numToSum = min(parameters.size, $$workgroupSize$$u);\n        for (var i = 1u; i < numToSum; i++) {\n            accumulator ${e.combineOp}= output[i];\n        }\n        // Store final reduction in the first element of result array\n        ${a};\n        output[0] = accumulator;\n    }\n`;return{name:e.name,config:[{name:"dtype"},{name:"workgroupSize"}],parameters:[{name:"size",shaderType:"u32"}],inputs:[{name:"input",shaderType:"array<f32>"}],outputs:[{name:"output",shaderType:"array<f32>",size:"workgroupSize"}],workgroupSize:["workgroupSize",1,1],workgroupCount:[1,1,1],shader:i}}function getReductionDimKernelSpec(e){let r=`\n    let outputIndex = global_id.x;\n    if (outputIndex >= parameters.size) {\n        return;\n    }\n    var i = outputIndex;\n    var outputIndex0 = u32(i / parameters.outputStride0);\n    i = i % parameters.outputStride0;\n    var outputIndex1 = u32(i / parameters.outputStride1);\n    i = i % parameters.outputStride1;\n    var outputIndex2 = u32(i / parameters.outputStride2);\n    i = i % parameters.outputStride2;\n    var outputIndex3 = i;\n    let dimN = parameters.inputShape$$dim$$;\n    var ${(0,s.exprCodeToWebGLShader)(e.init,{input:"input[inputIndex]",output:"accumulator"})};\n    for (var dimI = 0u; dimI < dimN; dimI++) {\n        outputIndex$$dim$$ = dimI;\n        let inputIndex =\n            outputIndex0 * parameters.inputStride0 +\n            outputIndex1 * parameters.inputStride1 +\n            outputIndex2 * parameters.inputStride2 +\n            outputIndex3;\n        ${(0,s.exprCodeToWebGLShader)(e.forward,{input:"input[inputIndex]",output:"accumulator"})};\n    }\n    ${void 0===e.reduce?"":(0,s.exprCodeToWebGLShader)(e.reduce,{input:"input[inputIndex]",output:"accumulator",inputSize:"dimN"})};\n    output[outputIndex] = accumulator;\n`;return{name:e.name+"_dim",config:[{name:"dtype"},{name:"dim"},{name:"maxdim"}],parameters:[{name:"inputShape0",shaderType:"u32"},{name:"inputShape1",shaderType:"u32"},{name:"inputShape2",shaderType:"u32"},{name:"inputShape3",shaderType:"u32"},{name:"inputStride0",shaderType:"u32"},{name:"inputStride1",shaderType:"u32"},{name:"inputStride2",shaderType:"u32"},{name:"inputStride3",shaderType:"u32"},{name:"outputStride0",shaderType:"u32"},{name:"outputStride1",shaderType:"u32"},{name:"outputStride2",shaderType:"u32"},{name:"outputStride3",shaderType:"u32"},{name:"size",shaderType:"u32"}],inputs:[{name:"input",shaderType:"array<f32>"}],outputs:[{name:"output",shaderType:"array<f32>",size:"size"}],workgroupSize:[256,1,1],workgroupCount:["size/256",1,1],shader:r}}function getBinaryKernelSpec(e,r,n,a){const i=[{name:"size",shaderType:"u32"}];if(a)for(let e=0;e<4;e++)i.push({name:`inputStrides${e}`,shaderType:"u32"}),i.push({name:`otherStrides${e}`,shaderType:"u32"}),i.push({name:`outputStrides${e}`,shaderType:"u32"});const o={input:"input[inputIndex]",other:"other[otherIndex]",output:"output[outputIndex]"};r&&(o.output="input[outputIndex]"),n&&(o.other="parameters.other",i.push({name:"other",shaderType:"f32"})),void 0!==e.alpha&&e.alpha&&(i.push({name:"alpha",shaderType:"f32"}),o.alpha="parameters.alpha");const u=(0,s.exprCodeToWebGLShader)(e.forward,o);let p;p=a?`\n        let outputIndex = global_id.x;\n        if (outputIndex >= parameters.size) {\n            return;\n        }\n        var i = outputIndex;\n        let outputIndex0 = u32(i / parameters.outputStrides0);\n        i = i % parameters.outputStrides0;\n        let outputIndex1 = u32(i / parameters.outputStrides1);\n        i = i % parameters.outputStrides1;\n        let outputIndex2 = u32(i / parameters.outputStrides2);\n        i = i % parameters.outputStrides2;\n        let outputIndex3 = i;\n        let inputIndex =\n            outputIndex0 * parameters.inputStrides0 +\n            outputIndex1 * parameters.inputStrides1 +\n            outputIndex2 * parameters.inputStrides2 +\n            outputIndex3;\n        let otherIndex =\n            outputIndex0 * parameters.otherStrides0 +\n            outputIndex1 * parameters.otherStrides1 +\n            outputIndex2 * parameters.otherStrides2 +\n            outputIndex3;\n        ${u};`:`\n        let outputIndex = global_id.x;\n        if (outputIndex >= parameters.size) {\n            return;\n        }\n        let inputIndex = outputIndex;\n        let otherIndex = outputIndex;\n        ${u};`;const d=[];n||d.push({name:"other",shaderType:"array<f32>"});let h="input";r||(d.splice(0,0,{name:"input",shaderType:"array<f32>"}),h="output");let c=e.name;return n&&(c+="_scalar"),a&&(c+="_strided"),r&&(c+="_"),{name:c,config:[{name:"dtype"}],parameters:i,inputs:d,outputs:[{name:h,shaderType:"array<f32>",size:"size"}],workgroupSize:[256,1,1],workgroupCount:["size/256",1,1],shader:p}}function getBinaryGradKernelSpec(e,r,n){const a=[{name:"size",shaderType:"u32"}],i={input:"input[global_id.x]",inputGrad:"inputGrad[global_id.x]",output:"output[global_id.x]",outputGrad:"outputGrad[global_id.x]",other:"other[global_id.x]",otherGrad:"otherGrad[global_id.x]"};n&&(i.other="parameters.other",i.otherGrad="otherGrad",a.push({name:"other",shaderType:"f32"})),void 0!==e.alpha&&e.alpha&&(a.push({name:"alpha",shaderType:"f32"}),i.alpha="parameters.alpha");const o=(0,s.parseCode)(r),u=(0,s.substituteIdentifiers)(o,i),p=`\n        if (global_id.x >= parameters.size) {\n            return;\n        }\n        ${n?"var otherGrad = 0.0;":""}\n        ${(0,s.exprNodeToWebGLShader)(u)};`,d=[{name:"input",shaderType:"array<f32>"}];n||d.push({name:"other",shaderType:"array<f32>"}),d.push({name:"outputGrad",shaderType:"array<f32>"});const h=[{name:"inputGrad",shaderType:"array<f32>",size:"size"}];return n||h.push({name:"otherGrad",shaderType:"array<f32>",size:"size"}),{name:e.name+(n?"_scalar":"")+"_grad",config:[{name:"dtype"}],parameters:a,inputs:d,outputs:h,workgroupSize:[256,1,1],workgroupCount:["size/256",1,1],shader:p}}function getUnaryKernelSpec(e){const r=[{name:"size",shaderType:"u32"}],n={input:"input[index]",output:"output[index]"};void 0!==e.alpha&&e.alpha&&(r.push({name:"alpha",shaderType:"f32"}),n.alpha="parameters.alpha");const a=(0,s.parseCode)(e.forward),i=(0,s.substituteIdentifiers)(a,n),o=`\n        var index = global_id.x;\n        if (index >= parameters.size) {\n            return;\n        }\n        ${(0,s.exprNodeToWebGLShader)(i)};`;return{name:e.name,config:[{name:"dtype"}],parameters:r,inputs:[{name:"input",shaderType:"array<f32>"}],outputs:[{name:"output",shaderType:"array<f32>",size:"size"}],workgroupSize:[256,1,1],workgroupCount:["size/256",1,1],shader:o}}function getUnaryInplaceKernelSpec(e){const r=[{name:"size",shaderType:"u32"}],n=(0,s.parseCode)(e.forward),a={input:"input[global_id.x]",output:"input[global_id.x]"};void 0!==e.alpha&&e.alpha&&(r.push({name:"alpha",shaderType:"f32"}),a.alpha="parameters.alpha");const i=(0,s.substituteIdentifiers)(n,a),o=`\n        if (global_id.x >= parameters.size) {\n            return;\n        }\n        ${(0,s.exprNodeToWebGLShader)(i)};`;return{name:e.name+"_",config:[{name:"dtype"}],parameters:r,inputs:[],outputs:[{name:"input",shaderType:"array<f32>",size:"size"}],workgroupSize:[256,1,1],workgroupCount:["size/256",1,1],shader:o}}r.CodeWriter=class CodeWriter{indentLevel=0;lines=[];indent(){this.indentLevel++}dedent(){this.indentLevel--}writeLine(e){this.lines.push("    ".repeat(this.indentLevel)+e)}toString(){return this.lines.join("\n")}},r.opSpecToKernelSpecs=function opSpecToKernelSpecs(e){return"reduction"==e.type?function getReductionKernelSpecs(e){const r=[getReductionKernelSpec(e),getReductionDimKernelSpec(e)];e.backward&&r.push(function getReductionGradKernelSpec(e,r){const n=`\n    let index = global_id.x;\n    if (index >= parameters.size) {\n        return;\n    }\n    ${(0,s.exprCodeToWebGLShader)(r,{input:"input[index]",output:"output[0]",outputGrad:"outputGrad[0]",inputGrad:"inputGrad[index]",inputSize:"parameters.size"})};\n`;return{name:e.name+"_grad",config:[{name:"dtype"},{name:"workgroupSize"}],parameters:[{name:"size",shaderType:"u32"}],inputs:[{name:"input",shaderType:"array<f32>"},{name:"output",shaderType:"array<f32>"},{name:"outputGrad",shaderType:"array<f32>"}],outputs:[{name:"inputGrad",shaderType:"array<f32>",size:"size"}],workgroupSize:["workgroupSize",1,1],workgroupCount:["size/workgroupSize",1,1],shader:n}}(e,e.backward));return r}(e):"binary"==e.type?function getBinaryKernelSpecs(e){const r=[getBinaryKernelSpec(e,!1,!1,!1),getBinaryKernelSpec(e,!0,!1,!1),getBinaryKernelSpec(e,!1,!0,!1),getBinaryKernelSpec(e,!0,!0,!1),getBinaryKernelSpec(e,!1,!1,!0),getBinaryKernelSpec(e,!0,!1,!0)];e.backward&&(r.push(getBinaryGradKernelSpec(e,e.backward,!1)),r.push(getBinaryGradKernelSpec(e,e.backward,!0)));return r}(e):function getUnaryKernelSpecs(e){const r=[getUnaryKernelSpec(e),getUnaryInplaceKernelSpec(e)];e.backward&&r.push(function getUnaryGradKernelSpec(e,r){const n=[{name:"size",shaderType:"u32"}],a={input:"input[global_id.x]",inputGrad:"inputGrad[global_id.x]",output:"output[global_id.x]",outputGrad:"outputGrad[global_id.x]"};void 0!==e.alpha&&e.alpha&&(n.push({name:"alpha",shaderType:"f32"}),a.alpha="parameters.alpha");const i=(0,s.parseCode)(r),o=(0,s.substituteIdentifiers)(i,a),u=`\n        if (global_id.x >= parameters.size) {\n            return;\n        }\n        ${(0,s.exprNodeToWebGLShader)(o)};`;return{name:e.name+"_grad",config:[{name:"dtype"}],parameters:n,inputs:[{name:"input",shaderType:"array<f32>"},{name:"outputGrad",shaderType:"array<f32>"}],outputs:[{name:"inputGrad",shaderType:"array<f32>",size:"size"}],workgroupSize:[256,1,1],workgroupCount:["size/256",1,1],shader:u}}(e,e.backward));return r}(e)}},500:(e,r,n)=>{Object.defineProperty(r,"__esModule",{value:!0}),r.view=r.unsqueeze=r.tensor=r.t=r.squeeze=r.reshape=r.numel=r.mm=r.matmul=r.linear=r.gather=r.flatten=r.conv2d=r.clone=r.cat=void 0;const s=n(149),a=n(967),i=n(81),o=n(25);function flatten(e,r=0,n=-1){if((r=(0,i.canonicalizeDim)(e.ndim,r))==(n=(0,i.canonicalizeDim)(e.ndim,n))&&0!=e.ndim)return e;return null!==collapseViewHelper(e,r,n)?function collapseView(e,r,n){const s=collapseViewHelper(e,r,n);if(null===s)throw new Error("Attempting to view a collapsed tensor, but no such view exists!");return e.withShape(s.shape,s.strides)}(e,r,n):function collapse(e,r,n){throw new Error("collapse not implemented yet")}()}function splitDim(e,r,n){(0,i.validateIdx)(e.ndim,r),(0,i.validateDimLength)(n);const s=Math.floor(e.shape[r]/n);if(e.shape[r]%n!=0)throw new Error(`Attempting to split dimension of length ${e.shape[r]}, but out length of ${n} divides it with a remainder!`);const a=[],o=[];for(let i=0;i<e.ndim;i++)i===r?(a.push(n),a.push(s),o.push(e.strides[i]*s),o.push(e.strides[i])):(a.push(e.shape[i]),o.push(e.strides[i]));return e.withShape(a,o)}function collapseViewHelper(e,r,n){let s,a;if(function validateCollapseArgs(e,r,n){const s=Math.max(1,e.ndim);(0,i.validateIdx)(s,r),(0,i.validateIdx)(s,n),(0,i.check)(n>=r,(()=>`Attempting to collapse but end, ${n}, is less than start, ${r}`))}(e,r,n),0===e.ndim?(s=[1],a=[1]):(s=e.shape,a=e.strides),0===e.ndim||r===n)return{shape:s,strides:a};let o=s[n],u=a[n];for(let i=n-1;i>=r;i--){if(0===s[i]||0===s[i+1]){o=0,u=0;break}if(1!==s[i]&&(o*=s[i],u=Math.min(u,a[i]),e.numel()>0&&1!=s[i+1]&&a[i]!==a[i+1]*s[i+1]))return null}const p=s.slice(0,r).concat([o]).concat(s.slice(n+1));let d=a.slice(0,r).concat([u]).concat(a.slice(n+1));return 0===e.numel()&&(d=(0,i.defaultStrides)(p)),{shape:p,strides:d}}function primitiveReshape(e,r){throw new Error("Copying reshape not implemented")}function reshapeViewHelper(e,r,n=!1){const s=function inferSize(e,r){let n=null,s=1;for(let r=0;r<e.length;r++){let a=e[r];-1==a?((0,i.check)(null===n,(()=>"only one dimension can be inferred")),n=r):a>=0?s*=a:(0,i.check)(!1,(()=>`invalid shape dimension ${a}`))}return(0,i.check)(r==s||null!==n&&s>0&&r%s==0,(()=>`shape '[${e}]' is invalid for input of size ${r}`)),null!==n&&((0,i.check)(0!=s,(()=>`cannot reshape tensor of 0 elements into shape ${e} because the unspecified dimension size -1 can be any \nvalue and is ambiguous`)),e[n]=Math.floor(r/s)),e}(r,e.numel());if((0,i.shapesAreEqual)(e.shape,s))return e.withShape(e.shape,e.strides);if(0===e.numel())return e.withShape(s,(0,i.defaultStrides)(s));if(0===e.ndim){let r=e;for(let e of s){if(1!==e)throw new Error("Expected length to be 1.");r=squeeze(r,-1)}return r}if(0===s.length){let r=e;for(let n of e.shape){if(1!==n)throw new Error("Expected length to be 1.");r=squeeze(r,-1)}return r}let a=0,o=e;for(let r of s){if(a>=o.ndim){if(1!==r)throw new Error("Expected length to be 1.");let e=o.ndim-1;o=splitDim(o,e,o.shape[e]),a++;continue}if(r===o.shape[a]){a++;continue}let i=o.shape[a],u=a;for(;i%r!=0;)u++,i*=o.shape[u];if(u!==a){if(null===collapseViewHelper(o,a,u)){if(n)return primitiveReshape();throw new Error(`Cannot view a tensor with shape ${e.shape} and strides ${e.strides} as a tensor with shape ${s}!`)}o=flatten(o,a,u)}i!==r&&(o=splitDim(o,a,r)),a++}for(;a<o.ndim;){if(1!==o.shape[a])throw new Error("Expected shape at index "+a+" to be 1.");o=squeeze(o,a)}return o}function squeeze(e,r){let n;if(void 0===r){n=[];for(let r=0;r<e.shape.length;r++)1===e.shape[r]&&n.push(r)}else n="number"==typeof r?[r]:r;const s=e.shape.length,a=s>0?-s:-1,i=s>0?s-1:0;for(let r in n){let s=n[r];if(s<a||s>i)throw new Error(`Dimension out of range (expected to be in range of [${a}, ${i}], but got ${s})`);s<0&&(n[r]=e.shape.length+s)}n.sort();const o=[],u=[];let p=0;for(let r=0;r<s;r++)p<n.length&&r===n[p]?(1!==e.shape[r]&&(o.push(e.shape[r]),u.push(e.strides[r])),p++):(o.push(e.shape[r]),u.push(e.strides[r]));return e.withShape(o,u)}r.cat=function cat(e,r){throw new Error("cat not implemented yet")},r.clone=function clone(e,r="preserveFormat"){if((0,s.shouldCreateGradient)(e))throw new Error("clone gradient not supported yet");{const r=e.storage.clone();return new a.Tensor({data:r,shape:e.shape,dtype:e.dtype,requiresGrad:e.requiresGrad})}},r.conv2d=function conv2d(e,r,n,a,i){if((0,s.shouldCreateGradient)(e,r))throw new Error("conv2d gradient not supported yet");{if(4!==e.shape.length||4!==r.shape.length)throw new Error(`Expected image tensor, got ${e.shape} and kernel ${r.shape}`);if(e.shape[1]!==r.shape[1])throw new Error(`Expected number of chennels in input image to match number of channels in kernel, got ${e.shape} and ${r.shape}`);const n={batchSize:e.shape[0],inputChannels:e.shape[1],outputChannels:r.shape[0],inputHeight:e.shape[2],inputWidth:e.shape[3],kernelHeight:r.shape[2],kernelWidth:r.shape[3],outputHeight:e.shape[2]-r.shape[2]+1,outputWidth:e.shape[3]-r.shape[3]+1};return e.runKernel("conv2d",{dtype:e.dtype},n,[[n.batchSize,n.outputChannels,n.outputHeight,n.outputWidth]],r)[0]}},r.flatten=flatten,r.gather=function gather(e,r,n){return(0,s.shouldCreateGradient)(e,n)?o.GatherFunction.apply(e,r,n):o.GatherFunction.forward([e,r,n])},r.linear=function linear(e,r,n){return(0,s.shouldCreateGradient)(e,r,n)?o.LinearFunction.apply(e,r,n):o.LinearFunction.forward([e,r,n])},r.matmul=function matmul(e,r){const n={shape:e.shape,strides:e.strides},s={shape:r.shape,strides:r.strides},a=n.shape.length,o=s.shape.length;if(0===a||0===o)throw new Error("matmul requires at least 1D tensors");let u,p,d,h,c=e,l=r;if(1===a&&1===o){if(n.shape[0]!==s.shape[0])throw new Error(`inconsistent tensor size, expected tensor [${n.shape}] and src [${s.shape}] to have the same number of elements, but got ${n.shape[0]} and ${s.shape[0]} elements respectively`);u="dot",p=n,d=s,h=[]}else if(2===a&&2===o){if(u="mm",p=n,d=s,h=[n.shape[0],s.shape[1]],p.shape[1]!==s.shape[0])throw new Error(`mat1 and mat2 shapes cannot be multiplied (${n.shape[0]}x${n.shape[1]} and ${s.shape[0]}x${s.shape[1]})`)}else if(1===a&&2===o){const a=s.shape.slice(),i=s.strides.slice();if(a[o-1]=s.shape[o-2],a[o-2]=s.shape[o-1],i[o-1]=s.strides[o-2],i[o-2]=s.strides[o-1],u="mv",p={shape:a,strides:i},d=n,h=[s.shape[o-1]],c=r,l=e,p.shape[1]!==d.shape[0])throw new Error(`mat1 and mat2 shapes cannot be multiplied (1x${d.shape[0]} and ${p.shape[1]}x${p.shape[0]})`)}else if(2===a&&1==o){if(u="mv",p=n,d=s,h=[n.shape[0]],p.shape[1]!==d.shape[0])throw new Error(`size mismatch, got ${p.shape[0]}, ${p.shape[0]}x${p.shape[1]},${s.shape[0]}`)}else{if(!(a>=1&&o>=1&&(a>2||o>2)))throw new Error(`matmul not supported for ${a}D and ${o}D tensors`);{u="bmm";const n=(0,i.broadcastBatchedMatmul)(e,r);if(p=(0,i.reshapeBatchedMatmul)(n.a),d=(0,i.reshapeBatchedMatmul)(n.b),h=n.output.shape,p.shape[2]!==d.shape[1])throw new Error(`mat1 and mat2 shapes cannot be multiplied (${p.shape[1]}x${p.shape[2]} and ${d.shape[1]}x${d.shape[2]})`)}}let f={};return"bmm"===u?f={batchSize:Math.max(p.shape[0],d.shape[0]),aRows:p.shape[1],aCols:p.shape[2],bCols:d.shape[2],aBatchStride:p.strides[0],aRowStride:p.strides[1],aColStride:p.strides[2],bBatchStride:d.strides[0],bRowStride:d.strides[1],bColStride:d.strides[2],alpha:1}:"mm"===u?f={aRows:p.shape[0],aCols:p.shape[1],bCols:d.shape[1],aRowStride:p.strides[0],aColStride:p.strides[1],bRowStride:d.strides[0],bColStride:d.strides[1],alpha:1}:"mv"===u?f={aRows:p.shape[0],aCols:p.shape[1],aRowStride:p.strides[0],aColStride:p.strides[1],bRowStride:d.strides[0]}:"dot"===u&&(f={aRows:p.shape[0],aRowStride:p.strides[0],bRowStride:d.strides[0]}),c.runKernel(u,{resultDtype:e.dtype},f,[h],l)[0]},r.mm=function mm(e,r){if((0,s.shouldCreateGradient)(e,r))throw new Error("mm gradient not supported yet");{if(2!==e.shape.length||2!==r.shape.length)throw new Error(`Expected 2D tensors, got ${e.shape} and ${r.shape}`);if(e.shape[1]!==r.shape[0])throw new Error(`Expected tensors inner dimensions to be compatible, got ${e.shape} and ${r.shape}`);const n={aRows:e.shape[0],aCols:e.shape[1],bCols:r.shape[1],aRowStride:e.strides[0],aColStride:e.strides[1],bRowStride:r.strides[0],bColStride:r.strides[1],alpha:1};return e.runKernel("mm",{resultDtype:e.dtype},n,[[n.aRows,n.bCols]],r)[0]}},r.numel=function numel(e){return(0,i.shapeSize)(e.shape)},r.reshape=function reshape(e,r){return reshapeViewHelper(e,r,!0)},r.squeeze=squeeze,r.t=function t(e){if(2!==e.shape.length)throw new Error(`Expected 2D tensor, got ${e.shape}`);if((0,s.shouldCreateGradient)(e))throw new Error("t gradient not supported yet");{let r=e.shape.slice();r.reverse();let n=e.strides.slice();return n.reverse(),e.withShape(r,n)}},r.tensor=function tensor(e,r,n,s){return e.hasOwnProperty("data")?new a.Tensor(e):new a.Tensor(e,r,n,s)},r.unsqueeze=function unsqueeze(e,r){const n=e.shape.length,s=n>0?-n-1:-2,a=n>0?n+1:2;let i;if(void 0===r)i=0;else{if(r<s||r>=a)throw new Error(`Dimension out of range (expected to be in range of [${s}, ${a-1}], but got ${r})`);i=r<0?r+n+1:r}const o=[],u=[];let p=0;for(let r=0;r<n+1;r++)r===i?(o.push(1),0===r?u.push(e.strides[0]*e.shape[0]):u.push(u[r-1])):(o.push(e.shape[p]),u.push(e.strides[p]),p++);return e.withShape(o,u)},r.view=function view(e,r){return reshapeViewHelper(e,r,!1)}},299:(e,r,n)=>{Object.defineProperty(r,"__esModule",{value:!0}),r.reduction=r.binaryWithAlpha=r.binary=r.unaryWithAlpha=r.unary=void 0;const s=n(149);r.unary=function unary(e,r){return(0,s.shouldCreateGradient)(r)?e.apply(r):e.forward([r])},r.unaryWithAlpha=function unaryWithAlpha(e,r,n){return(0,s.shouldCreateGradient)(r)?e.apply(r,n):e.forward([r,n])},r.binary=function binary(e,r,n){if("number"==typeof n){if((0,s.shouldCreateGradient)(r))return e.apply(r,n)}else{if(r.shape.length!==n.shape.length)throw new Error(`Shape dimensions must match. Got ${r.shape} and ${n.shape}`);if((0,s.shouldCreateGradient)(r)||(0,s.shouldCreateGradient)(n))return e.apply(r,n)}return e.forward([r,n])},r.binaryWithAlpha=function binaryWithAlpha(e,r,n,a){if("number"==typeof n){if((0,s.shouldCreateGradient)(r))return e.apply(r,n,a)}else if((0,s.shouldCreateGradient)(r,n))return e.apply(r,n,a);return e.forward([r,n,a])},r.reduction=function reduction(e,r,n,a){return(0,s.shouldCreateGradient)(r)?e.apply(r,n,a):e.forward([r,n,a])}},980:function(e,r,n){var s=this&&this.__createBinding||(Object.create?function(e,r,n,s){void 0===s&&(s=n);var a=Object.getOwnPropertyDescriptor(r,n);a&&!("get"in a?!r.__esModule:a.writable||a.configurable)||(a={enumerable:!0,get:function(){return r[n]}}),Object.defineProperty(e,s,a)}:function(e,r,n,s){void 0===s&&(s=n),e[s]=r[n]}),a=this&&this.__setModuleDefault||(Object.create?function(e,r){Object.defineProperty(e,"default",{enumerable:!0,value:r})}:function(e,r){e.default=r}),i=this&&this.__importStar||function(e){if(e&&e.__esModule)return e;var r={};if(null!=e)for(var n in e)"default"!==n&&Object.prototype.hasOwnProperty.call(e,n)&&s(r,e,n);return a(r,e),r};Object.defineProperty(r,"__esModule",{value:!0}),r.sin=r.silu=r.sign=r.sigmoid=r.rsqrt=r.round=r.relu=r.reciprocal=r.rad2deg=r.pow=r.positive=r.negative=r.neg=r.multiply=r.mul=r.logaddexp2=r.logaddexp=r.log2=r.log1p=r.log10=r.log=r.ldexp=r.hypot=r.frac=r.floor=r.expm1=r.exp2=r.exp=r.divide=r.div=r.deg2rad=r.cosh=r.cos=r.copysign=r.ceil=r.arctan2=r.atan2=r.arctan=r.atan=r.arcsinh=r.asinh=r.arcsin=r.asin=r.add=r.arccosh=r.acosh=r.arccos=r.acos=r.absolute=r.abs=void 0,r.countNonzero=r.sum=r.prod=r.norm=r.mean=r.any=r.all=r.xlogy=r.fix=r.trunc=r.tanh=r.tan=r.subtract=r.sub=r.square=r.sqrt=r.sinh=r.sinc=void 0;const o=i(n(869)),u=n(299);function abs(e){return(0,u.unary)(o.AbsFunction,e)}function acos(e){return(0,u.unary)(o.AcosFunction,e)}function acosh(e){return(0,u.unary)(o.AcoshFunction,e)}function asin(e){return(0,u.unary)(o.AsinFunction,e)}function asinh(e){return(0,u.unary)(o.AsinhFunction,e)}function atan(e){return(0,u.unary)(o.AtanFunction,e)}function atan2(e,r){return(0,u.binary)(o.Atan2Function,e,r)}function div(e,r){return(0,u.binary)(o.DivFunction,e,r)}function mul(e,r){return(0,u.binary)(o.MulFunction,e,r)}function neg(e){return(0,u.unary)(o.NegFunction,e)}function sub(e,r,n){return(0,u.binaryWithAlpha)(o.SubFunction,e,r,n)}function trunc(e){return(0,u.unary)(o.TruncFunction,e)}r.abs=abs,r.absolute=function absolute(e){return abs(e)},r.acos=acos,r.arccos=function arccos(e){return acos(e)},r.acosh=acosh,r.arccosh=function arccosh(e){return acosh(e)},r.add=function add(e,r,n){return(0,u.binaryWithAlpha)(o.AddFunction,e,r,n)},r.asin=asin,r.arcsin=function arcsin(e){return asin(e)},r.asinh=asinh,r.arcsinh=function arcsinh(e){return asinh(e)},r.atan=atan,r.arctan=function arctan(e){return atan(e)},r.atan2=atan2,r.arctan2=function arctan2(e,r){return atan2(e,r)},r.ceil=function ceil(e){return(0,u.unary)(o.CeilFunction,e)},r.copysign=function copysign(e,r){return(0,u.binary)(o.CopysignFunction,e,r)},r.cos=function cos(e){return(0,u.unary)(o.CosFunction,e)},r.cosh=function cosh(e){return(0,u.unary)(o.CoshFunction,e)},r.deg2rad=function deg2rad(e){return(0,u.unary)(o.Deg2radFunction,e)},r.div=div,r.divide=function divide(e,r){return div(e,r)},r.exp=function exp(e){return(0,u.unary)(o.ExpFunction,e)},r.exp2=function exp2(e){return(0,u.unary)(o.Exp2Function,e)},r.expm1=function expm1(e){return(0,u.unary)(o.Expm1Function,e)},r.floor=function floor(e){return(0,u.unary)(o.FloorFunction,e)},r.frac=function frac(e){return(0,u.unary)(o.FracFunction,e)},r.hypot=function hypot(e,r){return(0,u.binary)(o.HypotFunction,e,r)},r.ldexp=function ldexp(e,r){return(0,u.binary)(o.LdexpFunction,e,r)},r.log=function log(e){return(0,u.unary)(o.LogFunction,e)},r.log10=function log10(e){return(0,u.unary)(o.Log10Function,e)},r.log1p=function log1p(e){return(0,u.unary)(o.Log1pFunction,e)},r.log2=function log2(e){return(0,u.unary)(o.Log2Function,e)},r.logaddexp=function logaddexp(e,r){return(0,u.binary)(o.LogaddexpFunction,e,r)},r.logaddexp2=function logaddexp2(e,r){return(0,u.binary)(o.Logaddexp2Function,e,r)},r.mul=mul,r.multiply=function multiply(e,r){return mul(e,r)},r.neg=neg,r.negative=function negative(e){return neg(e)},r.positive=function positive(e){return(0,u.unary)(o.PositiveFunction,e)},r.pow=function pow(e,r){return(0,u.binary)(o.PowFunction,e,r)},r.rad2deg=function rad2deg(e){return(0,u.unary)(o.Rad2degFunction,e)},r.reciprocal=function reciprocal(e){return(0,u.unary)(o.ReciprocalFunction,e)},r.relu=function relu(e){return(0,u.unary)(o.ReluFunction,e)},r.round=function round(e){return(0,u.unary)(o.RoundFunction,e)},r.rsqrt=function rsqrt(e){return(0,u.unary)(o.RsqrtFunction,e)},r.sigmoid=function sigmoid(e){return(0,u.unary)(o.SigmoidFunction,e)},r.sign=function sign(e){return(0,u.unary)(o.SignFunction,e)},r.silu=function silu(e){return(0,u.unary)(o.SiluFunction,e)},r.sin=function sin(e){return(0,u.unary)(o.SinFunction,e)},r.sinc=function sinc(e){return(0,u.unary)(o.SincFunction,e)},r.sinh=function sinh(e){return(0,u.unary)(o.SinhFunction,e)},r.sqrt=function sqrt(e){return(0,u.unary)(o.SqrtFunction,e)},r.square=function square(e){return(0,u.unary)(o.SquareFunction,e)},r.sub=sub,r.subtract=function subtract(e,r,n){return sub(e,r,n)},r.tan=function tan(e){return(0,u.unary)(o.TanFunction,e)},r.tanh=function tanh(e){return(0,u.unary)(o.TanhFunction,e)},r.trunc=trunc,r.fix=function fix(e){return trunc(e)},r.xlogy=function xlogy(e,r){return(0,u.binary)(o.XlogyFunction,e,r)},r.all=function all(e,r,n){return(0,u.reduction)(o.AllFunction,e,r,n)},r.any=function any(e,r,n){return(0,u.reduction)(o.AnyFunction,e,r,n)},r.mean=function mean(e,r,n){return(0,u.reduction)(o.MeanFunction,e,r,n)},r.norm=function norm(e,r,n){return(0,u.reduction)(o.NormFunction,e,r,n)},r.prod=function prod(e,r,n){return(0,u.reduction)(o.ProdFunction,e,r,n)},r.sum=function sum(e,r,n){return(0,u.reduction)(o.SumFunction,e,r,n)},r.countNonzero=function countNonzero(e,r,n){return(0,u.reduction)(o.CountNonzeroFunction,e,r,n)}},737:(e,r)=>{Object.defineProperty(r,"__esModule",{value:!0}),r.getSeed=void 0;let n=42;r.getSeed=function getSeed(){const e=n;return n=2147483647*Math.random()>>>0,e}},504:(e,r,n)=>{Object.defineProperty(r,"__esModule",{value:!0}),r.saveSafetensorsAsync=void 0;const s=n(81),a=n(320);function dtypeToSafetensorDtype(e){switch(e){case"int8":return"I8";case"uint8":return"U8";case"int32":return"I32";case"uint32":return"U32";case"float32":return"F32";case"int64":return"I64";default:throw new Error(`Invalid dtype ${e}`)}}r.saveSafetensorsAsync=async function saveSafetensorsAsync(e){const r={};let n=0;for(const i in e){const o=e[i],u=o.shape,p=n+(0,s.shapeSize)(u)*(0,a.dtypeByteSize)(o.dtype);r[i]={dtype:dtypeToSafetensorDtype(o.dtype),shape:u,data_offsets:[n,p]},n=p}const i=JSON.stringify(r),o=(new TextEncoder).encode(i),u=new ArrayBuffer(8+o.byteLength+n);new DataView(u).setBigUint64(0,BigInt(o.byteLength),!0),new Uint8Array(u,8,o.byteLength).set(o),n=8+o.byteLength;for(const r in e){const i=e[r],o=(await i.storage.toTypedArrayAsync(i.dtype)).buffer,p=(0,s.shapeSize)(i.shape)*(0,a.dtypeByteSize)(i.dtype);new Uint8Array(u,n,p).set(new Uint8Array(o)),n+=p}return u}},81:(e,r)=>{function check(e,r){if(!e)throw new Error(r())}function defaultStrides(e){const r=[];let n=1;for(let s=e.length-1;s>=0;s--)r[s]=n,n*=Math.max(1,e[s]);return r}function contiguousStridedShape(e){return{shape:e,strides:defaultStrides(e)}}Object.defineProperty(r,"__esModule",{value:!0}),r.canonicalizeDim=r.validateDimLength=r.validateIdx=r.broadcastBatchedMatmul=r.reshapeBatchedMatmul=r.broadcastShapes=r.stridedShapeIsContiguous=r.contiguousStridedShape=r.defaultStrides=r.shapesAreEqual=r.shapeSize=r.getShape=r.check=void 0,r.check=check,r.getShape=function getShape(e){return"number"==typeof e?[e]:e},r.shapeSize=function shapeSize(e){return 0===e.length?1:1===e.length?e[0]:e.reduce(((e,r)=>e*r))},r.shapesAreEqual=function shapesAreEqual(e,r){if(e.length!==r.length)return!1;for(let n=0;n<e.length;n++)if(e[n]!==r[n])return!1;return!0},r.defaultStrides=defaultStrides,r.contiguousStridedShape=contiguousStridedShape,r.stridedShapeIsContiguous=function stridedShapeIsContiguous(e){const{shape:r,strides:n}=e;let s=1;for(let e=r.length-1;e>=0;e--){if(n[e]!==s)return!1;s*=r[e]}return!0},r.broadcastShapes=function broadcastShapes(e,r){const n=e.shape.slice(),s=r.shape.slice(),a=e.strides.slice(),i=r.strides.slice(),padFront=(e,r,n)=>{for(;e.length<n;)e.unshift(1),r.unshift(0)},o=Math.max(n.length,s.length);padFront(n,a,o),padFront(s,i,o);const u=[];for(let e=0;e<o;e++)if(1===n[e]||1===s[e])u[e]=Math.max(n[e],s[e]),1===n[e]&&(a[e]=0),1===s[e]&&(i[e]=0);else{if(n[e]!==s[e])throw new Error("The size of tensor a ("+n[e]+") must match the size of tensor b ("+s[e]+") at non-singleton dimension "+e);u[e]=n[e]}return{output:contiguousStridedShape(u),a:{shape:n,strides:a},b:{shape:s,strides:i}}},r.reshapeBatchedMatmul=function reshapeBatchedMatmul(e){const r=e.shape,n=e.strides;if(r.length<3)throw new Error("Input tensor must be at least 3D");let s=1;for(let e=0;e<r.length-2;e++)s*=r[e];return{shape:[s].concat(r.slice(r.length-2)),strides:[n[n.length-3],n[n.length-2],n[n.length-1]]}},r.broadcastBatchedMatmul=function broadcastBatchedMatmul(e,r){const n=e.shape.slice(),s=e.strides.slice(),a=r.shape.slice(),i=r.strides.slice();if(n.length<2&&a.length<2)throw new Error(`Expected at least 2D tensors for matmul broadcast, got ${n} and ${a}`);const padFront=(e,r,n)=>{for(;e.length<n;)e.unshift(1),r.unshift(0)};let o=null;1===n.length?(n.unshift(1),s.unshift(0),o=-2):1===a.length&&(a.push(1),i.push(0),o=-1);const u=Math.max(n.length,a.length);padFront(n,s,u),padFront(a,i,u);const p=[];for(let e=0;e<n.length-2;e++)if(1===n[e]||1===a[e])p[e]=Math.max(n[e],a[e]),1===n[e]&&(s[e]=0),1===a[e]&&(i[e]=0);else{if(n[e]!==a[e])throw new Error("The size of tensor a ("+n[e]+") must match the size of tensor b ("+a[e]+") at non-singleton dimension "+e);p[e]=n[e]}return p.push(n[n.length-2]),p.push(a[a.length-1]),null!==o&&p.splice(o,1),{output:contiguousStridedShape(p),a:{shape:n,strides:s},b:{shape:a,strides:i}}},r.validateIdx=function validateIdx(e,r){check(r>=0&&r<e||0===r,(()=>`index ${r} is out of bounds for dimension ${e}`))},r.validateDimLength=function validateDimLength(e){check(e>=0,(()=>`dimension length ${e} must be non-negative`))},r.canonicalizeDim=function canonicalizeDim(e,r,n=!0){if(e<0)throw new Error(`Rank must be non-negative, got ${e}`);if(0===e){if(!n)throw new Error(`Dimension specified as ${r} but tensor has no dimensions`);e=1}if(r>0&&r<e)return r;let s;if(s=r<0?r+e:r,s<0||s>=e)throw new Error(`Dimension out of range (expected to be in range of [${-e}, ${e}), but got ${r})`);return s}},421:(e,r,n)=>{Object.defineProperty(r,"__esModule",{value:!0}),r.BufferHeap=r.HeapBuffer=r.flatDataToArray=r.newTypedArrayFromArray=r.newStorageFromATypedArray=r.GPUBufferStorage=r.ArrayBufferStorage=r.UntypedStorage=void 0;const s=n(459),a=n(320),i=n(81);class UntypedStorage{async toArrayAsync(e){const r=await this.toTypedArrayAsync(e);return Array.from(r)}}r.UntypedStorage=UntypedStorage;class ArrayBufferStorage extends UntypedStorage{_buffer;_byteOffset=0;_byteSize;_heapBuffer=null;get device(){return s.cpuDevice}get cpuBuffer(){return this._buffer}get byteOffset(){return this._byteOffset}get byteSize(){return this._byteSize}constructor(e){if(super(),"number"==typeof e)this._buffer=new ArrayBuffer(e),this._byteSize=e;else if(e instanceof HeapBuffer)this._buffer=e.heap.buffer,this._byteOffset=e.offset,this._byteSize=e.byteSize,this._heapBuffer=e;else if(e instanceof ArrayBuffer)this._buffer=e,this._byteSize=e.byteLength;else if(e instanceof Uint8Array)this._buffer=e.buffer,this._byteSize=e.byteLength;else if(e instanceof Int32Array)this._buffer=e.buffer,this._byteSize=e.byteLength;else{if(!(e instanceof Float32Array))throw new Error(`Invalid constructor argument for ArrayBufferStorage. Expected number of bytes, ArrayBuffer, or a TypedArray. Got ${e} (${e.constructor.name})`);this._buffer=e.buffer,this._byteSize=e.byteLength}}getTypedArray(e){return(0,a.dtypedBufferToTypedArray)(e,this._buffer,this._byteOffset,this._byteSize)}async toTypedArrayAsync(e){return this.getTypedArray(e)}destroy(){}clone(){return new ArrayBufferStorage(this._buffer.slice(0))}}r.ArrayBufferStorage=ArrayBufferStorage;class GPUBufferStorage extends UntypedStorage{_device;_byteOffset=0;_byteSize;_buffer;_gpuDevice;get device(){return this._device}get byteSize(){return this._byteSize}get byteOffset(){return this._byteOffset}get gpuBuffer(){return this._buffer}get gpuDevice(){return this._gpuDevice}constructor(e,r,n){if(super(),this._device=r,this._gpuDevice=r.gpuDevice,e instanceof GPUBuffer)this._buffer=e,this._byteSize=this._buffer.size;else if(e instanceof HeapBuffer)this._buffer=e.heap.buffer,this._byteOffset=e.offset,this._byteSize=e.byteSize;else{if("number"!=typeof e||void 0===n||void 0===r)throw new Error(`Invalid constructor arguments for GPUBufferStorage. Expected GPUBuffer, or byteSize, usage, and device. Got ${e} (${e.constructor.name})`);{const r=4*Math.floor((e+3)/4);this._buffer=this._gpuDevice.createBuffer({mappedAtCreation:!0,size:r,usage:n}),this._byteSize=this._buffer.size}}}destroy(){this._buffer.destroy()}async toTypedArrayAsync(e){const r=this.copyBufferToReadableBuffer();if(await r.mapAsync(GPUMapMode.READ),"mapped"!==r.mapState)throw new Error("GPUBuffer failed to map");const n=r.getMappedRange();return(0,a.dtypedBufferToTypedArray)(e,n)}copyBufferToReadableBuffer(){const e=this._byteSize,r=this._gpuDevice.createBuffer({mappedAtCreation:!1,size:e,usage:GPUBufferUsage.COPY_DST|GPUBufferUsage.MAP_READ}),n=this._gpuDevice.createCommandEncoder();n.copyBufferToBuffer(this._buffer,this._byteOffset,r,0,e);const s=n.finish();return this._gpuDevice.queue.submit([s]),r}clone(){const e=GPUBufferUsage.COPY_DST|GPUBufferUsage.COPY_SRC|GPUBufferUsage.STORAGE,r=this._gpuDevice.createBuffer({mappedAtCreation:!1,size:this._buffer.size,usage:e}),n=this._gpuDevice.createCommandEncoder();n.copyBufferToBuffer(this._buffer,0,r,0,this._buffer.size);const s=n.finish();return this._gpuDevice.queue.submit([s]),new GPUBufferStorage(r,this._device)}}function getNearestPowerOfTwo(e){return Math.pow(2,Math.ceil(Math.log2(e)))}r.GPUBufferStorage=GPUBufferStorage,r.newStorageFromATypedArray=function newStorageFromATypedArray(e,r,n,s){const a=(0,i.defaultStrides)(r);return{storage:s.initStorage(r,n,(r=>{if(r.BYTES_PER_ELEMENT>1&&e instanceof Uint8Array){const n=new Uint8Array(r.buffer);if(n.length!==e.length)throw new Error(`TypedArray from Uint8Array length mismatch: ${n.length} !== ${e.length}`);n.set(e)}else{if(r.length!==e.length)throw new Error(`TypedArray length mismatch: ${r.length} !== ${e.length}`);r.set(e)}})),shape:r,strides:a}},r.newTypedArrayFromArray=function newTypedArrayFromArray(e,r,n){const s=[];null!==e&&function getShape(e){void 0!==e&&"number"!=typeof e&&(s.push(e.length),getShape(e[0]))}(e);const a=(0,i.defaultStrides)(s);return{storage:n.initStorage(s,r,(r=>{let n=0;null!==e&&function flatten(e){for(let s=0;s<e.length;s++){let a=e[s];if("number"==typeof a){for(let s=0;s<e.length;s++)r[n]=e[s],n++;return}if(!(a instanceof Array))throw new Error(`Invalid data type: ${a} (${a.constructor.name})`);flatten(a)}}(e)})),shape:s,strides:a}},r.flatDataToArray=function flatDataToArray(e,r,n){const s=r.length;return 0==s?e[0]:1==s&&1==r[0]?[e[0]]:1==s&&1==n[0]?Array.from(e):function readArray(s){const a=s.length;if(a==r.length-1){const i=function calculateOffset(e){let r=0;for(let s=0;s<e.length;s++)r+=e[s]*n[s];return r}(s),o=r[a],u=e.subarray(i,i+o);if(u.length!==o)throw new Error(`Failed to get sub array for index [${s}] (tensor shape [${r}] and strides [${n}]) at offset ${i} with length ${o} from buffer of length ${e.length}`);return Array.from(u)}{const e=[];for(let n=0;n<r[a];n++)s.push(n),e.push(readArray(s)),s.pop();return e}}([])};class HeapBuffer{heap;offset;order;get byteSize(){return 1<<this.order}constructor(e,r,n){this.heap=e,this.offset=r,this.order=n}free(){this.heap.free(this)}}r.HeapBuffer=HeapBuffer;class BufferHeap{_heapSize;_heapBuffer;_minOrder;orderFreeLists;get buffer(){return this._heapBuffer}get size(){return this._heapSize}constructor(e,r,n){if(this._heapSize=getNearestPowerOfTwo(r),this._heapSize>r)throw new Error(`Requested heap size ${r} is not a power of two`);this._heapBuffer=e,this._minOrder=n,this.orderFreeLists=[];const s=Math.log2(this._heapSize);for(let e=0;e<=s;++e)this.orderFreeLists.push([]);this.orderFreeLists[s].push(0)}static getBuddyOffset(e,r){return e^1<<r}alloc(e){const r=Math.max(this._minOrder,Math.log2(getNearestPowerOfTwo(e)));let n=r;for(;n<this.orderFreeLists.length&&0===this.orderFreeLists[n].length;)n++;if(n===this.orderFreeLists.length)return null;const s=this.orderFreeLists[n].pop(),a=new HeapBuffer(this,s,r);for(;n>r;){n--;const e=BufferHeap.getBuddyOffset(s,n);this.orderFreeLists[n].push(e)}return a}free(e){let r=e.offset,n=e.order;for(;n<this.orderFreeLists.length-1;){const e=BufferHeap.getBuddyOffset(r,n),s=this.orderFreeLists[n].indexOf(e);if(-1===s)break;this.orderFreeLists[n].splice(s,1),n++,r=Math.min(r,e)}this.orderFreeLists[n].push(r)}}r.BufferHeap=BufferHeap},967:function(e,r,n){var s=this&&this.__createBinding||(Object.create?function(e,r,n,s){void 0===s&&(s=n);var a=Object.getOwnPropertyDescriptor(r,n);a&&!("get"in a?!r.__esModule:a.writable||a.configurable)||(a={enumerable:!0,get:function(){return r[n]}}),Object.defineProperty(e,s,a)}:function(e,r,n,s){void 0===s&&(s=n),e[s]=r[n]}),a=this&&this.__setModuleDefault||(Object.create?function(e,r){Object.defineProperty(e,"default",{enumerable:!0,value:r})}:function(e,r){e.default=r}),i=this&&this.__importStar||function(e){if(e&&e.__esModule)return e;var r={};if(null!=e)for(var n in e)"default"!==n&&Object.prototype.hasOwnProperty.call(e,n)&&s(r,e,n);return a(r,e),r};Object.defineProperty(r,"__esModule",{value:!0}),r.Tensor=void 0;const o=n(459),u=n(81),p=n(882),d=n(320),h=n(421),c=n(149),l=n(679),f=i(n(980)),g=i(n(500)),y=n(647),S=n(700),m=n(737);class Tensor extends y.TensorBase{_device;_dtype;_shape;_strides;_requiresGrad=!1;_gradFunc;_gradCtx;grad=null;_node;get node(){return this._node}get storage(){return this._node.node.storages[this._node.outputIndex]}get dtype(){return this._dtype}get shape(){return this._shape}get strides(){return this._strides}get ndim(){return this._shape.length}get device(){return this._device}get isContiguous(){let e=this.strides,r=this.shape,n=1;for(let s=r.length-1;s>=0;s--){if(e[s]!==n)return!1;n*=r[s]}return!0}get isScalar(){return 0===this.shape.length||1===this.shape.length&&1===this.shape[0]}get requiresGrad(){return this._requiresGrad}set requiresGrad(e){if(this._gradFunc)throw new Error("You can only change requiresGrad flags of leaf variables. If you want to use a computed variable in a subgraph that doesn't require differentiation use valueNoGrad = value.detach().");this._requiresGrad=e}get gradFunc(){return this._gradFunc}constructor(e,r,n,s){super();let a=(0,o.getDevice)(n),i=(0,d.getDtype)(r);if(null===e)throw new Error("Cannot create tensor from null");if("number"==typeof e){const r=(0,h.newTypedArrayFromArray)([e],i,a);this._dtype=i,this._shape=[],this._strides=[],this._node=new S.SourceNode(r.storage,this._dtype,this._shape,this._strides).getOutputRef(0)}else if(e instanceof Array){const r=(0,h.newTypedArrayFromArray)(e,i,a);this._dtype=i,this._shape=r.shape,this._strides=r.strides,this._node=new S.SourceNode(r.storage,this._dtype,this._shape,this._strides).getOutputRef(0)}else if(e instanceof Uint8Array){const r=[e.length],n=(0,h.newStorageFromATypedArray)(e,r,i,a);this._dtype=i,this._shape=n.shape,this._strides=n.strides,this._node=new S.SourceNode(n.storage,this._dtype,this._shape,this._strides).getOutputRef(0)}else if(e.hasOwnProperty("node")&&e.node instanceof S.GraphNode){const r=e,n=r.node.outputs[r.outputIndex];this._dtype=n.dtype,this._shape=n.shape,this._strides=n.strides,this._node=r}else{if(!e.hasOwnProperty("data"))throw new Error("Invalid data type for Tensor constructor. Expected an array of values or a json object with a 'data' property.");{const r=e;let n;if(a=r.device?(0,o.getDevice)(r.device):a,i=r.dtype?(0,d.getDtype)(r.dtype):i,s=s||r.requiresGrad,"number"==typeof r.data){n=(0,h.newTypedArrayFromArray)([r.data],i,a).storage,this._dtype=i,this._shape=r.shape||[],this._strides=(0,u.defaultStrides)(this._shape)}else if(r.data instanceof Array){const e=(0,h.newTypedArrayFromArray)(r.data,i,a);n=e.storage,this._dtype=i,this._shape=r.shape||e.shape,this._strides=r.strides||e.strides}else if(r.data instanceof Uint8Array){const e=r.shape||[r.data.length],s=(0,h.newStorageFromATypedArray)(r.data,e,i,a);n=s.storage,this._dtype=i,this._shape=s.shape,this._strides=s.strides}else{if(!(r.data instanceof h.UntypedStorage))throw new Error("Cannot create tensor from data with type "+r.data.constructor.name);if(n=r.data,this._dtype=i,void 0===r.shape&&void 0===r.strides)throw new Error("Cannot create tensor from storage without also specifying the shape and strides.");this._shape=r.shape||[],this._strides=r.strides||(0,u.defaultStrides)(this._shape)}this._node=new S.SourceNode(n,this._dtype,this._shape,this._strides).getOutputRef(0)}}this._device=a,this._requiresGrad=s||!1,this._gradFunc=null,this._gradCtx=null,this.grad=null}get[Symbol.toStringTag](){return"Tensor"}toString(){let e=this.requiresGrad?", requiresGrad=true":"";return this._gradFunc&&(e=", gradFunc"),`tensor([${this.shape}], ${this.dtype}${e})`}async toArrayAsync(){const e=await this.storage.toTypedArrayAsync(this.dtype);return(0,h.flatDataToArray)(e,this.shape,this.strides)}eager(){return this.node.node.eager(),this}withShape(e,r){if((0,u.shapeSize)(e)!=(0,u.shapeSize)(this.shape))throw new Error(`Cannot reshape tensor of size ${this.shape} to ${e}`);{const n=new S.ViewNode(this.node,e,r),s=new Tensor(n.getOutputRef(0));return s.requiresGrad=this.requiresGrad,s}}runKernelInplace(e,r,n,...s){if(this.requiresGrad&&(0,c.isGradEnabled)())throw new Error("A tensor that requires a gradient cannot be used in an in-place operation");if(this._node.node.refCount>0){const a=e.endsWith("_")?e.slice(0,-1):e,i=this.device.getKernel(a,r),o=[this._node.addRef(),...s.map((e=>e._node.addRef()))],u=[{shape:this.shape,dtype:this.dtype,strides:this.strides}],p=new S.KernelNode(i,o,n,u);this._node=p.getOutputRef(0)}else{const a=this.device.getKernel(e,r),i=s.map((e=>e.storage));a.run(i,n,[this.storage])}return this}runKernel(e,r,n,s,...a){if(0===s.length)throw new Error(`Cannot run kernel "${e}" without any outputs`);const i=this.device.getKernel(e,r);s[0];{const e=[this._node.addRef(),...a.map((e=>e._node.addRef()))],r=[];for(let e=0;e<s.length;e++){const n=s[e];n instanceof Tensor?r.push({shape:n.shape,dtype:n.dtype,strides:n.strides}):r.push({shape:n,dtype:(0,l.shaderTypeToDtype)(i.spec.outputs[e].shaderType),strides:(0,u.defaultStrides)(n)})}const o=new S.KernelNode(i,e,n,r);return s.map(((e,r)=>new Tensor(o.getOutputRef(r))))}}detach(){return this._requiresGrad||this._gradFunc?new Tensor({data:this.storage,dtype:this.dtype,requiresGrad:!1,shape:this.shape,strides:this.strides,device:this.device}):this}setGradientFunction(e,r){this._gradFunc=r,this._gradCtx=e,this._requiresGrad=!0}backward(e){if(!this.requiresGrad)throw new Error("Cannot call backward on a tensor that does not require gradients");let r;if(e)r=e;else{if(!this.isScalar)throw new Error("Gradient can only be implicitly created for scalar outputs");r=(0,p.ones)(1,this.dtype,this.device)}if(this.grad?this.grad.add_(r).eager():this.grad=r.eager(),!this._gradFunc||!this._gradCtx)return;const n=this._gradFunc(this._gradCtx,r),s=this._gradCtx.inputsWithGradient;for(let e=0;e<s.length;e++){const r=s[e];if(null===r)continue;const a=n[e];if(!a)throw new Error(`Gradient function did not return a gradient for input #${e} (out of ${s.length}). ${n.length} gradients were returned.`);r.backward(a)}}expand(e){const r=e.slice(),n=Array(r.length).fill(0);let s=r.length-1,a=this.shape,i=this.strides;for(let e=a.length-1;e>=0;e--)1===a[e]?n[s]=0:(n[s]=i[e],s--),-1===r[s]&&(r[s]=a[e]);return this.withShape(r,n)}flatten(e=0,r=-1){return g.flatten(this,e,r)}gather(e,r){return g.gather(this,e,r)}mm(e){return g.mm(this,e)}numel(){return g.numel(this)}reshape(e){return g.reshape(this,e)}reshapeAs(e){return g.reshape(this,e.shape)}squeeze(e){return g.squeeze(this,e)}t(){return g.t(this)}uniform_(e,r){const n={size:(0,u.shapeSize)(this.shape),seed:(0,m.getSeed)(),lowerBound:e,upperBound:r};return this.runKernelInplace("uniform_",{dtype:this.dtype},n)}unsqueeze(e){return g.unsqueeze(this,e)}view(e){return g.view(this,e)}viewAs(e){return g.view(this,e.shape)}zero_(){throw new Error("Tensor zero_ is not implemented")}abs(){return f.abs(this)}absolute(){return f.abs(this)}abs_(){const e={size:(0,u.shapeSize)(this.shape)};return this.runKernelInplace("abs_",{dtype:this.dtype},e)}acos(){return f.acos(this)}arccos(){return f.acos(this)}acos_(){const e={size:(0,u.shapeSize)(this.shape)};return this.runKernelInplace("acos_",{dtype:this.dtype},e)}acosh(){return f.acosh(this)}arccosh(){return f.acosh(this)}acosh_(){const e={size:(0,u.shapeSize)(this.shape)};return this.runKernelInplace("acosh_",{dtype:this.dtype},e)}add(e,r){return f.add(this,e,r)}add_(e,r){if("number"==typeof e){const n={size:(0,u.shapeSize)(this.shape),other:e,alpha:r||1};return this.runKernelInplace("add_scalar_",{dtype:this.dtype},n)}{const n=(0,u.broadcastShapes)(this,e);if((0,u.stridedShapeIsContiguous)(n.a)&&(0,u.stridedShapeIsContiguous)(n.b)){const n={size:(0,u.shapeSize)(this.shape),alpha:r||1};return this.runKernelInplace("add_",{dtype:this.dtype},n,e)}{const s=n.a.shape.length,a=n.b.shape.length;if(s>4||a>4)throw new Error("Broadcasting not supported for tensors with more than 4 dimensions");const i={inputStrides0:s>0?n.a.strides[0]:0,otherStrides0:a>0?n.b.strides[0]:0,outputStrides0:n.output.shape.length>0?n.output.strides[0]:1,inputStrides1:s>1?n.a.strides[1]:0,otherStrides1:a>1?n.b.strides[1]:0,outputStrides1:n.output.shape.length>1?n.output.strides[1]:1,inputStrides2:s>2?n.a.strides[2]:0,otherStrides2:a>2?n.b.strides[2]:0,outputStrides2:n.output.shape.length>2?n.output.strides[2]:1,inputStrides3:s>3?n.a.strides[3]:0,otherStrides3:a>3?n.b.strides[3]:0,outputStrides3:n.output.shape.length>3?n.output.strides[3]:1,size:(0,u.shapeSize)(n.output.shape),alpha:r||1};return this.runKernelInplace("add_strided_",{dtype:this.dtype},i,e)}}}asin(){return f.asin(this)}arcsin(){return f.asin(this)}asin_(){const e={size:(0,u.shapeSize)(this.shape)};return this.runKernelInplace("asin_",{dtype:this.dtype},e)}asinh(){return f.asinh(this)}arcsinh(){return f.asinh(this)}asinh_(){const e={size:(0,u.shapeSize)(this.shape)};return this.runKernelInplace("asinh_",{dtype:this.dtype},e)}atan(){return f.atan(this)}arctan(){return f.atan(this)}atan_(){const e={size:(0,u.shapeSize)(this.shape)};return this.runKernelInplace("atan_",{dtype:this.dtype},e)}atan2(e){return f.atan2(this,e)}arctan2(e){return f.atan2(this,e)}atan2_(e){if("number"==typeof e){const r={size:(0,u.shapeSize)(this.shape),other:e};return this.runKernelInplace("atan2_scalar_",{dtype:this.dtype},r)}{const r=(0,u.broadcastShapes)(this,e);if((0,u.stridedShapeIsContiguous)(r.a)&&(0,u.stridedShapeIsContiguous)(r.b)){const r={size:(0,u.shapeSize)(this.shape)};return this.runKernelInplace("atan2_",{dtype:this.dtype},r,e)}{const n=r.a.shape.length,s=r.b.shape.length;if(n>4||s>4)throw new Error("Broadcasting not supported for tensors with more than 4 dimensions");const a={inputStrides0:n>0?r.a.strides[0]:0,otherStrides0:s>0?r.b.strides[0]:0,outputStrides0:r.output.shape.length>0?r.output.strides[0]:1,inputStrides1:n>1?r.a.strides[1]:0,otherStrides1:s>1?r.b.strides[1]:0,outputStrides1:r.output.shape.length>1?r.output.strides[1]:1,inputStrides2:n>2?r.a.strides[2]:0,otherStrides2:s>2?r.b.strides[2]:0,outputStrides2:r.output.shape.length>2?r.output.strides[2]:1,inputStrides3:n>3?r.a.strides[3]:0,otherStrides3:s>3?r.b.strides[3]:0,outputStrides3:r.output.shape.length>3?r.output.strides[3]:1,size:(0,u.shapeSize)(r.output.shape)};return this.runKernelInplace("atan2_strided_",{dtype:this.dtype},a,e)}}}ceil(){return f.ceil(this)}ceil_(){const e={size:(0,u.shapeSize)(this.shape)};return this.runKernelInplace("ceil_",{dtype:this.dtype},e)}copysign(e){return f.copysign(this,e)}copysign_(e){if("number"==typeof e){const r={size:(0,u.shapeSize)(this.shape),other:e};return this.runKernelInplace("copysign_scalar_",{dtype:this.dtype},r)}{const r=(0,u.broadcastShapes)(this,e);if((0,u.stridedShapeIsContiguous)(r.a)&&(0,u.stridedShapeIsContiguous)(r.b)){const r={size:(0,u.shapeSize)(this.shape)};return this.runKernelInplace("copysign_",{dtype:this.dtype},r,e)}{const n=r.a.shape.length,s=r.b.shape.length;if(n>4||s>4)throw new Error("Broadcasting not supported for tensors with more than 4 dimensions");const a={inputStrides0:n>0?r.a.strides[0]:0,otherStrides0:s>0?r.b.strides[0]:0,outputStrides0:r.output.shape.length>0?r.output.strides[0]:1,inputStrides1:n>1?r.a.strides[1]:0,otherStrides1:s>1?r.b.strides[1]:0,outputStrides1:r.output.shape.length>1?r.output.strides[1]:1,inputStrides2:n>2?r.a.strides[2]:0,otherStrides2:s>2?r.b.strides[2]:0,outputStrides2:r.output.shape.length>2?r.output.strides[2]:1,inputStrides3:n>3?r.a.strides[3]:0,otherStrides3:s>3?r.b.strides[3]:0,outputStrides3:r.output.shape.length>3?r.output.strides[3]:1,size:(0,u.shapeSize)(r.output.shape)};return this.runKernelInplace("copysign_strided_",{dtype:this.dtype},a,e)}}}cos(){return f.cos(this)}cos_(){const e={size:(0,u.shapeSize)(this.shape)};return this.runKernelInplace("cos_",{dtype:this.dtype},e)}cosh(){return f.cosh(this)}cosh_(){const e={size:(0,u.shapeSize)(this.shape)};return this.runKernelInplace("cosh_",{dtype:this.dtype},e)}deg2rad(){return f.deg2rad(this)}deg2rad_(){const e={size:(0,u.shapeSize)(this.shape)};return this.runKernelInplace("deg2rad_",{dtype:this.dtype},e)}div(e){return f.div(this,e)}divide(e){return f.div(this,e)}div_(e){if("number"==typeof e){const r={size:(0,u.shapeSize)(this.shape),other:e};return this.runKernelInplace("div_scalar_",{dtype:this.dtype},r)}{const r=(0,u.broadcastShapes)(this,e);if((0,u.stridedShapeIsContiguous)(r.a)&&(0,u.stridedShapeIsContiguous)(r.b)){const r={size:(0,u.shapeSize)(this.shape)};return this.runKernelInplace("div_",{dtype:this.dtype},r,e)}{const n=r.a.shape.length,s=r.b.shape.length;if(n>4||s>4)throw new Error("Broadcasting not supported for tensors with more than 4 dimensions");const a={inputStrides0:n>0?r.a.strides[0]:0,otherStrides0:s>0?r.b.strides[0]:0,outputStrides0:r.output.shape.length>0?r.output.strides[0]:1,inputStrides1:n>1?r.a.strides[1]:0,otherStrides1:s>1?r.b.strides[1]:0,outputStrides1:r.output.shape.length>1?r.output.strides[1]:1,inputStrides2:n>2?r.a.strides[2]:0,otherStrides2:s>2?r.b.strides[2]:0,outputStrides2:r.output.shape.length>2?r.output.strides[2]:1,inputStrides3:n>3?r.a.strides[3]:0,otherStrides3:s>3?r.b.strides[3]:0,outputStrides3:r.output.shape.length>3?r.output.strides[3]:1,size:(0,u.shapeSize)(r.output.shape)};return this.runKernelInplace("div_strided_",{dtype:this.dtype},a,e)}}}exp(){return f.exp(this)}exp_(){const e={size:(0,u.shapeSize)(this.shape)};return this.runKernelInplace("exp_",{dtype:this.dtype},e)}exp2(){return f.exp2(this)}exp2_(){const e={size:(0,u.shapeSize)(this.shape)};return this.runKernelInplace("exp2_",{dtype:this.dtype},e)}expm1(){return f.expm1(this)}expm1_(){const e={size:(0,u.shapeSize)(this.shape)};return this.runKernelInplace("expm1_",{dtype:this.dtype},e)}floor(){return f.floor(this)}floor_(){const e={size:(0,u.shapeSize)(this.shape)};return this.runKernelInplace("floor_",{dtype:this.dtype},e)}frac(){return f.frac(this)}frac_(){const e={size:(0,u.shapeSize)(this.shape)};return this.runKernelInplace("frac_",{dtype:this.dtype},e)}hypot(e){return f.hypot(this,e)}hypot_(e){if("number"==typeof e){const r={size:(0,u.shapeSize)(this.shape),other:e};return this.runKernelInplace("hypot_scalar_",{dtype:this.dtype},r)}{const r=(0,u.broadcastShapes)(this,e);if((0,u.stridedShapeIsContiguous)(r.a)&&(0,u.stridedShapeIsContiguous)(r.b)){const r={size:(0,u.shapeSize)(this.shape)};return this.runKernelInplace("hypot_",{dtype:this.dtype},r,e)}{const n=r.a.shape.length,s=r.b.shape.length;if(n>4||s>4)throw new Error("Broadcasting not supported for tensors with more than 4 dimensions");const a={inputStrides0:n>0?r.a.strides[0]:0,otherStrides0:s>0?r.b.strides[0]:0,outputStrides0:r.output.shape.length>0?r.output.strides[0]:1,inputStrides1:n>1?r.a.strides[1]:0,otherStrides1:s>1?r.b.strides[1]:0,outputStrides1:r.output.shape.length>1?r.output.strides[1]:1,inputStrides2:n>2?r.a.strides[2]:0,otherStrides2:s>2?r.b.strides[2]:0,outputStrides2:r.output.shape.length>2?r.output.strides[2]:1,inputStrides3:n>3?r.a.strides[3]:0,otherStrides3:s>3?r.b.strides[3]:0,outputStrides3:r.output.shape.length>3?r.output.strides[3]:1,size:(0,u.shapeSize)(r.output.shape)};return this.runKernelInplace("hypot_strided_",{dtype:this.dtype},a,e)}}}ldexp(e){return f.ldexp(this,e)}ldexp_(e){if("number"==typeof e){const r={size:(0,u.shapeSize)(this.shape),other:e};return this.runKernelInplace("ldexp_scalar_",{dtype:this.dtype},r)}{const r=(0,u.broadcastShapes)(this,e);if((0,u.stridedShapeIsContiguous)(r.a)&&(0,u.stridedShapeIsContiguous)(r.b)){const r={size:(0,u.shapeSize)(this.shape)};return this.runKernelInplace("ldexp_",{dtype:this.dtype},r,e)}{const n=r.a.shape.length,s=r.b.shape.length;if(n>4||s>4)throw new Error("Broadcasting not supported for tensors with more than 4 dimensions");const a={inputStrides0:n>0?r.a.strides[0]:0,otherStrides0:s>0?r.b.strides[0]:0,outputStrides0:r.output.shape.length>0?r.output.strides[0]:1,inputStrides1:n>1?r.a.strides[1]:0,otherStrides1:s>1?r.b.strides[1]:0,outputStrides1:r.output.shape.length>1?r.output.strides[1]:1,inputStrides2:n>2?r.a.strides[2]:0,otherStrides2:s>2?r.b.strides[2]:0,outputStrides2:r.output.shape.length>2?r.output.strides[2]:1,inputStrides3:n>3?r.a.strides[3]:0,otherStrides3:s>3?r.b.strides[3]:0,outputStrides3:r.output.shape.length>3?r.output.strides[3]:1,size:(0,u.shapeSize)(r.output.shape)};return this.runKernelInplace("ldexp_strided_",{dtype:this.dtype},a,e)}}}log(){return f.log(this)}log_(){const e={size:(0,u.shapeSize)(this.shape)};return this.runKernelInplace("log_",{dtype:this.dtype},e)}log10(){return f.log10(this)}log10_(){const e={size:(0,u.shapeSize)(this.shape)};return this.runKernelInplace("log10_",{dtype:this.dtype},e)}log1p(){return f.log1p(this)}log1p_(){const e={size:(0,u.shapeSize)(this.shape)};return this.runKernelInplace("log1p_",{dtype:this.dtype},e)}log2(){return f.log2(this)}log2_(){const e={size:(0,u.shapeSize)(this.shape)};return this.runKernelInplace("log2_",{dtype:this.dtype},e)}logaddexp(e){return f.logaddexp(this,e)}logaddexp_(e){if("number"==typeof e){const r={size:(0,u.shapeSize)(this.shape),other:e};return this.runKernelInplace("logaddexp_scalar_",{dtype:this.dtype},r)}{const r=(0,u.broadcastShapes)(this,e);if((0,u.stridedShapeIsContiguous)(r.a)&&(0,u.stridedShapeIsContiguous)(r.b)){const r={size:(0,u.shapeSize)(this.shape)};return this.runKernelInplace("logaddexp_",{dtype:this.dtype},r,e)}{const n=r.a.shape.length,s=r.b.shape.length;if(n>4||s>4)throw new Error("Broadcasting not supported for tensors with more than 4 dimensions");const a={inputStrides0:n>0?r.a.strides[0]:0,otherStrides0:s>0?r.b.strides[0]:0,outputStrides0:r.output.shape.length>0?r.output.strides[0]:1,inputStrides1:n>1?r.a.strides[1]:0,otherStrides1:s>1?r.b.strides[1]:0,outputStrides1:r.output.shape.length>1?r.output.strides[1]:1,inputStrides2:n>2?r.a.strides[2]:0,otherStrides2:s>2?r.b.strides[2]:0,outputStrides2:r.output.shape.length>2?r.output.strides[2]:1,inputStrides3:n>3?r.a.strides[3]:0,otherStrides3:s>3?r.b.strides[3]:0,outputStrides3:r.output.shape.length>3?r.output.strides[3]:1,size:(0,u.shapeSize)(r.output.shape)};return this.runKernelInplace("logaddexp_strided_",{dtype:this.dtype},a,e)}}}logaddexp2(e){return f.logaddexp2(this,e)}logaddexp2_(e){if("number"==typeof e){const r={size:(0,u.shapeSize)(this.shape),other:e};return this.runKernelInplace("logaddexp2_scalar_",{dtype:this.dtype},r)}{const r=(0,u.broadcastShapes)(this,e);if((0,u.stridedShapeIsContiguous)(r.a)&&(0,u.stridedShapeIsContiguous)(r.b)){const r={size:(0,u.shapeSize)(this.shape)};return this.runKernelInplace("logaddexp2_",{dtype:this.dtype},r,e)}{const n=r.a.shape.length,s=r.b.shape.length;if(n>4||s>4)throw new Error("Broadcasting not supported for tensors with more than 4 dimensions");const a={inputStrides0:n>0?r.a.strides[0]:0,otherStrides0:s>0?r.b.strides[0]:0,outputStrides0:r.output.shape.length>0?r.output.strides[0]:1,inputStrides1:n>1?r.a.strides[1]:0,otherStrides1:s>1?r.b.strides[1]:0,outputStrides1:r.output.shape.length>1?r.output.strides[1]:1,inputStrides2:n>2?r.a.strides[2]:0,otherStrides2:s>2?r.b.strides[2]:0,outputStrides2:r.output.shape.length>2?r.output.strides[2]:1,inputStrides3:n>3?r.a.strides[3]:0,otherStrides3:s>3?r.b.strides[3]:0,outputStrides3:r.output.shape.length>3?r.output.strides[3]:1,size:(0,u.shapeSize)(r.output.shape)};return this.runKernelInplace("logaddexp2_strided_",{dtype:this.dtype},a,e)}}}mul(e){return f.mul(this,e)}multiply(e){return f.mul(this,e)}mul_(e){if("number"==typeof e){const r={size:(0,u.shapeSize)(this.shape),other:e};return this.runKernelInplace("mul_scalar_",{dtype:this.dtype},r)}{const r=(0,u.broadcastShapes)(this,e);if((0,u.stridedShapeIsContiguous)(r.a)&&(0,u.stridedShapeIsContiguous)(r.b)){const r={size:(0,u.shapeSize)(this.shape)};return this.runKernelInplace("mul_",{dtype:this.dtype},r,e)}{const n=r.a.shape.length,s=r.b.shape.length;if(n>4||s>4)throw new Error("Broadcasting not supported for tensors with more than 4 dimensions");const a={inputStrides0:n>0?r.a.strides[0]:0,otherStrides0:s>0?r.b.strides[0]:0,outputStrides0:r.output.shape.length>0?r.output.strides[0]:1,inputStrides1:n>1?r.a.strides[1]:0,otherStrides1:s>1?r.b.strides[1]:0,outputStrides1:r.output.shape.length>1?r.output.strides[1]:1,inputStrides2:n>2?r.a.strides[2]:0,otherStrides2:s>2?r.b.strides[2]:0,outputStrides2:r.output.shape.length>2?r.output.strides[2]:1,inputStrides3:n>3?r.a.strides[3]:0,otherStrides3:s>3?r.b.strides[3]:0,outputStrides3:r.output.shape.length>3?r.output.strides[3]:1,size:(0,u.shapeSize)(r.output.shape)};return this.runKernelInplace("mul_strided_",{dtype:this.dtype},a,e)}}}neg(){return f.neg(this)}negative(){return f.neg(this)}neg_(){const e={size:(0,u.shapeSize)(this.shape)};return this.runKernelInplace("neg_",{dtype:this.dtype},e)}positive(){return f.positive(this)}positive_(){const e={size:(0,u.shapeSize)(this.shape)};return this.runKernelInplace("positive_",{dtype:this.dtype},e)}pow(e){return f.pow(this,e)}pow_(e){if("number"==typeof e){const r={size:(0,u.shapeSize)(this.shape),other:e};return this.runKernelInplace("pow_scalar_",{dtype:this.dtype},r)}{const r=(0,u.broadcastShapes)(this,e);if((0,u.stridedShapeIsContiguous)(r.a)&&(0,u.stridedShapeIsContiguous)(r.b)){const r={size:(0,u.shapeSize)(this.shape)};return this.runKernelInplace("pow_",{dtype:this.dtype},r,e)}{const n=r.a.shape.length,s=r.b.shape.length;if(n>4||s>4)throw new Error("Broadcasting not supported for tensors with more than 4 dimensions");const a={inputStrides0:n>0?r.a.strides[0]:0,otherStrides0:s>0?r.b.strides[0]:0,outputStrides0:r.output.shape.length>0?r.output.strides[0]:1,inputStrides1:n>1?r.a.strides[1]:0,otherStrides1:s>1?r.b.strides[1]:0,outputStrides1:r.output.shape.length>1?r.output.strides[1]:1,inputStrides2:n>2?r.a.strides[2]:0,otherStrides2:s>2?r.b.strides[2]:0,outputStrides2:r.output.shape.length>2?r.output.strides[2]:1,inputStrides3:n>3?r.a.strides[3]:0,otherStrides3:s>3?r.b.strides[3]:0,outputStrides3:r.output.shape.length>3?r.output.strides[3]:1,size:(0,u.shapeSize)(r.output.shape)};return this.runKernelInplace("pow_strided_",{dtype:this.dtype},a,e)}}}rad2deg(){return f.rad2deg(this)}rad2deg_(){const e={size:(0,u.shapeSize)(this.shape)};return this.runKernelInplace("rad2deg_",{dtype:this.dtype},e)}reciprocal(){return f.reciprocal(this)}reciprocal_(){const e={size:(0,u.shapeSize)(this.shape)};return this.runKernelInplace("reciprocal_",{dtype:this.dtype},e)}relu(){return f.relu(this)}relu_(){const e={size:(0,u.shapeSize)(this.shape)};return this.runKernelInplace("relu_",{dtype:this.dtype},e)}round(){return f.round(this)}round_(){const e={size:(0,u.shapeSize)(this.shape)};return this.runKernelInplace("round_",{dtype:this.dtype},e)}rsqrt(){return f.rsqrt(this)}rsqrt_(){const e={size:(0,u.shapeSize)(this.shape)};return this.runKernelInplace("rsqrt_",{dtype:this.dtype},e)}sigmoid(){return f.sigmoid(this)}sigmoid_(){const e={size:(0,u.shapeSize)(this.shape)};return this.runKernelInplace("sigmoid_",{dtype:this.dtype},e)}sign(){return f.sign(this)}sign_(){const e={size:(0,u.shapeSize)(this.shape)};return this.runKernelInplace("sign_",{dtype:this.dtype},e)}silu(){return f.silu(this)}silu_(){const e={size:(0,u.shapeSize)(this.shape)};return this.runKernelInplace("silu_",{dtype:this.dtype},e)}sin(){return f.sin(this)}sin_(){const e={size:(0,u.shapeSize)(this.shape)};return this.runKernelInplace("sin_",{dtype:this.dtype},e)}sinc(){return f.sinc(this)}sinc_(){const e={size:(0,u.shapeSize)(this.shape)};return this.runKernelInplace("sinc_",{dtype:this.dtype},e)}sinh(){return f.sinh(this)}sinh_(){const e={size:(0,u.shapeSize)(this.shape)};return this.runKernelInplace("sinh_",{dtype:this.dtype},e)}sqrt(){return f.sqrt(this)}sqrt_(){const e={size:(0,u.shapeSize)(this.shape)};return this.runKernelInplace("sqrt_",{dtype:this.dtype},e)}square(){return f.square(this)}square_(){const e={size:(0,u.shapeSize)(this.shape)};return this.runKernelInplace("square_",{dtype:this.dtype},e)}sub(e,r){return f.sub(this,e,r)}subtract(e,r){return f.sub(this,e,r)}sub_(e,r){if("number"==typeof e){const n={size:(0,u.shapeSize)(this.shape),other:e,alpha:r||1};return this.runKernelInplace("sub_scalar_",{dtype:this.dtype},n)}{const n=(0,u.broadcastShapes)(this,e);if((0,u.stridedShapeIsContiguous)(n.a)&&(0,u.stridedShapeIsContiguous)(n.b)){const n={size:(0,u.shapeSize)(this.shape),alpha:r||1};return this.runKernelInplace("sub_",{dtype:this.dtype},n,e)}{const s=n.a.shape.length,a=n.b.shape.length;if(s>4||a>4)throw new Error("Broadcasting not supported for tensors with more than 4 dimensions");const i={inputStrides0:s>0?n.a.strides[0]:0,otherStrides0:a>0?n.b.strides[0]:0,outputStrides0:n.output.shape.length>0?n.output.strides[0]:1,inputStrides1:s>1?n.a.strides[1]:0,otherStrides1:a>1?n.b.strides[1]:0,outputStrides1:n.output.shape.length>1?n.output.strides[1]:1,inputStrides2:s>2?n.a.strides[2]:0,otherStrides2:a>2?n.b.strides[2]:0,outputStrides2:n.output.shape.length>2?n.output.strides[2]:1,inputStrides3:s>3?n.a.strides[3]:0,otherStrides3:a>3?n.b.strides[3]:0,outputStrides3:n.output.shape.length>3?n.output.strides[3]:1,size:(0,u.shapeSize)(n.output.shape),alpha:r||1};return this.runKernelInplace("sub_strided_",{dtype:this.dtype},i,e)}}}tan(){return f.tan(this)}tan_(){const e={size:(0,u.shapeSize)(this.shape)};return this.runKernelInplace("tan_",{dtype:this.dtype},e)}tanh(){return f.tanh(this)}tanh_(){const e={size:(0,u.shapeSize)(this.shape)};return this.runKernelInplace("tanh_",{dtype:this.dtype},e)}trunc(){return f.trunc(this)}fix(){return f.trunc(this)}trunc_(){const e={size:(0,u.shapeSize)(this.shape)};return this.runKernelInplace("trunc_",{dtype:this.dtype},e)}xlogy(e){return f.xlogy(this,e)}xlogy_(e){if("number"==typeof e){const r={size:(0,u.shapeSize)(this.shape),other:e};return this.runKernelInplace("xlogy_scalar_",{dtype:this.dtype},r)}{const r=(0,u.broadcastShapes)(this,e);if((0,u.stridedShapeIsContiguous)(r.a)&&(0,u.stridedShapeIsContiguous)(r.b)){const r={size:(0,u.shapeSize)(this.shape)};return this.runKernelInplace("xlogy_",{dtype:this.dtype},r,e)}{const n=r.a.shape.length,s=r.b.shape.length;if(n>4||s>4)throw new Error("Broadcasting not supported for tensors with more than 4 dimensions");const a={inputStrides0:n>0?r.a.strides[0]:0,otherStrides0:s>0?r.b.strides[0]:0,outputStrides0:r.output.shape.length>0?r.output.strides[0]:1,inputStrides1:n>1?r.a.strides[1]:0,otherStrides1:s>1?r.b.strides[1]:0,outputStrides1:r.output.shape.length>1?r.output.strides[1]:1,inputStrides2:n>2?r.a.strides[2]:0,otherStrides2:s>2?r.b.strides[2]:0,outputStrides2:r.output.shape.length>2?r.output.strides[2]:1,inputStrides3:n>3?r.a.strides[3]:0,otherStrides3:s>3?r.b.strides[3]:0,outputStrides3:r.output.shape.length>3?r.output.strides[3]:1,size:(0,u.shapeSize)(r.output.shape)};return this.runKernelInplace("xlogy_strided_",{dtype:this.dtype},a,e)}}}all(e,r){return f.all(this,e,r)}any(e,r){return f.any(this,e,r)}mean(e,r){return f.mean(this,e,r)}norm(e,r){return f.norm(this,e,r)}prod(e,r){return f.prod(this,e,r)}sum(e,r){return f.sum(this,e,r)}countNonzero(e,r){return f.countNonzero(this,e,r)}}r.Tensor=Tensor},647:(e,r)=>{Object.defineProperty(r,"__esModule",{value:!0}),r.TensorBase=void 0;r.TensorBase=class TensorBase{}},414:(e,r,n)=>{Object.defineProperty(r,"__esModule",{value:!0}),r.initWebGPUAsync=r.hasWebGPU=void 0;const s=n(459);r.hasWebGPU=function hasWebGPU(){return!!navigator.gpu},r.initWebGPUAsync=async function initWebGPUAsync(){return await(0,s.discoverWebGPUDevicesAsync)()}}},__webpack_module_cache__={};function __webpack_require__(e){var r=__webpack_module_cache__[e];if(void 0!==r)return r.exports;var n=__webpack_module_cache__[e]={exports:{}};return __webpack_modules__[e].call(n.exports,n,n.exports,__webpack_require__),n.exports}var __webpack_exports__=__webpack_require__(156);return __webpack_exports__})()));
//# sourceMappingURL=torch.js.map